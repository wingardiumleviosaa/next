[{"objectID":"1690105500","permalink":"/post/k8s-ssl-not-eat-right/","title":"[Ingress] 指定了 TLS 憑證，卻吃到不正確的","content":"TL; DR\r在公司部屬 Ingress 資源後，發現一直沒法法吃到指定的憑證，結果才發現是因為 wildcard 的問題。\nProblem\r公司 Kubernetes 環境的 ingress gateway 有預設的 tls 憑證 (*.southeastasia.azure.wistron.com)，我自己申請的憑證為 *.wistron.com，想要指定給 tls 路徑為 abc.southeastasia.azure.wistron.com 的網站，但部署後連網頁發現吃的憑證會是預設的，而不是指定的。\nReason\r使用 wildcard 的憑證，只能吃到第一階的域名，比如說如果有 *.example.com 的 SSL 證書，那麼僅適用於 www.example.com 或 XXXX.example.com 等主機，不適用於 demo.app1.example.com 等主機。\nReference\rhttps://stackoverflow.com/questions/26744696/ssl-multilevel-subdomain-wildcard\r","date":"2023-07-23T17:45:00+08:00","updated":"2023-07-23T17:45:00+08:00"},{"objectID":"1690093560","permalink":"/post/network-dns-and-fqdn/network-dns-and-fqdn/","title":"DNS and FQDN","content":"FQDN\rFully Qualified Domain Name，是網際網路上特定計算機或主機的完整域名。\nDNS\r當我們申請域名時，會看到網址註冊服務商詢問是否有 DNS 的服務(大部分的網域提供商都會提供免費的 DNS 服務，如 GoDaddy, Cloudflare, Amazon Route 53)。DNS (Domain Name System) 是一種用於將域名轉換為 IP 地址的系統，以便在 Internet 上定位網絡資源。\n紀錄類型\r紀錄類型是為了輔助 DNS server (DNS 伺服器)可以更有效率地查找網址對應的 IP 位址，常使用到的紀錄類型如下:\nA (Address) Record:\rA 紀錄代表的就是 IPv4 地址，是最常用到的紀錄類型。直接將域名解析為相應的 IP 地址，如將 \u0026ldquo;example.com\u0026rdquo; 解析為 \u0026ldquo;192.0.2.1\u0026rdquo;。\nAAAA (IPv6 Address) Record:\rAAAA 記錄類似於 A 記錄，將域名映射到 IPv6 地址。若網站所在的主機是使用 IPv6 格式的 IP，則可選擇 AAAA 紀錄做為紀錄類型。\nCNAME記錄\rCNAME 記錄用於創建別名，將一個域名映射到另一個域名，讓 DNS 將網址解析到另一個目標網址上。如將 \u0026ldquo;\rwww.example.com\r\u0026rdquo; 用 CNAME 記錄解析至 \u0026ldquo;example.com\u0026rdquo; 上。DNS 查詢步驟如下:\n網站訪客在輸入 \u0026ldquo;\rwww.example.com\r\u0026rdquo; 時，DNS 會發現它是一個 CNAME 記錄，即 \u0026ldquo;example.com\u0026rdquo;。 查詢 \u0026ldquo;example.com\u0026rdquo; 所設定 A 紀錄值中的主機 IP 日後 \u0026ldquo;example.com\u0026rdquo; 設定的 A 紀錄值有所變更時，就不需要再去更改 \u0026ldquo;\rwww.example.com\r\u0026rdquo; 紀錄值。因此在使用 CNAME 紀錄時，對應到的目標網址必須帶有 A 紀錄。 MX (Mail Exchange) Record:\rMX 記錄指定郵件伺服器的地址，該地址負責接收特定域名的電子郵件。例如 A 要寄 Email 給 B 時，B 設定的 MX 紀錄值可以讓 A 的郵件主機透過 MX 紀錄找到 B 負責收信的郵件主機，讓 A 可以順利將 Email 寄給 B。\nNS (Name Server) Record:\rNS 記錄指定維護該域名 DNS 資訊的 DNS 伺服器，通常由域名註冊商設定，NS 記錄告訴 DNS 查詢該域名的資訊在哪裡可以找到。 名稱伺服器 (Name Server) 是一種 DNS 伺服器，上面儲存了網域的所有 DNS 記錄，包括 A 記錄、MX 記錄或 CNAME 記錄。\n","date":"2023-07-23T14:26:00+08:00","updated":"2023-07-23T14:26:00+08:00"},{"objectID":"1686094200","permalink":"/post/blog-final-hugo-migration/","title":"終於正式搬到 Hugo 了!!","content":"\nTL; DR\r本來以為今年的工作不會變動，殊不知在四月底突然有了新的機會，趁著一個禮拜的 gap week，來把一直想換很久的技術部落格換掉。\n架站歷史\r原先是使用社群強大的 Hexo + NexT，並透過 GitHub Page 發佈，中間過渡期自己用 Hugo + zozo 架過一次用來記錄生活的 ulanotes。隨著部落格文章增長，發現在發佈編譯時真的愈等愈長，便陸續尋覓好看的 Hugo Theme，原本找到 Eureka，但發現有很多在 Hexo 有的功能這邊都沒有 (歸檔、search..)，再來找到也是數一數二用得多的 Stack。最後終於才發現在 Hexo 的舊愛 NexT，已經有支援 Hugo 靜態編譯了！繞了一圈還是回到 NexT 的懷抱 (◍•ᴗ•◍)\n大感謝\r紀錄一下上一版使用的 stack 所參考的部落格。\nhttps://siae.me/p/stack/\rhttps://irithys.com/\rhttps://www.vincentthh35.com/\rulagraphy\r但我還是很喜歡之前陪伴很久的 UlaGraphy\r，所以不會關站 XD\nHexo Admin，還有 NexT 真的是爆炸好用!!! ಠ_ಠ\n關於首圖\r這張照片是第一份工作在當業務的時候拍的，雖然從畢業後到現在工程師這條路上繞了些路，但我相信一切都是最好的安排 ❀ ❀ ❀\n","date":"2023-06-06T23:30:00+00:00","updated":"2023-07-23T16:00:00+08:00"},{"objectID":"1685985540","permalink":"/post/golang-and-nodejs-client-access-k8s/","title":"[Golang \u0026 Node.js] Access Kubernetes Out of the Cluster","content":"Prepare .kube/config\r使用 Rancher 產生好帶有 user token 的 kubeconfig 檔案存取，將 config 檔下載後放置與 code 同層的目錄下。\nGo Clinet\r查詢指定的 namespace 是否存在\n1package main 2 3import ( 4\t\u0026#34;context\u0026#34; 5\t\u0026#34;fmt\u0026#34; 6 7\tmetav1 \u0026#34;k8s.io/apimachinery/pkg/apis/meta/v1\u0026#34; 8\t\u0026#34;k8s.io/client-go/kubernetes\u0026#34; 9\t\u0026#34;k8s.io/client-go/tools/clientcmd\u0026#34; 10) 11 12func main() { 13\t// uses the current context in kubeconfig 14\t// path-to-kubeconfig -- for example, /root/.kube/config 15\tconfig, _ := clientcmd.BuildConfigFromFlags(\u0026#34;\u0026#34;, \u0026#34;./config\u0026#34;) 16\t// creates the clientset 17\tclientset, _ := kubernetes.NewForConfig(config) 18\tns, err := clientset.CoreV1().Namespaces().Get(context.TODO(), \u0026#34;mynamespace\u0026#34;, metav1.GetOptions{}) 19\tif err != nil { 20\tfmt.Println(err) 21\t} else { 22\tfmt.Println(\u0026#34;namespace exist: \u0026#34;, ns) 23\t} 24} JavaScript Client\r查詢指定的 namespace 是否存在\n版本一：\n1try{ 2 await k8sApi.readNamespace(\u0026#39;mynamespace\u0026#39;).then( (res) =\u0026gt; { 3\tif (res.response.statusCode === 200 || res.response.statusCode === 201){ 4\tstatus = \u0026#39;synced\u0026#39; 5\tlogger.info(`mynamespace namespace 成功 sync 至 Kubernetes Cluster`); 6\t} 7\t},(err) =\u0026gt; { 8\tstatus = \u0026#39;sync failed\u0026#39;; 9\tlogger.error(`Kubernetes Cluster 無該 mynamespace namespace: ${err.body}`); 10 }); 11 return status 12}catch(e){ 13 logger.error(`k8s 連線錯誤: ${e}`) 14 return status 15} 版本二：\n1const k8s = require(\u0026#39;@kubernetes/client-node\u0026#39;); 2 3process.env[\u0026#39;NODE_TLS_REJECT_UNAUTHORIZED\u0026#39;] = \u0026#39;0\u0026#39; 4const kc = new k8s.KubeConfig(); 5kc.loadFromFile(\u0026#34;./config\u0026#34;) 6 7const k8sApi = kc.makeApiClient(k8s.CoreV1Api); 8 9let nslist = [] 10 11async function getNamespace(checkNs: string) { 12 await k8sApi.listNamespace().then(res=\u0026gt;{ 13 res.body.items.forEach(ns =\u0026gt; nslist.push(ns.metadata.name)) 14 }) 15 // console.log(nslist) 16 17 for (let i of nslist) { 18 if(i === checkNs){ 19 console.log(\u0026#39;namespace created! sync success!\u0026#39;, i) 20 } 21 } 22} Refernece\rhttps://kubernetes.io/docs/tasks/administer-cluster/access-cluster-api/#programmatic-access-to-the-api\r首圖圖片來源\r","date":"2023-06-05T17:19:00+00:00","updated":"2023-06-05T17:19:00+00:00"},{"objectID":"1685981940","permalink":"/post/golang-render-img-in-html/","title":"[Golang] Render Image in HTML","content":"TL;DR\r紀錄怎麼在 golang 的 gin api server 回傳帶有圖片的 api。\n程式碼\r1c.HTML(http.StatusOK, \u0026#34;img.html\u0026#34;, gin.H{ 2\t\u0026#34;images\u0026#34;: images, 3}) 1\u0026lt;!doctype html\u0026gt; 2\u0026lt;html\u0026gt; 3\u0026lt;head\u0026gt; 4 \u0026lt;title\u0026gt;Grab Reflow History\u0026lt;/title\u0026gt; 5\u0026lt;/head\u0026gt; 6\u0026lt;body\u0026gt; 7 \u0026lt;div class=\u0026#34;margin-body\u0026#34;\u0026gt; 8 \u0026lt;div class=\u0026#34;row\u0026#34;\u0026gt; 9 {{range .images}} 10 \u0026lt;div class=\u0026#34;image\u0026#34;\u0026gt; 11 \u0026lt;img src=\u0026#34;data:image/png;base64,{{.}}\u0026#34;/\u0026gt; 12 \u0026lt;/div\u0026gt; 13 {{end}} 14 \u0026lt;/div\u0026gt; 15 \u0026lt;/div\u0026gt; 16\u0026lt;/body\u0026gt; 17\u0026lt;/html\u0026gt; Reference\rhttps://ithelp.ithome.com.tw/articles/10269897\rhttps://stackoverflow.com/questions/57020601/how-to-render-show-images-in-html-created-on-the-fly-with-golang-and-gin-gonic\r","date":"2023-06-05T16:19:00+00:00","updated":"2023-06-05T16:19:00+00:00"},{"objectID":"1678828200","permalink":"/post/terrascan-init-with-connect-to-github-error/","title":"[Terrascan] init with connect to github error","content":"TL;DR\r在使用 terrascan 當作 IAC 語法檢查工具時，發現跑在公司內部環境的 gitlab pipeline 都會卡在 terrascan init 的錯誤。\n問題描述\rDockerfile 是透過 Gitlab Pipeline 幫忙打包的，在 Job 中跑 git clone 到公司內部存放 OPA Policy 的 Repo 皆沒問題，但是 Job 跑到 terrascan init 時總是會報 error\tcli/init.go:42\tfailed to initialize terrascan. error : could not connect to github.com 的錯誤。\n環境\rterrascan: 1.16.0 Code Tracing\r因為在前面有用 git clone 測試連結到目標 repo 是通的，所以就把問題縮小在是 terrascan 在 init 時做了什麼動作。可以在 terrascan init 時加上 -l debug lebel 幫助排查問題。\n1func DownloadPolicies() error { 2\taccessToken := config.GetPolicyAccessToken() 3\tpolicyBasePath := config.GetPolicyBasePath() 4 5\tzap.S().Debug(\u0026#34;downloading policies\u0026#34;) 6\tzap.S().Debugf(\u0026#34;base directory path : %s\u0026#34;, policyBasePath) 7 8\terr := os.RemoveAll(policyBasePath) 9\tif err != nil { 10\treturn fmt.Errorf(\u0026#34;unable to delete base folder. error: \u0026#39;%w\u0026#39;\u0026#34;, err) 11\t} 12 13\tif accessToken == \u0026#34;\u0026#34; { 14\treturn downloadDefaultPolicies(policyBasePath) 15\t} 16 17\treturn dowloadEnvironmentPolicies(policyBasePath, accessToken) 18} init 時會去 clone policy，如果沒有額外指定，會去抓取 tenable/terrascan 官方的 repo，而在公司環境雖然是指定內部的 repo，不過該 repo 也是在內網中公開的，所以去到的是 downloadDefaultPolicies function。\n1func downloadDefaultPolicies(policyBasePath string) error { 2\tif !connected(terrascanReadmeURL) { 3\treturn errNoConnection 4\t} 5 6\trepoURL := config.GetPolicyRepoURL() 7\tbranch := config.GetPolicyBranch() 8 9\tzap.S().Debugf(\u0026#34;policy directory path : %s\u0026#34;, repoURL) 10\tzap.S().Debugf(\u0026#34;policy repo url : %s\u0026#34;, repoURL) 11\tzap.S().Debugf(\u0026#34;policy repo git branch : %s\u0026#34;, branch) 12\tzap.S().Debugf(\u0026#34;cloning terrascan repo at %s\u0026#34;, policyBasePath) 13 14\t// clone the repo 15\tr, err := git.PlainClone(policyBasePath, false, \u0026amp;git.CloneOptions{ 16\tURL: repoURL, 17\t}) 18 19\tif err != nil { 20\treturn fmt.Errorf(\u0026#34;failed to download policies. error: \u0026#39;%w\u0026#39;\u0026#34;, err) 21\t} 22 23\t// create working tree 24\tw, err := r.Worktree() 25\tif err != nil { 26\treturn fmt.Errorf(\u0026#34;failed to create working tree. error: \u0026#39;%w\u0026#39;\u0026#34;, err) 27\t} 28 29\t// fetch references 30\terr = r.Fetch(\u0026amp;git.FetchOptions{ 31\tRefSpecs: []gitConfig.RefSpec{\u0026#34;refs/*:refs/*\u0026#34;, \u0026#34;HEAD:refs/heads/HEAD\u0026#34;}, 32\t}) 33\tif err != nil { 34\treturn fmt.Errorf(\u0026#34;failed to fetch references from git repo. error: \u0026#39;%w\u0026#39;\u0026#34;, err) 35\t} 36 37\t// checkout policies branch 38\terr = w.Checkout(\u0026amp;git.CheckoutOptions{ 39\tBranch: plumbing.ReferenceName(fmt.Sprintf(\u0026#34;refs/heads/%s\u0026#34;, branch)), 40\tForce: true, 41\t}) 42\tif err != nil { 43\treturn fmt.Errorf(\u0026#34;failed to checkout git branch \u0026#39;%s\u0026#39;. error: \u0026#39;%w\u0026#39;\u0026#34;, branch, err) 44\t} 45 46\treturn nil 47} 48 49func connected(url string) bool { 50\t_, err := http.Get(url) 51\treturn err == nil 52} 這邊就直接看到原因了，在 function 的一開始會去做 connected(terrascanReadmeURL)，而這個丟進去的參數並不是定義在 config 檔的 repo_url，而是 terrascanReadmeURL (\rhttps://raw.githubusercontent.com/tenable/terrascan/master/README.md\r)，所以在內網環境因為公司的 Kubernetes 有設防火牆，所以直接被擋掉了。\n解決方法\r捨棄 terrascan init，直接自己用 git clone 並把 clone 下來的 repo 放到 terrascan 的指定資料夾 ~/.terrascan/，跳過 source code 的 bug。\n","date":"2023-03-14T21:10:00+00:00","updated":"2023-03-14T21:10:00+00:00"},{"objectID":"1676737920","permalink":"/post/get-azure-vault-secret-for-all/","title":"[Azure] 使用 Vault Secrets API 如何取得超過 25 筆後的資料","content":"TL; DR\r使用 Azure key vault 的 API Get 所有 secrets 時可以利用 maxresults 的參數限制回傳的筆數，預設為 25 筆，但經測試，最大的顯示筆數也就是 25 筆了，要如何取得全部的 secrets 呢? 以此篇記錄。\n利用 nextLink 取得下一頁資料\r假設目前的 vault URI 為 https://arm-vault-prd.vault.azure.net/ 的，我們先來看最一開始使用 get secrets 返回的結果，因不想一次顯示 25 筆，利用 maxresults 參數指令顯示兩筆：\n1{ 2 \u0026#34;value\u0026#34;: [ 3 { 4 \u0026#34;id\u0026#34;: \u0026#34;https://arm-vault-prd.vault.azure.net/secrets/azure-AZRAutoTesting-prd-azure-client-id\u0026#34;, 5 \u0026#34;attributes\u0026#34;: { 6 \u0026#34;enabled\u0026#34;: true, 7 \u0026#34;created\u0026#34;: 1651506090, 8 \u0026#34;updated\u0026#34;: 1651506090, 9 \u0026#34;recoveryLevel\u0026#34;: \u0026#34;Recoverable+Purgeable\u0026#34;, 10 \u0026#34;recoverableDays\u0026#34;: 90 11 }, 12 \u0026#34;tags\u0026#34;: {} 13 }, 14 { 15 \u0026#34;id\u0026#34;: \u0026#34;https://arm-vault-prd.vault.azure.net/secrets/azure-AZRAutoTesting-prd-azure-client-secret\u0026#34;, 16 \u0026#34;attributes\u0026#34;: { 17 \u0026#34;enabled\u0026#34;: true, 18 \u0026#34;created\u0026#34;: 1651506109, 19 \u0026#34;updated\u0026#34;: 1651506109, 20 \u0026#34;recoveryLevel\u0026#34;: \u0026#34;Recoverable+Purgeable\u0026#34;, 21 \u0026#34;recoverableDays\u0026#34;: 90 22 }, 23 \u0026#34;tags\u0026#34;: {} 24 } 25 ], 26 \u0026#34;nextLink\u0026#34;: \u0026#34;https://arm-vault-prd.vault.azure.net:443/secrets?api-version=7.4\u0026amp;$skiptoken=eyJOZXh0TWFya2VyIjoiMiExMzIhTURBd01EVXpJWE5sWTNKbGRDOUJXbFZTUlMxQldsSkJWVlJQVkVWVFZFbE9SeTFRVWtRdFFWcFZVa1V0VTFWQ1UwTlNTVkJVU1U5T0xVbEVJVEF3TURBeU9DRTVPVGs1TFRFeUxUTXhWREl6T2pVNU9qVTVMams1T1RrNU9UbGFJUS0tIiwiVGFyZ2V0TG9jYXRpb24iOjB9\u0026amp;maxresults=2\u0026#34; 27} 可以發現回傳的資訊最後有一個 nextLink 的參數，這個就是下兩筆 secrets 取得的位址！請注意使用此 API 一樣要在 header 輸入 Authencation 的資訊。 用 Postman 試打的結果如下：\n第一次:\n第二次使用第一次回傳的 NextLink:\n","date":"2023-02-18T16:32:00+00:00","updated":"2023-02-18T16:32:00+00:00"},{"objectID":"1676392320","permalink":"/post/create-subscription-by-azure-mgmt-api-and-spn/","title":"[Azure] 透過 Managment API 建立訂閱帳戶","content":"TL;DR\r本篇文章記錄透過 Azure Management API 使用 Azure Service Principal 權限授權，建立 Billing Account 類型為 Enterprise Agreement 的 Subscription 訂閱帳戶。\n先決條件: 賦予權限\r在 Azure Portal 上沒有針對 Billing Account 的 IAM 可以設定，需要透過 API 賦予 subscription creator role 給對應的服務主體(service principal)。這個步驟可以透過 Microsoft Learn 提供的 API focus mode 功能實作，請前往\renrollment-account-role-assignments 的 API 文檔\r。\n權限列表\r附上在 EA 收費下，SPN 有四種權限可被賦予，一個 SPN 只能被賦予一種權限。\nRole Role definition ID Actions allowed EnrollmentReader 24f8edb6-1668-4659-b5e2-40bb5f3a7d7e Enrollment readers can view data at the enrollment, department, and account scopes. The data contains charges for all of the subscriptions under the scopes, including across tenants. Can view the Azure Prepayment (previously called monetary commitment) balance associated with the enrollment. EA purchaser da6647fb-7651-49ee-be91-c43c4877f0c4 Purchase reservation orders and view reservation transactions. It has all the permissions of EnrollmentReader, which will in turn have all the permissions of DepartmentReader. It can view usage and charges across all accounts and subscriptions. Can view the Azure Prepayment (previously called monetary commitment) balance associated with the enrollment. DepartmentReader db609904-a47f-4794-9be8-9bd86fbffd8a Download the usage details for the department they administer. Can view the usage and charges associated with their department. SubscriptionCreator a0bcee42-bf30-4d1b-926a-48d21664ef71 Create new subscriptions in the given scope of Account. 透過 REST API 建立訂閱帳戶\r透過 Subscription 的 Alias API\r1PUT https://management.azure.com/providers/Microsoft.Subscription/aliases/aliasForNewSub?api-version=2020-09-01 2 3{ 4 \u0026#34;properties\u0026#34;: { 5 \u0026#34;displayName\u0026#34;: \u0026#34;Contoso MCA subscription\u0026#34;, 6 \u0026#34;billingScope\u0026#34;: \u0026#34;/providers/Microsoft.Billing/billingAccounts/e879cf0f-2b4d-5431-109a-f72fc9868693:024cabf4-7321-4cf9-be59-df0c77ca51de_2019-05-31/billingProfiles/PE2Q-NOIT-BG7-TGB/invoiceSections/MTT4-OBS7-PJA-TGB\u0026#34;, 7 \u0026#34;workload\u0026#34;: \u0026#34;Production\u0026#34; 8 } 9} Info\nAzure 的 aliases 資源類型是擴展資源 (extension resource)，可以將其應用於另一個資源。\nImplement by NodeJS\r1public async createSubscription(subscriptionName: string, envWorkload: string, envBillingScope: string): Promise\u0026lt;string\u0026gt; { 2 let subscriptionId = \u0026#39;\u0026#39;; 3 try{ 4 const authResponse = await this.getMgmtToken(); 5 const createBody = { 6 properties: { 7 billingScope: envBillingScope, 8 displayName: subscriptionName, 9 workload: envWorkload, 10 }, 11 }; 12 13 const response = await fetch( 14 `${this.conf.subscriptionsApi.aliasUrl}/${subscriptionName}${this.conf.subscriptionsApi.aliasUrlVersion}`, 15 { 16 method: \u0026#39;put\u0026#39;, 17 body: JSON.stringify(createBody), 18 headers: {\u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, Authorization: `Bearer ${authResponse.accessToken}`}, 19 }, 20 ) 21 22 if (!response.ok) { 23 throw new Error(`建立 subscription 發生錯誤, status: ${response.status}, errorMsg: ${JSON.stringify(await response.json())}`); 24 } 25 26 const body : any = await response.json(); 27 28 subscriptionId = Object(Object(body).properties).subscriptionId; 29 30 }catch(e){ 31 logger.error(e) 32 } 33 return subscriptionId; 34 } Reference\rhttps://learn.microsoft.com/en-us/azure/templates/microsoft.subscription/aliases?pivots=deployment-language-bicep\rhttps://learn.microsoft.com/en-us/azure/cost-management-billing/manage/assign-roles-azure-service-principals#assign-the-subscription-creator-role-to-the-spn\r首圖圖片來源\r","date":"2023-02-14T16:32:00+00:00","updated":"2023-02-14T16:32:00+00:00"},{"objectID":"1674893940","permalink":"/post/golang-winrm-multiple-command-sample-code/","title":"[Golang] 使用 winrm 下多行指令操作遠端機器","content":"TL; DR\r使用 masterzen/winrm\r套件透過 winrm protocal 遠端連線至指定機器並下 powershell 指令操作。\n程式碼\r1// 直接寫成一個 function~ 2func WinExec() { 3\tparams := winrm.DefaultParameters 4 5\tendpoint := winrm.NewEndpoint(\u0026#34;10.37.36.79\u0026#34;, 5985, false, false, nil, nil, nil, 0) 6\tclient, err := winrm.NewClientWithParameters(endpoint, \u0026#34;Administrator\u0026#34;, \u0026#34;Wistron@2017\u0026#34;, params) 7\tif err != nil { 8\tpanic(err) 9\t} 10 11\tctx, cancel := context.WithCancel(context.Background()) 12\tdefer cancel() 13 14\tshell, err := client.CreateShell() 15\tif err != nil { 16\tpanic(err) 17\t} 18\tdefer shell.Close() 19 20\t// 可以在這邊下多行指令 21\tscript := fmt.Sprintf(` 22\tipconfig 23\t`) 24 25\tcmd, err := shell.ExecuteWithContext(ctx, winrm.Powershell(script)) 26\t// _, err = client.RunWithContext(ctx, \u0026#34;echo username\u0026#34;, os.Stdout, os.Stderr) 27 28\tif err != nil { 29\tpanic(err) 30\t} 31 32\tdefer cmd.Close() 33 34\tvar wg sync.WaitGroup 35\tcopyFunc := func(w io.Writer, r io.Reader) { 36\tdefer wg.Done() 37\tio.Copy(w, r) 38\t} 39 40\twg.Add(2) 41\tgo copyFunc(os.Stdout, cmd.Stdout) 42\tgo copyFunc(os.Stderr, cmd.Stderr) 43 44\tcmd.Wait() 45\twg.Wait() 46 47\tif cmd.ExitCode() != 0 { 48\tpanic(cmd.ExitCode()) 49\t} 50} ","date":"2023-01-28T08:19:00+00:00","updated":"2023-01-28T08:19:00+00:00"},{"objectID":"1673521560","permalink":"/post/win10-winrm-to-remote-host/","title":"[Windows] 使用 winrm 遠端連線到其他主機","content":"遠端機器設定\rwinrm service 預設都是未啟用的狀態，先檢視狀態；如無返回資訊，則是沒有啟動；\n1winrm enumerate winrm/config/listener 針對 winrm service 進行基礎配置：\n1winrm quickconfig 重新檢視 winrm service listener:\n1winrm e winrm/config/listener 為 winrm service 配置auth:\n1winrm set winrm/config/service/auth \u0026#39;@{Basic=\u0026#34;true\u0026#34;}\u0026#39; 為 winrm service 配置加密方式為允許非加密：\n1winrm set winrm/config/service \u0026#39;@{AllowUnencrypted=\u0026#34;true\u0026#34;}\u0026#39; 本地機器設定\r如果要用 powershell 連的話，本地端也需要設置 trustedhosts 的 value\n1Set-Item WSMan:\\localhost\\Client\\TrustedHosts -Value \u0026#34;*\u0026#34; -Force 如果丟 Access Denied 的錯誤的話，則使用 Administrator 身分開啟 powershell 並設置\n1Set-ItemProperty -Path HKLM:\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Policies\\System -Name LocalAccountTokenFilterPolicy -Value 1 -Type DWord 使用 Invoke-Command 連線下指令\r1Invoke-Command -ComputerName \u0026lt;遠端系統名稱\u0026gt; -ScriptBlock { \u0026lt;遠端指令\u0026gt; } -credential \u0026lt;遠端系統使用者帳戶名\u0026gt; 將 \u0026lt;遠端系統名稱\u0026gt; 替換為遠端系統的名稱或 IP 位址，並將 \u0026lt;遠端指令\u0026gt; 替換為要在遠端系統上執行的 PowerShell 指令。指令應該放在大括號 {} 內，以便於形成腳本塊。最後帶上要使用哪個使用者帳戶執行，會跳出視窗輸入帳密資訊。\nReference\rhttps://dotblogs.com.tw/momoBear/2018/01/04/160854\rhttps://social.technet.microsoft.com/Forums/Azure/en-US/b853e77c-7c9a-4231-a32b-c63727ec5868/access-denied-when-trying-to-set-trusted-hosts-for-psremoting?forum=winserverpowershell\r","date":"2023-01-12T11:06:00+00:00","updated":"2023-01-12T11:06:00+00:00"},{"objectID":"1673345580","permalink":"/post/os-win10-openssh/win10-openssh/","title":"[Windows] 使用 openssh 連線到其他主機","content":"在連線機器上的在 Windows 選用功能下安裝 OpenSSH Server\n或是使用 powershell 設定\n1Get-Service -Name *ssh* 2Start-Service sshd 3Set-Service -Name sshd -StartupType \u0026#39;Automatic\u0026#39; 4New-NetFirewallRule -Name sshd -DisplayName \u0026#39;OpenSSH Server (sshd)\u0026#39; -Enabled True -Direction Inbound -Protocol TCP -Action Allow -LocalPort 22 便可以使用 22 端口連線至機器上\n1ssh -p 22 Administrator@192.168.1.90 ","date":"2023-01-10T10:13:00+00:00","updated":"2023-01-10T10:13:00+00:00"},{"objectID":"1672258920","permalink":"/post/docker-exec-no-such-file-or-directory-in-wsl/","title":"[Docker] 在 wsl docker 出現 exec no such file or directory 錯誤","content":"TL;DR\r在 windows 環境下 build docker image 並跑 container 時會出現 exec xxx no such file or directory 的錯誤，但明明使用同樣的 dockerfile 在 linux 環境卻沒問題。\nReason\r原因是因為換行問題，Docker 不認識 Windows 的 CRLF 換行。可以看這個範例更清楚，在 alpine container 環境原本的 CRLF 換行會被解譯成 \\r。\nSolution\r將 Dockerfile 的換行符從 CRLF 改成 LF，可使用 VS Code 最下方 status bar 的 Select End of Line Sequence 功能，並存檔再重 build 即可。\n","date":"2022-12-28T20:22:00+00:00","updated":"2022-12-28T20:22:00+00:00"},{"objectID":"1671393120","permalink":"/post/node-send-mail-by-nodemailer/","title":"[Node.js] 使用 NodeMailer 發送信件","content":"TL;DR\r在本地端透過 nodemailer 寄送郵件時，發現預設的 25 port 會有 connect ECONNREFUSED 的錯誤無法傳送。\nSolution\r1import * as nodemailer from \u0026#39;nodemailer\u0026#39;; 2import {Options} from \u0026#39;nodemailer/lib/mailer\u0026#39;; 3 4async function mail(){ 5 const transporter = nodemailer.createTransport({ 6 host: \u0026#34;hqsmtp.abc.com\u0026#34;, 7 port: 587, 8 secure: false, 9 auth: { 10 user: \u0026#34;test1@abc.com\u0026#34;, 11 pass: \u0026#34;!Passw0rd\u0026#34; 12 }, 13 tls: { 14 rejectUnauthorized: false 15 } 16 }); 17 18 const options: Options = { 19\tfrom: \u0026#34;test1@abc.com\u0026#34;, 20\tto: \u0026#34;test2@abc.com\u0026#34;, 21\tsubject: \u0026#34;test\u0026#34;, 22\ttext: \u0026#34;test\u0026#34;, 23 }; 24 await transporter.sendMail(options); 25 transporter.close(); 26} secure \u0026lt;boolean\u0026gt; – If false (the default) then TLS is used if server supports the STARTTLS extension. For port 587 or 25 keep it false\nrejectUnauthorized \u0026lt;boolean\u0026gt; – If not false the server will reject any connection which is not authorized with the list of supplied CAs. This option only has an effect if requestCert is true. Default: true.\nReference\rhttps://stackoverflow.com/questions/46742402/error-self-signed-certificate-in-certificate-chain-nodejs-nodemailer-express\r","date":"2022-12-18T19:52:00+00:00","updated":"2022-12-18T19:52:00+00:00"},{"objectID":"1668892680","permalink":"/post/powershell-partition-and-format-all-disk-at-once/","title":"[PowerShell] 一次分割所有硬碟","content":"Get-Disk\r取得當前連接到 win10 的所有硬碟\n1Get-Disk 硬碟初始化\r將硬碟設置為 online 以及關閉 readonly，並初始化所有硬碟指定分割區樣式為 GPT（GUID Partition Table）。\n1try { 2 Get-Disk | ?{$_.number -ne 0}| Set-Disk -IsOffline $False 3 Get-Disk | ?{$_.number -ne 0}| Set-Disk -isReadOnly $False 4 Get-Disk | ?{$_.number -ne 0}| Initialize-Disk -PartitionStyle GPT 5}catch{ 6 Write-Host $_.Exception.Message 7} 磁碟分割\r依序分割磁碟並指派磁碟槽代號，最後格式化\n1try { 2 # 為所有硬碟建立分區並自動指派硬碟代號以及使用最大的硬碟大小 3 Get-Disk | ?{$_.number -ne 0}| New-Partition -AssignDriveLetter -UseMaximumSize 4 # 取得所有硬碟的分區並格式化 5 Get-Disk | ?{$_.number -ne 0}| Get-Partition |?{$_.type -like \u0026#34;Basic\u0026#34;}| Format-Volume -Confirm:$false 6}catch{ 7 Write-Host $_.Exception.Message 8} 其中 Format-Volume 使用預設的檔案系統 NTFS，還有其他可能的值：\nNTFS：預設檔案系統，提供了對大型檔案和分割區的支援，並具有高度的可靠性和安全性。 FAT32：與 Windows 和其他操作系統的相容性較好，但不支援單個檔案超過 4GB。 exFAT：與 Windows 和其他操作系統的相容性較好，支援大型檔案和分割區，但不如 NTFS。 Reference\rhttp://www.alexandreviot.net/2015/05/02/powershell-how-to-format-all-disks/\r","date":"2022-11-19T21:18:00+00:00","updated":"2022-11-19T21:18:00+00:00"},{"objectID":"1666552080","permalink":"/post/k8s-cronjob-run-job-alpine-at-last-sunday-of-the-month/cronjob-for-last-day-of-the-month-using-kubernetes/","title":"CronJob to Run the Job (alpine base image) at the Last Sunday of the Month","content":"TL;DR\r本篇文章記錄如何在 aplpine base 的 container 中於每月的最後一個禮拜日執行指定任務。\nSoluiton\r1apiVersion: batch/v1 2kind: CronJob 3metadata: 4 name: hello 5 namespace: test 6spec: 7 schedule: \u0026#34;0 8 * * 0\u0026#34; 8 jobTemplate: 9 spec: 10 template: 11 spec: 12 containers: 13 - name: hello 14 image: alpine:3.16 15 resources: 16 limits: 17 cpu: \u0026#34;2\u0026#34; 18 memory: 4000Mi 19 imagePullPolicy: Always 20 command: 21 - /bin/sh 22 - -c 23 - \u0026#39;[ $(date +%m) -ne $(date -d \u0026#34;@$(($(date +%s) + 604800))\u0026#34; +%m) ] \u0026amp;\u0026amp; echo Hello from the Kubernetes cluster\u0026#39; 24 restartPolicy: OnFailure 坑\r原本前面判斷的語法是寫在 ubuntu 下 run 得好好的 [ $(date +%m) -ne $(date -d +7days +%m) ]，但是殊不知搬到 alpine container 後會報錯 date: invalid date '+7days'。所以改寫成 apline 環境中認得的 -d 格式！\nReference\rhttps://stackoverflow.com/questions/71651529/schedule-cronjob-for-last-day-of-the-month-using-kubernetes\rhttps://unix.stackexchange.com/a/522622\r","date":"2022-10-23T19:08:00+00:00","updated":"2022-10-23T19:08:00+00:00"},{"objectID":"1666552020","permalink":"/post/k8s-minikube-pull-img-from-harbor-in-wsl2/minikube-pull-harbor-image/","title":"Minikube Pull Image from Private Repository in WSL2","content":"TL;DR\r在 windows 環境的 wsl2 Ubuntu 上跑了 Minikube 要用來測試 kubernetes 的應用佈屬，但一直卡在拉取公司內部 harbor 鏡像的時候報 x509: certificate signed by unknown authority 錯誤。\n嘗試在 docker desktop 的 docker engine 加上 insecure-registries 的參數但是 minikube 不曉得為什麼吃不到，也試過把憑證放到 /etc/ssl/certs 下但一樣不認得 =__=? 總之最後才發現 minikube 的 docker 跟 docker desktop 的 docker 環境是分開的。\nSolution\r1vi ~/.minikube/machines/\u0026lt;PROFILE_NAME\u0026gt;/config.json (in my case ~/.minikube/machines/minikube/config.json) add private repo on InsecureRegistry attribute (json path: HostOptions.EngineOptions.InsecureRegistry) 1minikube stop 2minikube start Then, change the Docker daemon from Minikube\n1eval $(minikube docker-env) Reference\rhttps://stackoverflow.com/questions/38748717/can-not-pull-docker-image-from-private-repo-when-using-minikube\rhttps://tachingchen.com/tw/blog/build-docker-image-in-minikube-vm/\rhttps://stackoverflow.com/questions/52310599/what-does-minikube-docker-env-mean\r","date":"2022-10-23T19:07:00+00:00","updated":"2022-10-23T19:07:00+00:00"},{"objectID":"1666551840","permalink":"/post/k8s-run-crond-as-non-root-in-alpine/run-crond-as-non-root-in-alpine-container-by-pod-or-deployment/","title":"Run Crond as Non Root in Alpine Container by Pod/Deployment","content":"TL;DR\r在\r上一篇文章\r中試了在 alpine docker container 中使用 non root user 跑 crond，但將 build 好的 docker image 搬到 kubernetes 給 deployment 的 pod 使用時，卻會出現 initgroup operation not permitted 的錯誤。\n踩坑踩了整整三天，該改的權限都改了，最後終於找到 supercronic 這個酷東西 T_T\nDockerfile\r這邊的 dockerfile 直接先下載好 supercronic build 好的 binary，再 COPY 進 image 中，也可以參考 installation instruction\r在 build 的階段下載。\n1# We want to populate the module cache based on the go.{mod,sum} files. 2COPY go.mod . 3COPY go.sum . 4 5RUN go mod download 6 7COPY . . 8 9# Build the Go app 10RUN go build -o ./out/ccoe-bot . 11 12# Start fresh from a smaller image 13FROM harbor.wistron.com/base_image/alpine:3.12 14 15USER root 16 17RUN apk update \u0026amp;\u0026amp; addgroup --gid 1000 ccoebot \u0026amp;\u0026amp; adduser --disabled-password --ingroup ccoebot --uid \u0026#34;1000\u0026#34; ccoebot 18 19COPY --from=build_base --chown=ccoebot:ccoebot /tmp/ccoe-bot/out/ccoe-bot /home/ccoebot/app/ccoe-bot 20COPY bin/git-sync /usr/bin 21COPY bin/terragrunt /usr/bin 22COPY bin/terrascan /usr/bin 23COPY bin/supercronic /usr/bin 24COPY --chown=ccoebot:ccoebot init.sh /home/ccoebot/app 25 26USER ccoebot 27RUN git clone https://gitlab.wistron.com/ccoe/terrascan_policy.git /home/ccoebot/terrascan_policy 28WORKDIR /home/ccoebot/terrascan_policy 29RUN terrascan init -c terrascan-config.toml 30WORKDIR /home/ccoebot/.terrascan 31RUN git config --bool branch.master.sync true \u0026amp;\u0026amp; git branch -D HEAD 32 33# set scheduler for git-sync 34RUN echo \u0026#34;*/10 * * * * cd /home/ccoebot/.terrascan \u0026amp;\u0026amp; date \u0026gt;\u0026gt; /home/ccoebot/app/sync.log \u0026amp;\u0026amp; /usr/bin/git-sync \u0026gt;\u0026gt; /home/ccoebot/app/sync.log\u0026#34; \u0026gt;\u0026gt; /home/ccoebot/app/mycron 35 36# This container exposes port 8080 to the outside world 37EXPOSE 8080 38 39# Run the binary program produced by `go install` 40#CMD [\u0026#34;/app/ccoe-bot\u0026#34;] 41# repack the ccoe-bot and crond to init.sh 42 43CMD [\u0026#34;/home/ccoebot/app/init.sh\u0026#34;] init.sh\r這裡又是另一個要注意的地方，因為 supercronic 也是一個要跑的使用者程序，故這個案例同時會有兩個程序需要在 CMD 裡面一同跑起，使用下面的寫法完成在同個 container 中跑兩個程序。\n1#!/bin/bash 2 3/usr/bin/supercronic /home/ccoebot/app/mycron \u0026amp; 4P1=$! 5/home/ccoebot/app/ccoe-bot \u0026amp; 6P2=$! 7wait $P1 $P2 或是\n1#!/bin/bash 2 3# Start the first process 4/usr/bin/supercronic /home/ccoebot/app/mycron \u0026amp; 5 6# Start the second process 7/home/ccoebot/app/ccoe-bot \u0026amp; 8 9# Wait for any process to exit 10wait -n 11 12# Exit with status of process that exited first 13exit $? kubernetes deployment yaml\r1apiVersion: apps/v1 2kind: Deployment 3metadata: 4 name: dev-ccoebot 5 namespace: atlantis 6spec: 7 progressDeadlineSeconds: 600 8 replicas: 1 9 revisionHistoryLimit: 10 10 selector: 11 matchLabels: 12 app: dev-ccoebot 13 strategy: 14 rollingUpdate: 15 maxSurge: 1 16 maxUnavailable: 0 17 type: RollingUpdate 18 template: 19 spec: 20 containers: 21 - image: harbor.wistron.com/k8sprdwhqccoe/ccoe-bot:nonroot 22 imagePullPolicy: IfNotPresent 23 name: dev-ccoebot 24 resources: 25 requests: 26 memory: \u0026#39;256Mi\u0026#39; 27 cpu: \u0026#39;512m\u0026#39; 28 limits: 29 memory: \u0026#39;1024Mi\u0026#39; 30 cpu: \u0026#39;1024m\u0026#39; 31 securityContext: 32 runAsUser: 1000 33 runAsGroup: 1000 34 allowPrivilegeEscalation: false 35 privileged: false 36 readOnlyRootFilesystem: false 37 runAsNonRoot: true 38 stdin: true 39 terminationMessagePath: /dev/termination-log 40 terminationMessagePolicy: File 41 tty: true 42 dnsPolicy: ClusterFirst 43 imagePullSecrets: 44 - name: hbsrt 45 restartPolicy: Always 46 schedulerName: default-scheduler 47 securityContext: 48 runAsUser: 1000 49 runAsGroup: 1000 50 fsGroup: 1000 51 runAsNonRoot: true 52 terminationGracePeriodSeconds: 30 Result\rReference\r拯救我用 supercronic 的文章, 感謝 alesk 大\nhttps://gist.github.com/alesk/33b716f04cdce0751473b8232405dc32\rRun Mutiple Process in Container: https://docs.docker.com/config/containers/multi-service_container/\rhttps://stackoverflow.com/a/56663151\r","date":"2022-10-23T19:04:00+00:00","updated":"2022-10-23T19:04:00+00:00"},{"objectID":"1666551720","permalink":"/post/devops-run-cronjob-as-non-root-user-in-alpine-docker-container/docker-run-cronjob-as-non-root-user-in-alpine-container/","title":"[Docker] Run Cronjob as Non Root User in Alpine Container","content":"TL;DR\rapline image 預設只能給 root 執行 crond，但剛好遇到有不允許 container 使用 root 執行的安全政策的情況。本篇記錄如何在 apline container 中使用 non root user 執行 crond。\nDockerfile\r1FROM harbor.wistron.com/base_image/alpine:3.12 2 3USER root 4RUN apk update \u0026amp;\u0026amp; apk --no-cache add dcron libcap 5 6RUN addgroup --gid 1000 ula \u0026amp;\u0026amp; adduser --disabled-password --ingroup ula --uid \u0026#34;1000\u0026#34; ula 7 8RUN chown ula:ula /usr/sbin/crond \u0026amp;\u0026amp; setcap cap_setgid=ep /usr/sbin/crond 9 10COPY --chown=ula:ula init.sh /home/ula/app 11RUN touch /etc/crontabs/ula \u0026amp;\u0026amp; chown -R ula:ula /etc/crontabs/ula 12 13USER ula 14 15# set scheduler 16RUN echo \u0026#34;*/10 * * * * date \u0026gt;\u0026gt; /home/ccoebot/app/sync.log\u0026#34; \u0026gt;\u0026gt; /etc/crontabs/ula 17 18# This container exposes port 8080 to the outside world 19EXPOSE 8080 20 21CMD [\u0026#34;/home/ula/app/init.sh\u0026#34;] init.sh\r1#!/bin/bash 2crond -l 8 Result\rReference\rhttps://github.com/gliderlabs/docker-alpine/issues/381\r","date":"2022-10-23T19:02:00+00:00","updated":"2022-10-23T19:02:00+00:00"},{"objectID":"1666551540","permalink":"/post/devops-ansible-windows-update-with-auth-proxy/ansible-windows-update-with-auth-proxy/","title":"[Ansible] Windows Update with Authenticated Proxy","content":"背景\r在透過 ansible windows update module 更新 windows 時，都會因為 proxy 沒有設置而失敗。\n解決方式\r1- name: Configure IE proxy settings to apply to all users 2 ansible.windows.win_regedit: 3 path: HKLM:\\SOFTWARE\\Policies\\Microsoft\\Windows\\CurrentVersion\\Internet Settings 4 name: ProxySettingsPerUser 5 data: 0 6 type: dword 7 state: present 8 9- name: Configure IE to use a specific proxy per protocol 10 community.windows.win_inet_proxy: 11 auto_detect: no 12 proxy: whqproxys.abc.com:8080 13 14- name: Set credential to use for proxy auth 15 community.windows.win_credential: 16 name: whqproxys.abc.com 17 type: generic_password 18 username: \u0026#34;{{ proxy_user }}\u0026#34; 19 secret: \u0026#34;{{ lookup(\u0026#39;env\u0026#39;, \u0026#39;VC_PASSWORD\u0026#39;) }}\u0026#34; 20 state: present 21 become: yes 22 become_user: Administrator 23 become_method: runas 24 25- name: Set the proxy to be able to run Windows Updates 26 ansible.windows.win_command: 27 cmd: netsh winhttp import proxy source=ie 28 29- name: Search-only, return list of found updates (if any), log to txt file 30 ansible.windows.win_updates: 31 category_names: \u0026#39;*\u0026#39; 32 state: searched 33 log_path: C:\\Users\\Administrator\\Desktop\\ansible_update_list.txt 34 35- name: Windows Update which may take a significant amount of time to complete 36 ansible.windows.win_updates: 37 category_names: \u0026#39;*\u0026#39; 38 state: installed 39 reboot: yes 40 log_path: C:\\Users\\Administrator\\Desktop\\ansible_update_log.txt 補充\rAnsible win_inet_proxy 的文件\r在設定 IE http proxy 時是透過 win_http_proxy 的模組，但該作法好像失敗，還是需用 netsh winhttp import proxy source=ie 才可正確應用。\n1# 失敗 2- name: Import IE proxy configuration to WinHTTP 3 community.windows.win_http_proxy: 4 source: ie Reference\r感謝極為稀有的 serverfault 上的問題 T_T\r","date":"2022-10-23T18:59:00+00:00","updated":"2022-10-23T18:59:00+00:00"},{"objectID":"1666551300","permalink":"/post/programming-node-get-access-token-graph-api/node-get-azure-graph-access-token/","title":"[Node.js] Get Access Token 並使用 Microsoft Graph API 操作 AD Application","content":"情景\r使用 NodeJS @azure/msal-node module 取得 Access Token，並進一步使用 Microsoft Graph API 對操作 Azure AD Application 做 Role Assignment。\n作法\r1import * as https from \u0026#39;https\u0026#39;; 2import fetch from \u0026#39;node-fetch\u0026#39;; 3import * as msal from \u0026#39;@azure/msal-node\u0026#39;; 4 5const httpsAgent = new https.Agent({ 6 rejectUnauthorized: false, 7 keepAlive: true 8}); 9 10const msalConfig = { 11 auth: { 12 clientId: \u0026#39;\u0026lt;YOUR CLIENT ID\u0026gt;\u0026#39;, 13 clientSecret: \u0026#39;\u0026lt;YOUR CLIENT SECRET\u0026gt;\u0026#39;, 14 authority: \u0026#39;https://login.microsoftonline.com/\u0026lt;YOUR TENANT ID\u0026gt;/\u0026#39; 15 }, 16}; 17 18async function getToken(): Promise\u0026lt;any\u0026gt; { 19 const cca = new msal.ConfidentialClientApplication(msalConfig); 20 try{ 21 return await cca.acquireTokenByClientCredential({ 22 scopes: [\u0026#39;https://graph.microsoft.com/.default\u0026#39;], 23 }); 24 }catch(e){ 25 console.log(`Get Azure Access Token Error: ${e}`); 26 } 27} 28 29async function addAzureOpsgenieAppRole(userPrincipalName: string): Promise\u0026lt;any\u0026gt; { 30 const authResponse = await getToken(); 31 const response = await fetch(`https://graph.microsoft.com/v1.0/users/${userPrincipalName}`, { 32 method: \u0026#39;get\u0026#39;, 33 headers: {\u0026#39;Authorization\u0026#39;: `Bearer ${authResponse.accessToken}`}, 34 agent: httpsAgent 35 }).then( response =\u0026gt; { 36 if(response.ok){ 37 return response.json(); 38 }else{ 39 return Promise.reject(response); 40 } 41 }).then( data =\u0026gt; { 42 const createBody = { 43 principalId: data.id, 44 resourceId: \u0026#39;\u0026lt;YOUR RESOURCE ID\u0026gt;\u0026#39;, 45 appRoleId: \u0026#39;\u0026lt;YOUR APPLICATION ROLE ID\u0026gt;\u0026#39; 46 } 47 return fetch(`https://graph.microsoft.com/v1.0/users/${data.id}/appRoleAssignments`, { 48 method: \u0026#39;post\u0026#39;, 49 body: JSON.stringify(createBody), 50 headers: {\u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Authorization\u0026#39;: `Bearer ${authResponse.accessToken}`}, 51 agent: httpsAgent 52 }).then( response =\u0026gt; { 53 if (response.ok) { 54 console.log(\u0026#39;Successfully add user as assgined role to Azure AD Application\u0026#39;) 55 return response.json(); 56 } else { 57 return Promise.reject(response); 58 } 59 }) 60 }).catch( e =\u0026gt; { 61 console.warn(\u0026#39;User Assignment already exists or error happens\u0026#39;, e) 62 }); 63} 64 65addAzureOpsgenieAppRole(\u0026#39;ulahsieh@abc.com\u0026#39;) ","date":"2022-10-23T18:55:00+00:00","updated":"2022-10-23T18:55:00+00:00"},{"objectID":"1666551120","permalink":"/post/devops-ansible-cannot-resolve-module-action-windows-win-powershell/ansible-coundnot-resolve-module-action/","title":"[Ansible] couldn't resolve module/action 'ansible.windows.win_powershell","content":"問題\r在 alphine 3.13 安裝 ansible 以及相關 ansible 模組後打包 docker image，在執行時 ansible.windows.win_powershell 模組時會報錯\n1ERROR! couldn\u0026#39;t resolve module/action \u0026#39;ansible.windows.win_powershell\u0026#39;. This often indicates a misspelling, missing collection, or incorrect module path. 原因\r原先使用的 docker base image 為公司內部事先包好的 alpine 3.13 版本，以該版為基礎 apk add 安裝的 ansible-core 只到 2.10，其中 ansible.windows collection 並無包含 win_powershell 模組。\n解決方法\r將 docker base image alpine 升級到 3.16，即可解決。\n補充一下，在公司內部網路上無法直接使用 dockerhub 的 alpine，因為沒有 proxy 出不去外網，故須先自己設 proxy\n1FROM alpine:3.16 2 3ENV http_proxy=http://whqproxys.wistron.com:8080 4ENV https_proxy=http://whqproxys.wistron.com:8080 5ENV no_proxy=127.0.0.1,localhost,wistron.com,wistron.com.cn 6 7RUN apk upgrade \u0026amp;\u0026amp; rm -rf /var/cache/apk/* Reference\rhttps://bytemeta.vip/repo/ansible-collections/ansible.windows/issues/282\r","date":"2022-10-23T18:52:00+00:00","updated":"2022-10-23T18:52:00+00:00"},{"objectID":"1663792260","permalink":"/post/devops-terraform-azurerm-key-vault-secret-error-provider-produced-inconsistent-result/terraform-azurerm-key-vault-bug/","title":"[Terraform] azurerm_key_vault_secret Error: Provider produced inconsistent result after apply ","content":"問題描述\r使用 Terraform 的 azurerm provider 創建 key vault secret 時，會出現以下錯誤：\n1│ Error: Provider produced inconsistent result after apply 2│ 3│ When applying changes to azurerm_key_vault_secret.sbcrptnid_srt, provider 4│ \u0026#34;provider[\\\u0026#34;registry.terraform.io/hashicorp/azurerm\\\u0026#34;]\u0026#34; produced an unexpected 5│ new value: Root resource was present, but now absent. 6│ 7│ This is a bug in the provider, which should be reported in the provider\u0026#39;s own 8│ issue tracker. 雖然報錯，但 secret 資源有成功被創建。該結果也可以在第二次 apply 後發現到，會有不同錯誤訊息指出該 resource 已經存在的錯誤：\n1╷ 2│ Error: A resource with the ID \u0026#34;https://kv-srt-test.vault.azure.net/secrets/test/458f9046eeef4ae08a114xxxxxxxxxx\u0026#34; already exists - to be managed via Terraform this resource needs to be imported into the State. Please see the resource documentation for \u0026#34;azurerm_key_vault_secret\u0026#34; for more information. 3│ 4│ with azurerm_key_vault_secret.sbcrptnid_srt, 5│ on main.tf line 5, in resource \u0026#34;azurerm_key_vault_secret\u0026#34; \u0026#34;sbcrptnid_srt\u0026#34;: 6│ 5: resource \u0026#34;azurerm_key_vault_secret\u0026#34; \u0026#34;sbcrptnid_srt\u0026#34; { 7│ 為了完善自動化流程，打算透過 terraform import 的手法將建成功但沒記錄在 terraform state file 的 secret 匯入，但匯入時會報錯：\n1Error: Cannot import non-existent remote object 2│ 3│ While attempting to import an existing object to 4│ \u0026#34;azurerm_key_vault_secret.sbcrptnid_srt\u0026#34;, the provider detected that no object 5│ exists with the given id. Only pre-existing objects can be imported; check that 6│ the id is correct and that it is associated with the provider\u0026#39;s configured 7│ region or endpoint, or use \u0026#34;terraform apply\u0026#34; to create a new remote object for 8│ this resource. 原因\r該錯誤參考 azurerm 的 gitlab issue #11059\r，可以發現到如錯誤訊息所描述的是 provider 的 bug。\n解決方法\r目前 provider 尚未 close 掉該 bug，但可以變相從 terraform import 去更新 state file，解決 apply 都會報錯的問題。參考該 issue 的討論，原本試著用 community 提供的為 key vault 加上 tag 的方式，但失敗，後來在同樣的 issue 下看到解決方式： 用 az account set -s ... 先指定 key vault 的 subscription，接著再下 terraform import，就可以成功匯入 secret 至 state file。\n最後照常下 terraform apply 就都不會報錯了。\n","date":"2022-09-21T20:31:00+00:00","updated":"2022-09-21T20:31:00+00:00"},{"objectID":"1662585180","permalink":"/post/kubernets-basic/","title":"Kubernetes Introduction","content":"簡介\r官方定義:\nKubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.\rKubernetes 又稱為 k8s，最初由 google 用 golang 開發而後釋出的專案。用於操作自動化容器，包括部署，調度和節點集群間擴展。\n架構\r上圖為一個簡易的 Kubernetes Cluster，通常一個 Cluster 中會有多個 Master 作為備援，但為了簡化我們只顯示一個。\n運作方式\r使用者透過 User Command（kubectl）建立 Pod 時經過使用者身份的認證後，再將指令傳遞到 Master Node 中的 API Server，API Server 會把指令備份到 etcd 。接下來 controller-manager 會從 API Server 收到需要創建一個新的 Pod 的訊息，並檢查如果資源許可，就會建立一個新的 Pod。最後 Scheduler 在定期訪問 API Server 時，會詢問 controller-manager 是否有建置新的 Pod，如果發現新建立的 Pod 時，Scheduler 就會負責把 Pod 配送到最適合的一個 Node 上面。\n節點與組件\rMaster Node\r為 Kubernetes 叢集的控制台，負責管理集群、協調所有活動，包含的元件如下：\nAPI Server\rAPI Server 管理 Kubernetes 的所有 api interface，用來和集群中的各節點通訊並進行操作。\nScheduler\rScheduler 是 Pods 調度員，監視新建立但還沒有被指定要跑在哪個 Worker Node 上的 Pod，並根據每個 Node 上面資源去協調出一個最適合放置的對象給該 Pod。\nController Manager\r負責管理並運行 Kubernetes controller 的組件，controller 是許多負責監視 Cluster 狀態的 Process，又可分為下列不同的種類\nNode controller - 負責通知與回應節點的狀態 Replication controller - 負責每個複寫系統內維持設定的 Pod 數量 End-Point controller - 負責端點的服務發布 Service Account \u0026amp; Token controller - 負責創建服務帳戶與新生成的 Namespace 的 API 存取 Token etcd\r用來存放 Kubernetes Cluster 的資料作為備份，當 Master 因為某些原因而故障時，我們可以透過 etcd 幫我們還原 Kubernetes 的狀態。\nWorker Node\r為 Kubernetes 的 runtime 執行環境，包含的元件如下：\nPod\rKubernetes pod 是 Kubernetes 管理的最小單元，裡面包含一個或多個 container，可視為一個應用程式的邏輯主機。 同一個 Pod 中的 Containers 共享相同資源及網路，彼此透過 local port number 溝通。pod 運行在私有隔離的網絡上，默認情況下在同一集群的其他 pod 和 service 中可見，但是外部不可見，需要藉助 service 暴露給外部。\nKubelet\rKubelet 接受 API server 的命令，用來啟動 pod 並監測狀態，確保所有 container 都在運行。它每隔幾秒鐘向 master node 提供一次 heartbeat。如果 replication controller 未收到該消息，則將該節點標記為不正常。\nKube Proxy\r進行網路連線的 forwarding，負責將 request 轉發到正確的 container。\nResource\rhttps://blog.sensu.io/how-kubernetes-works\rhttps://medium.com/@C.W.Hu/kubernetes-basic-concept-tutorial-e033e3504ec0\rhttps://ithelp.ithome.com.tw/articles/10202135\r","date":"2022-09-07T21:13:00+00:00","updated":"2022-09-07T21:13:00+00:00"},{"objectID":"1662499020","permalink":"/post/os-wsl2-ubuntu-nsenter-cannot-open-proc-ns-time/wsl2-ubuntu-nsenter-cannot-open-proc-no-such-file-or-directory/","title":"[WSL2] Ubuntu 22.04 LTS nsenter: cannot open /proc/xxx/ns/time: No such file or directory","content":"問題\r在使用 https://github.com/DamionGans/ubuntu-wsl2-systemd-script.git\r安裝 systemd 後，重啟 wsl，再開 terminal 時會遇到 cannot open /proc/xxx/ns/time: No such file or directory 的錯誤。\n解決方法\r使用 powershell 開啟 wsl\n1wsl -d Ubuntu-22.04 -e bash --norc 修改 enter-systemd-namespace 檔案\n1sudo vi /usr/sbin/enter-systemd-namespace 將下面幾行取代原先的區塊\n1USER_HOME=\u0026#34;$(getent passwd | awk -F: \u0026#39;$1==\u0026#34;\u0026#39;\u0026#34;$SUDO_USER\u0026#34;\u0026#39;\u0026#34; {print $6}\u0026#39;)\u0026#34; 2if [ -n \u0026#34;$SYSTEMD_PID\u0026#34; ] \u0026amp;\u0026amp; [ \u0026#34;$SYSTEMD_PID\u0026#34; != \u0026#34;1\u0026#34; ]; then 3 if [ -n \u0026#34;$1\u0026#34; ] \u0026amp;\u0026amp; [ \u0026#34;$1\u0026#34; != \u0026#34;bash --login\u0026#34; ] \u0026amp;\u0026amp; [ \u0026#34;$1\u0026#34; != \u0026#34;/bin/bash --login\u0026#34; ]; then 4 exec /usr/bin/nsenter -t \u0026#34;$SYSTEMD_PID\u0026#34; -m -p \\ 5 /usr/bin/sudo -H -u \u0026#34;$SUDO_USER\u0026#34; \\ 6 /bin/bash -c \u0026#39;set -a; [ -f \u0026#34;$HOME/.systemd-env\u0026#34; ] \u0026amp;\u0026amp; source \u0026#34;$HOME/.systemd-env\u0026#34;; set +a; exec bash -c \u0026#39;\u0026#34;$(printf \u0026#34;%q\u0026#34; \u0026#34;$@\u0026#34;)\u0026#34; 7 else 8 exec /usr/bin/nsenter -t \u0026#34;$SYSTEMD_PID\u0026#34; -m -p \\ 9 /bin/login -p -f \u0026#34;$SUDO_USER\u0026#34; \\ 10 $([ -f \u0026#34;$USER_HOME/.systemd-env\u0026#34; ] \u0026amp;\u0026amp; /bin/cat \u0026#34;$USER_HOME/.systemd-env\u0026#34; | xargs printf \u0026#39; %q\u0026#39;) 11 fi 12 echo \u0026#34;Existential crisis\u0026#34; 13 exit 1 14fi 退出 powershell terminal 之後再重新使用 wsl terminal 就可以順利進入系統。\n補充\r在每次重開機後，WSL 的 docker 都不會自動被叫起來\n處理方式為： 開啟 powershell，關閉對應 wsl\n1wsl -t Ubuntu-22.04 進入 Docker Desktop，取消對應的 wsl integration 勾選，按 Apply \u0026amp; Restart\n重啟後再重複勾選 wsl integration，並重啟\n回到 powershell，重新啟動 wsl wsl -d Ubuntu-22.04 後，就可以使用 docker 了。\nReference\rhttps://github.com/DamionGans/ubuntu-wsl2-systemd-script/issues/76#issuecomment-1122717349\rhttps://github.com/DamionGans/ubuntu-wsl2-systemd-script/issues/36#issuecomment-732090101\r","date":"2022-09-06T21:17:00+00:00","updated":"2022-09-06T21:17:00+00:00"},{"objectID":"1662498660","permalink":"/post/devops-ansible-winrm-plaintext-the-specified-credential-were-reject-by-server/ansible-winrm-plaintext-credentials-error/","title":"[Ansible] winrm plaintext: the specified credentials were rejected by the server","content":"紀錄 Ansible 使用 Add host 動態新增 Windows 控制節點時，遇到的 winrm 問題及解決方法。\n問題\r在只有配置以下參數的情況下連結遠端 windows server，會出現標題寫的錯誤訊息。\n1ansible_user: user@DOMAIN.COM 2ansible_password: password 3ansible_connection: winrm 4ansible_ssh_port: 5986 解決方式\r確認 remote windows 的 winrm 模組是否有配置 Ansible host 配置 1- name: Add the host 2 add_host: 3 name: win2019 4 ansible_connection: winrm 5 ansible_port: 5985 6 ansible_host: \u0026#34;10.37.39.222\u0026#34; 7 ansible_user: \u0026#34;Administrator\u0026#34; 8 ansible_password: \u0026#34;Passw@ord\u0026#34; 9 ansible_winrm_transport: ntlm 10 ansible_winrm_server_cert_validation: ignore 11 no_log: true ","date":"2022-09-06T21:11:00+00:00","updated":"2022-09-06T21:11:00+00:00"},{"objectID":"1662498420","permalink":"/post/os-powershell-get-acl-drive-not-found/powershell-get-acl-drive-not-found-error/","title":"[PowerShell] Get-Acl Drive Not Found Error","content":"紀錄一下在 windows10 環境下無法正常使用 get-acl 訪問 active directory 網域服務下的資源的問題與解決方式。\n問題描述\r透過在 Azure 上建立一台 Windows (21h2)，並建立 peering 到公司內部的 hub，使這台 VM 可以用公司內部的 IP 連線並加入公司網域，但要透過這台 VM 執行 active directory module 的系列操作時，會遇到以下問題：\n首先在 import ActiveDirectory 時會有以下錯誤 在使用 get-acl cmdlet 時會遇到以下錯誤 Solution\r在 import active directory module 後，針對找不到 drive 的錯誤去新增 PS Drive。 首先在目前環境下使用 Get-PSDrive 查看目前掛載的目錄，的確在目前環境中沒有任何 AD 為命名的 drive。\n用以下方式新增，並切換目錄:\n1New-PSDrive -Name AD -PSProvider ActiveDirectory -Root \u0026#34;OU=CCOE,OU=Security_Group,OU=Group_Object,DC=whq,DC=wistron\u0026#34; -server \u0026#34;hqnhudc1.whq.wistron\u0026#34; 2 3set-location AD:\\ 完成後便可以正常使用 get-acl 指令去取得指定路徑的存取權限：\n1get-acl -path \u0026#39;Microsoft.ActiveDirectory.Management.dll\\ActiveDirectory:://RootDSE/CN=COGTESTAD,OU=CCOE,OU=Security_Group,OU=Group_Object,DC=whq,DC=wistron\u0026#39; 注意 path 後帶的目錄位址不能直接使用 distinguished name，而是完整的路徑\n補充說明\r測試後，該 new drive 的動作獨立於每個 powershell session，所以必須在每次開啟 terminal 時都動作，如果想要在每次開啟後都自動加載的話，請參考 PowerShell Profile 的設定檔設定。\nReference\rhttps://www.reddit.com/r/PowerShell/comments/38o6me/how_do_i_save_a_newpsdrive_psprovider/\r","date":"2022-09-06T21:07:00+00:00","updated":"2022-09-06T21:07:00+00:00"},{"objectID":"1662498120","permalink":"/post/k8s-kustomize-render-configmap-hashing-name-cronjob-resource/kustomization-render-failed-in-cronjob/","title":"Kustomize Can’t Render the ConfigMap Hashing Name to CronJob Resource","content":"問題\r在使用 kustomize 配置 Kubernetes 資源時，Kustomization 定義的 ConfigMap 無法正確的渲染到 CronJob 資源中。 原 yaml 檔如下:\n1# 建立 templatevar 文件 2cat \u0026lt;\u0026lt;EOF \u0026gt;templatevar 3FOO=Bar 4EOF 5 6# 建立 cronjob 文件 7cat \u0026lt;\u0026lt;EOF \u0026gt;cronjob.yaml 8apiVersion: batch/v1 9kind: CronJob 10metadata: 11 name: hello 12 namespace: infrase 13spec: 14 schedule: \u0026#34;25,45,05 * * * *\u0026#34; 15 concurrencyPolicy: Replace 16 jobTemplate: 17 spec: 18 template: 19 spec: 20 containers: 21 - name: hello 22 image: busybox:1.28 23 resources: 24 limits: 25 cpu: \u0026#34;1\u0026#34; 26 memory: 500Mi 27 imagePullPolicy: IfNotPresent 28 securityContext: 29 runAsNonRoot: true 30 runAsUser: 1000 31 allowPrivilegeEscalation: false 32 command: 33 - /bin/sh 34 - -c 35 - date; echo Hello from the Kubernetes cluster \u0026amp;\u0026amp; cat /config/templatevar 36 volumeMounts: 37 - mountPath: /config/ 38 name: templatevar 39 volumes: 40 - name: templatevar 41 configMap: 42 name: templatevar 43 restartPolicy: OnFailure 44EOF 45 46# 建立 kustomization 文件 47cat \u0026lt;\u0026lt;EOF \u0026gt;./kustomization.yaml 48apiVersion: kustomize.config.k8s.io/v1beta1 49kind: Kustomization 50resources: 51 - cronjob.yaml 52 53configMapGenerator: 54- name: templatevar 55 files: 56 - templatevar 57EOF 使用 kustomize 渲染後，可以看到 CronJob 中指定的 ConfigMap 沒有正確吃到 configMapGenerator 所產生的檔案。\n1kubectl kustomize ./ 1apiVersion: v1 2data: 3 templatevar: \u0026#34;FOO=Bar\u0026#34; 4kind: ConfigMap 5metadata: 6 name: templatevar-tk9cdghbt6 7 namespace: infrase 8--- 9apiVersion: batch/v1 10kind: CronJob 11metadata: 12 name: hello 13 namespace: infrase 14spec: 15 jobTemplate: 16 spec: 17 template: 18 spec: 19 containers: 20 - command: 21 - /bin/sh 22 - -c 23 - date; echo Hello from the Kubernetes cluster \u0026amp;\u0026amp; cat /config/templatevar 24 image: busybox:1.28 25 imagePullPolicy: IfNotPresent 26 name: hello 27 securityContext: 28 allowPrivilegeEscalation: false 29 runAsNonRoot: true 30 runAsUser: 1000 31 volumeMounts: 32 - mountPath: /config/ 33 name: templatevar 34 imagePullSecrets: 35 - name: hbsrt 36 restartPolicy: OnFailure 37 volumes: 38 - configMap: 39 name: templatevar 40 name: templatevar 41 schedule: 57 * * * * 解決方式\r需要在 kustomiztion 檔案中指定 namespace\n1apiVersion: kustomize.config.k8s.io/v1beta1 2kind: Kustomization 3namespace: infrase 4resources: 5 - cronjob.yaml 6 7configMapGenerator: 8- name: templatevar 9 files: 10 - templatevar Reference\rhttps://github.com/kubernetes-sigs/kustomize/issues/1301\r","date":"2022-09-06T21:02:00+00:00","updated":"2022-09-06T21:02:00+00:00"},{"objectID":"1655133180","permalink":"/post/win10-plink-automate-ssh-copy-id/","title":"[Windows] 用 plink 自動化 ssh-copy-id 到指定機器","content":"研究怎麼不透過人為輸入密碼，將本地主機上的公鑰上傳到遠端主機的 authorized_keys 文件中，以實現 SSH 金鑰驗證。\n1cat id_rsa.pub | plink -ssh root@10.37.39.69 -pwfile pw.txt \u0026#34;cat \u0026gt;\u0026gt; ~/.ssh/authorized_keys\u0026#34; Info\nPlink 是由 PuTTY 套件提供的命令行工具，多被使用在自動執行的場景應用，可在自動化部署執行 SSH 的指令。\n","date":"2022-06-13T15:13:00+00:00","updated":"2022-06-13T15:13:00+00:00"},{"objectID":"1653905220","permalink":"/post/devops-azure-iot-hub-to-nodered/azure-iot-hub-to-nodered/","title":"Azure IoT Hub node-red 實做","content":"本篇記錄如何使用 Node-Red 實作 Azure IoT Hub 的資料傳輸。\n安裝 Node-red\r在雲端以及地端兩邊都準備好 node-red\n1# 安裝 nodejs \u0026amp; npm 2$ sudo apt update 3$ sudo apt install nodejs -y 4$ sudo apt install npm -y 5$ sudo npm install npm@6.14.0 -g 6$ sudo npm cache clean -f 7$ sudo npm install -g n 8$ sudo n 10.22.0 stable 9 10# 安裝 node-red 11$ sudo npm install -g --unsafe-perm node-red 12 13# 啟動 node-red 14$ node-red 建立 IoT Hub\r在 azure 建立一個 IoT Hub\n安裝 Node-Red Node\r在雲端以及地端的機器中的 manage palette 安裝 node-red-contrib-azure-iot-hub\n準備Local Node-red 的 workflow\r註冊 Device\r需要將機器註冊到 Azure IoT Hub 中，準備了下面的 workflow\n1[{\u0026#34;id\u0026#34;:\u0026#34;8420253b.d369c8\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;tab\u0026#34;,\u0026#34;label\u0026#34;:\u0026#34;Flow 1\u0026#34;,\u0026#34;disabled\u0026#34;:false,\u0026#34;info\u0026#34;:\u0026#34;\u0026#34;},{\u0026#34;id\u0026#34;:\u0026#34;a937e034.afa3c\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;azureiothubregistry\u0026#34;,\u0026#34;z\u0026#34;:\u0026#34;8420253b.d369c8\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;Azure IoT Hub Registry\u0026#34;,\u0026#34;x\u0026#34;:410,\u0026#34;y\u0026#34;:180,\u0026#34;wires\u0026#34;:[[\u0026#34;b7a4e98a.e50b18\u0026#34;]]},{\u0026#34;id\u0026#34;:\u0026#34;dbdd68fc.579708\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;inject\u0026#34;,\u0026#34;z\u0026#34;:\u0026#34;8420253b.d369c8\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;Register Payload\u0026#34;,\u0026#34;props\u0026#34;:[{\u0026#34;p\u0026#34;:\u0026#34;payload\u0026#34;}],\u0026#34;repeat\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;crontab\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;once\u0026#34;:false,\u0026#34;onceDelay\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;topic\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;payload\u0026#34;:\u0026#34;{\\\u0026#34;deviceId\\\u0026#34;:\\\u0026#34;hello\\\u0026#34;}\u0026#34;,\u0026#34;payloadType\u0026#34;:\u0026#34;json\u0026#34;,\u0026#34;x\u0026#34;:180,\u0026#34;y\u0026#34;:180,\u0026#34;wires\u0026#34;:[[\u0026#34;a937e034.afa3c\u0026#34;]]},{\u0026#34;id\u0026#34;:\u0026#34;b7a4e98a.e50b18\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;debug\u0026#34;,\u0026#34;z\u0026#34;:\u0026#34;8420253b.d369c8\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;Log\u0026#34;,\u0026#34;active\u0026#34;:true,\u0026#34;console\u0026#34;:\u0026#34;false\u0026#34;,\u0026#34;complete\u0026#34;:\u0026#34;true\u0026#34;,\u0026#34;x\u0026#34;:650,\u0026#34;y\u0026#34;:180,\u0026#34;wires\u0026#34;:[]}] 在 Azure IoT Hub Registry node 中貼上 IoT Hub 的 connectionString (primary 或 secondary 皆可行)\n在 inject node 中設定 device id\nDeploy 後按下 inject，便註冊 device 成功了，可以看到在 debug window 中回傳了該 device 的金鑰。\n發送資料\r註冊完機器後，開始準備丟資料，使用以下的 workflow\n1[{\u0026#34;id\u0026#34;:\u0026#34;7db92193.aed53\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;tab\u0026#34;,\u0026#34;label\u0026#34;:\u0026#34;Flow 2\u0026#34;,\u0026#34;disabled\u0026#34;:false,\u0026#34;info\u0026#34;:\u0026#34;\u0026#34;},{\u0026#34;id\u0026#34;:\u0026#34;897ffdd4.12ba3\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;debug\u0026#34;,\u0026#34;z\u0026#34;:\u0026#34;7db92193.aed53\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;Log\u0026#34;,\u0026#34;active\u0026#34;:true,\u0026#34;console\u0026#34;:\u0026#34;false\u0026#34;,\u0026#34;complete\u0026#34;:\u0026#34;true\u0026#34;,\u0026#34;x\u0026#34;:870,\u0026#34;y\u0026#34;:200,\u0026#34;wires\u0026#34;:[]},{\u0026#34;id\u0026#34;:\u0026#34;66c43e6.4f5f7c\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;azureiothub\u0026#34;,\u0026#34;z\u0026#34;:\u0026#34;7db92193.aed53\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;Azure IoT Hub\u0026#34;,\u0026#34;protocol\u0026#34;:\u0026#34;http\u0026#34;,\u0026#34;x\u0026#34;:660,\u0026#34;y\u0026#34;:200,\u0026#34;wires\u0026#34;:[[\u0026#34;897ffdd4.12ba3\u0026#34;]]},{\u0026#34;id\u0026#34;:\u0026#34;c5686695.31d5e8\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;inject\u0026#34;,\u0026#34;z\u0026#34;:\u0026#34;7db92193.aed53\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;Send Payload\u0026#34;,\u0026#34;props\u0026#34;:[],\u0026#34;repeat\u0026#34;:\u0026#34;5\u0026#34;,\u0026#34;crontab\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;once\u0026#34;:false,\u0026#34;onceDelay\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;topic\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;x\u0026#34;:200,\u0026#34;y\u0026#34;:200,\u0026#34;wires\u0026#34;:[[\u0026#34;84141515.9a1188\u0026#34;]]},{\u0026#34;id\u0026#34;:\u0026#34;84141515.9a1188\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;function\u0026#34;,\u0026#34;z\u0026#34;:\u0026#34;7db92193.aed53\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;func\u0026#34;:\u0026#34;let a = Math.floor(Math.random()*100)+1;\\nmsg.payload={\\n \\\u0026#34;deviceId\\\u0026#34;:\\\u0026#34;device1\\\u0026#34;,\\n \\\u0026#34;key\\\u0026#34;:\\\u0026#34;pOX7pNKnt2aJoTy5JX4BYXidsezO+fr1sEz0TQQt1YM=\\\u0026#34;,\\n \\\u0026#34;protocol\\\u0026#34;:\\\u0026#34;http\\\u0026#34;,\\n \\\u0026#34;data\\\u0026#34;: a\\n}\\nreturn msg;\u0026#34;,\u0026#34;outputs\u0026#34;:1,\u0026#34;noerr\u0026#34;:0,\u0026#34;initialize\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;finalize\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;x\u0026#34;:420,\u0026#34;y\u0026#34;:200,\u0026#34;wires\u0026#34;:[[\u0026#34;66c43e6.4f5f7c\u0026#34;]]}] 在 Azure IoT Hub node 中設定 IoT Hub 的 hostname，並指定想要使用的 protocol\n並在 msg.payload 中以 JSON 的形式指定下面訊息：\n1msg.payload={ 2 \u0026#34;deviceId\u0026#34;:\u0026#34;device1\u0026#34;, 3 \u0026#34;key\u0026#34;:\u0026#34;pOX7pNKnt2aJoTy5JX4BYXidsezO+fr1sEz0TQQt1YM=\u0026#34;, 4 \u0026#34;protocol\u0026#34;:\u0026#34;http\u0026#34;, 5 \u0026#34;data\u0026#34;: data 6} 按下 Deploy 後便可以看到資料已經成功送出\n準備 Cloud Node-red 的 workflow\r為了將收到的資料呈現在 dashboard，在 manage palette 中安裝了 node-red-dashboard。\n準備　Azure IoT Hub Receiver 接收IoT Hub 中的資料，並接上 chart node，以 line chart 的方式將收到的資料呈現在 dashboard 上。\n1[{\u0026#34;id\u0026#34;:\u0026#34;abefc0d0.8ba21\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;azureiothubreceiver\u0026#34;,\u0026#34;z\u0026#34;:\u0026#34;3664d073.9332c\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;Azure IoT Hub Receiver\u0026#34;,\u0026#34;x\u0026#34;:360,\u0026#34;y\u0026#34;:180,\u0026#34;wires\u0026#34;:[[\u0026#34;ac07fe9f.05a0c\u0026#34;]]},{\u0026#34;id\u0026#34;:\u0026#34;935ce90f.e63358\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;debug\u0026#34;,\u0026#34;z\u0026#34;:\u0026#34;3664d073.9332c\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;Log\u0026#34;,\u0026#34;active\u0026#34;:true,\u0026#34;tosidebar\u0026#34;:true,\u0026#34;console\u0026#34;:false,\u0026#34;tostatus\u0026#34;:false,\u0026#34;complete\u0026#34;:\u0026#34;payload\u0026#34;,\u0026#34;targetType\u0026#34;:\u0026#34;msg\u0026#34;,\u0026#34;statusVal\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;statusType\u0026#34;:\u0026#34;auto\u0026#34;,\u0026#34;x\u0026#34;:810,\u0026#34;y\u0026#34;:180,\u0026#34;wires\u0026#34;:[]},{\u0026#34;id\u0026#34;:\u0026#34;ac07fe9f.05a0c\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;ui_chart\u0026#34;,\u0026#34;z\u0026#34;:\u0026#34;3664d073.9332c\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;group\u0026#34;:\u0026#34;9f102f5f.29f03\u0026#34;,\u0026#34;order\u0026#34;:0,\u0026#34;width\u0026#34;:\u0026#34;0\u0026#34;,\u0026#34;height\u0026#34;:\u0026#34;0\u0026#34;,\u0026#34;label\u0026#34;:\u0026#34;chart2\u0026#34;,\u0026#34;chartType\u0026#34;:\u0026#34;line\u0026#34;,\u0026#34;legend\u0026#34;:\u0026#34;false\u0026#34;,\u0026#34;xformat\u0026#34;:\u0026#34;HH:mm:ss\u0026#34;,\u0026#34;interpolate\u0026#34;:\u0026#34;linear\u0026#34;,\u0026#34;nodata\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;dot\u0026#34;:false,\u0026#34;ymin\u0026#34;:\u0026#34;0\u0026#34;,\u0026#34;ymax\u0026#34;:\u0026#34;100\u0026#34;,\u0026#34;removeOlder\u0026#34;:1,\u0026#34;removeOlderPoints\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;removeOlderUnit\u0026#34;:\u0026#34;3600\u0026#34;,\u0026#34;cutout\u0026#34;:0,\u0026#34;useOneColor\u0026#34;:false,\u0026#34;useUTC\u0026#34;:false,\u0026#34;colors\u0026#34;:[\u0026#34;#1f77b4\u0026#34;,\u0026#34;#aec7e8\u0026#34;,\u0026#34;#ff7f0e\u0026#34;,\u0026#34;#2ca02c\u0026#34;,\u0026#34;#98df8a\u0026#34;,\u0026#34;#d62728\u0026#34;,\u0026#34;#ff9896\u0026#34;,\u0026#34;#9467bd\u0026#34;,\u0026#34;#c5b0d5\u0026#34;],\u0026#34;useOldStyle\u0026#34;:false,\u0026#34;outputs\u0026#34;:1,\u0026#34;x\u0026#34;:610,\u0026#34;y\u0026#34;:180,\u0026#34;wires\u0026#34;:[[\u0026#34;935ce90f.e63358\u0026#34;]]},{\u0026#34;id\u0026#34;:\u0026#34;9f102f5f.29f03\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;ui_group\u0026#34;,\u0026#34;z\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;Default\u0026#34;,\u0026#34;tab\u0026#34;:\u0026#34;a79849a0.efb138\u0026#34;,\u0026#34;order\u0026#34;:1,\u0026#34;disp\u0026#34;:true,\u0026#34;width\u0026#34;:\u0026#34;15\u0026#34;,\u0026#34;collapse\u0026#34;:false},{\u0026#34;id\u0026#34;:\u0026#34;a79849a0.efb138\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;ui_tab\u0026#34;,\u0026#34;z\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;Home\u0026#34;,\u0026#34;icon\u0026#34;:\u0026#34;dashboard\u0026#34;,\u0026#34;disabled\u0026#34;:false,\u0026#34;hidden\u0026#34;:false}] Deploy 之後可以看到 dashboard 成功呈現。\n","date":"2022-05-30T10:07:00+00:00","updated":"2022-05-30T10:07:00+00:00"},{"objectID":"1653692400","permalink":"/post/coldplay-speed-of-sound/coldplay-speed-of-sound/","title":"[Coldplay] Speed of Sound","content":"How long before I get in,\nbefore it starts\nbefore I begin\n還要多久 我才能到達呢\n還要多久 這一切才開始呢\n還要多久 我才能開始呢\nHow long before you decide\nbefore I know what it feels like\n還要多久 你才能決定呢\n還要多久 我才能知道這什麼感覺呢\nWhere to\nwhere do I go?\nif you never try\nthen you\u0026rsquo;ll never know\n哪裡呢\n我該去哪裡呢\n但你若不嘗試 就永遠不會知道\nHow long do I have to climb\nup on the side of this mountain of mine\n我還需要爬多久才能爬上自己人生的巔峰\nLook up\nI look up at night\nplanets are moving at the speed of light\n往上看\n我在夜裡仰望星空\n星星們在光速飛行\nClimb up\nup in the trees\nevery chance that you get\nis a chance you seize\n向上爬\n往樹上爬\n你得到的每個機會都是你可以抓住的!\nHow long am I gonna stand\nwith my head stuck under the sand？\nI\u0026rsquo;ll start before I couldn\u0026rsquo;t stop\nor before I see things in the right way up\n我還要忍受逃避現實多久\n我會在我無法忍受前開始\n或在我以正確的方式看清事物前開始\nAll that noise and all that sound\nall those places I got found\nand birds go flying at the speed of sound\nto show you how it all began\n突然我意識到所有那些噪音、聲音、地方\n以及剎那飛過的群鳥\n都只爲啓示\n一切如此開端\nbirds came flying from the underground\nif you could see it then you\u0026rsquo;ll understand\n群鳥從地下飛起\n當你看見這一幕時你也會醒悟\nideas that you\u0026rsquo;ll never find\nall the inventors could never design\nthe buildings that you put up\nJapan and China all lit up\n有些道理永遠想不通\n有些設計發明家也做不到\n而你的創造\n把中國和日本都照亮\nthe sign that I couldn\u0026rsquo;t read\nor a light that I couldn\u0026rsquo;t see\nsome things you have to believe\nbut others are puzzles\nare puzzles of me\n有些跡象我無法解讀\n或我無法看見的光芒\n有些東西你必須相信\n但其他謎題就只是謎題\nall that noise and all that sound\nall those places I got found\nand birds go flying at the speed of sound\nto show you how it all began\n突然我意識到所有那些噪音、聲音、地方\n以及剎那飛過的群鳥\n都只爲啓示\n一切如此開端\nbirds came flying from the underground\nif you could see it then you\u0026rsquo;ll understand\noh, when you see it then you\u0026rsquo;ll understand\n當鳥群從地下飛出\n你看見就懂了\n你看見就懂了\nall those signs I knew what they meant\nsome things you can\u0026rsquo;t invent\nsome get made\nsome get sent\n我明白了所有的跡象\n有些東西你無法發明\n一些就是這麼做出來的\n另一些是被外星人送來的 XD\nand birds go flying at the speed of sound\nto show you how it all began\nbirds came flying from the underground\nIf you could see it then you\u0026rsquo;ll understand\nOh, when you see it then you\u0026rsquo;ll understand\n鳥剎那間飛過\n都只爲啓示\n一切如此開端\n當鳥群從地下飛出\n你一看見就懂了\n我想 當你看見時你也會懂的~\n","date":"2022-05-27T23:00:00+00:00","updated":"2022-05-27T23:00:00+00:00"},{"objectID":"1653254220","permalink":"/post/k8s-helm-migrate-stable-nginx-ingress-to-ingress-nginx/helm-migrate-stable-nginx-ingress-to-ingress-nginx/","title":"Helm Migrate stable/nginx-ingress to ingress-nginx","content":"前言\r原先集群使用的 helm chart 為 stable/nginx-ingress，而此 helm chart 已經被棄用，若 nginx 維持在舊版的話，之後新的漏洞修補都無法被含括。\n此篇記錄如何將集群上面跑的 nginx-ingress-controller 換成新的版本的 chart ingress-nginx/ingress-nginx。\nCurrent stable/nginx-ingress\r原先 nginx-ingress 使用的版本\n1$ kubectl exec -it -n nginx-ingress nginx-ingress-controller-585bb7f5b4-2nlzz -- /nginx-ingress-controller 2------------------------------------------------------------------------------- 3NGINX Ingress controller 4 Release: v0.34.1 5 Build: v20200715-ingress-nginx-2.11.0-8-gda5fa45e2 6 Repository: https://github.com/kubernetes/ingress-nginx 7 nginx version: nginx/1.19.1 8------------------------------------------------------------------------------- 查詢該 helm chart 有沒有更新的版本可直接更新，但結果如下，目前集群安裝的已經是該 chart 的最新版本了，且已標示 deprecated 不會再維護。\n1$ helm search repo stable/nginx-ingress --versions 2NAME CHART VERSION APP VERSION DESCRIPTION 3stable/nginx-ingress 1.41.3 v0.34.1 DEPRECATED! An nginx Ingress controller that us... 4stable/nginx-ingress 1.41.2 v0.34.1 An nginx Ingress controller that uses ConfigMap... 5stable/nginx-ingress 1.41.1 v0.34.1 An nginx Ingress controller that uses ConfigMap... 6stable/nginx-ingress 1.41.0 v0.34.0 An nginx Ingress controller that uses ConfigMap... 7stable/nginx-ingress 1.40.3 0.32.0 An nginx Ingress controller that uses ConfigMap... 8stable/nginx-ingress 1.40.2 0.32.0 An nginx Ingress controller that uses ConfigMap... 9stable/nginx-ingress 1.40.1 0.32.0 An nginx Ingress Install ingress-nginx/ingress-nginx\r安裝operator\n1$ helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx 2$ helm repo update 3$ kubectl create ns ingress-nginx 4$ helm install ingress-nginx ingress-nginx/ingress-nginx --namespace ingress-nginx 完成後應該會看到以下輸出\n1[root@master1 ~]# helm install ingress-nginx ingress-nginx/ingress-nginx 2helm install ingress-nginx ingress-nginx/ingress-nginx 3NAME: ingress-nginx 4LAST DEPLOYED: Wed Aug 18 13:41:42 2021 5NAMESPACE: default 6STATUS: deployed 7REVISION: 1 8TEST SUITE: None 9NOTES: 10The ingress-nginx controller has been installed. 11It may take a few minutes for the LoadBalancer IP to be available. 12You can watch the status by running \u0026#39;kubectl --namespace default get services -o wide -w ingress-nginx-controller\u0026#39; 13 14An example Ingress that makes use of the controller: 15 16 apiVersion: networking.k8s.io/v1beta1 17 kind: Ingress 18 metadata: 19 annotations: 20 kubernetes.io/ingress.class: nginx 21 name: example 22 namespace: foo 23 spec: 24 rules: 25 - host: www.example.com 26 http: 27 paths: 28 - backend: 29 serviceName: exampleService 30 servicePort: 80 31 path: / 32 # This section is only required if TLS is to be enabled for the Ingress 33 tls: 34 - hosts: 35 - www.example.com 36 secretName: example-tls 37 38If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided: 39 40 apiVersion: v1 41 kind: Secret 42 metadata: 43 name: example-tls 44 namespace: foo 45 data: 46 tls.crt: \u0026lt;base64 encoded cert\u0026gt; 47 tls.key: \u0026lt;base64 encoded key\u0026gt; 48 type: kubernetes.io/tls 驗證安裝\r1$ kubectl exec -it -n ingress-nginx ingress-nginx-controller-b65df6fbb-jx4tf -- /nginx-ingress-controller --version 2------------------------------------------------------------------------------- 3NGINX Ingress controller 4 Release: v0.48.1 5 Build: 30809c066cd027079cbb32dccc8a101d6fbffdcb 6 Repository: https://github.com/kubernetes/ingress-nginx 7 nginx version: nginx/1.20.1 8 9------------------------------------------------------------------------------- Create ingress resource\r準備 ingress resource 的 yaml 檔，請注意雖然上方的安裝成功的訊息有示範 ingress 的 yaml，但因為 networking.k8s.io/v1beta1\r已在 Kubernetes 1.19+ 被棄用，如果維持使用，則會遇到 Warning: [networking.k8s.io/v1beta1](http://networking.k8s.io/v1beta1) Ingress is deprecated in v1.19+, unavailable in v1.22+; use [networking.k8s.io/v1](http://networking.k8s.io/v1) Ingress的錯誤，所以參考ingress-nginx(\rhttps://kubernetes.github.io/ingress-nginx/user-guide/basic-usage/\r) 的官網，改成以下格式：\n1apiVersion: networking.k8s.io/v1 2kind: Ingress 3metadata: 4 name: ingress-myservicea 5 annotations: 6 # use the shared ingress-nginx 7 kubernetes.io/ingress.class: \u0026#34;nginx\u0026#34; 8spec: 9 rules: 10 - host: myservicea.foo.org 11 http: 12 paths: 13 - path: / 14 pathType: Prefix 15 backend: 16 service: 17 name: myservicea 18 port: 19 number: 80 20--- 21apiVersion: networking.k8s.io/v1 22kind: Ingress 23metadata: 24 name: ingress-myserviceb 25 annotations: 26 # use the shared ingress-nginx 27 kubernetes.io/ingress.class: \u0026#34;nginx\u0026#34; 28spec: 29 rules: 30 - host: myserviceb.foo.org 31 http: 32 paths: 33 - path: / 34 pathType: Prefix 35 backend: 36 service: 37 name: myserviceb 38 port: 39 number: 80 刪除舊的 ingress controller\r確認所有流量都已經導到新的 controller 後，就可以把舊的 stable/nginx-ingress 的 controller 刪掉了。\n1$ helm uninstall nginx-ingress Reference\rhttps://jfrog.com/blog/migrate-nginx-from-stable-helm-charts-with-chartcenter/\r補充：如果要實現 zero-downtime 的部屬，可以參考這篇文章 https://medium.com/codecademy-engineering/kubernetes-nginx-and-zero-downtime-in-production-2c910c6a5ed8\r","date":"2022-05-22T21:17:00+00:00","updated":"2022-05-22T21:17:00+00:00"},{"objectID":"1653051600","permalink":"/post/coldplay-hymn-for-the-weekend/","title":"[Coldplay] Hymn For The Weekend","content":"A drink for me, drink for me\nOh-ah-oh-ah\nThen we\u0026rsquo;ll shoot across the sky\nSymphony\nThen we\u0026rsquo;ll shoot across the sky\nWe\u0026rsquo;re on a\u0026hellip;\nA drink for me, drink for me\n(Oh-ah-oh-ah)\nThen we\u0026rsquo;ll shoot across the sky\nSymphony\n(So high, so high)\nThen we\u0026rsquo;ll shoot across the sky\nOh, angels sent from up above 天使從天而降\nYou know you make my world light up 你的出現讓我的世界充滿了光芒\nWhen I was down, when I was hurt 當我失落，當我受傷\nYou came to lift me up 你來振奮我起來\nLife is a drink, and love\u0026rsquo;s a drug 生命就像一杯酒，愛就像一種毒品\nOh now I think I must be miles up 我想我現在已經飄飄欲仙了\nWhen I was a river, dried up 當我變成乾枯的河流\nYou came to rain a flood 你帶來了洪水\nSo drink for me, drink for me 所以為我乾杯，為我乾杯\nWhen I was so thirsty 當我口渴時\nWe\u0026rsquo;re on a symphony 我們在一場交響樂中\nNow I just can\u0026rsquo;t get enough 現在我已經無法自拔\nPut your wings on me, wings on me 把你的翅膀放在我身上吧\nWhen I was so heavy 當我如此沉重時\nWe\u0026rsquo;re on a symphony 我們在一場交響樂\nWhen I\u0026rsquo;m lower, lower, lower, low 當我更低落、更低落、更低落時\nAh-oh-ah-oh-ah\nGot me feeling drunk and high\nSo high, so high\nOh-ah-oh-ah-oh-ah\nI\u0026rsquo;m feeling drunk and high\nSo high, so high\n(Woo)\n(Woo-ooo-ooo-woo)\nOh, angels sent from up above 天使從天而降\nI feel it coursing through my blood 我覺得全身血液正沸騰中\nLife is a drink, your love\u0026rsquo;s about 生命是杯酒\nTo make the stars come out 你的愛是讓星星都閃耀\nPut your wings on me, wings on me\nWhen I was so heavy\nWe\u0026rsquo;re on a symphony\nWhen I\u0026rsquo;m lower, lower, lower, low\nAh-oh-ah-oh-ah\nGot me feeling drunk and high\nSo high, so high\nOh-ah-oh-ah-oh-ah\nI\u0026rsquo;m feeling drunk and high\nSo high, so high\nAh-oh-ah-oh-ah\nLa, la, la, la, la, la, la\nSo high, so high\nAh-oh-ah-oh-ah\nI\u0026rsquo;m feeling drunk and high\nSo high, so high\nThen we\u0026rsquo;ll shoot across the sky\nThen we\u0026rsquo;ll shoot across the\u0026hellip;\nThen we\u0026rsquo;ll shoot across the sky\nThen we\u0026rsquo;ll shoot across the\u0026hellip;\nThen we\u0026rsquo;ll shoot across the sky\nThen we\u0026rsquo;ll shoot across the\u0026hellip;\nThen we\u0026rsquo;ll shoot across the sky\nThen we\u0026rsquo;ll shoot across the\u0026hellip;\nThen we\u0026rsquo;ll shoot across the sky\nThen we\u0026rsquo;ll shoot across the\u0026hellip;\n","date":"2022-05-20T13:00:00+00:00","updated":"2022-05-20T13:00:00+00:00"},{"objectID":"1652965320","permalink":"/post/ansible-get-fact-to-target-nic/","title":"[Ansible] get_fact ipv4 address 取到 eth0","content":"TL; DR\rAnsible 的控制節點以及受控節點皆使用 VM 打起來，在控制節點使用 get-facts 要取得受控節點的 IP 資料時，總是會取得第一張網卡 eth0，但目標是要取得第二張網卡。\n1$ ifconfig 2eth0 Link encap:Ethernet HWaddr 08:00:27:84:06:a3 3 inet addr:10.0.2.15 Bcast:10.0.2.255 Mask:255.255.255.0 4 inet6 addr: fe80::a00:27ff:fe84:6a3/64 Scope:Link 5 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 6 RX packets:1922 errors:0 dropped:0 overruns:0 frame:0 7 TX packets:1247 errors:0 dropped:0 overruns:0 carrier:0 8 collisions:0 txqueuelen:1000 9 RX bytes:191345 (191.3 KB) TX bytes:163962 (163.9 KB) 10 11eth1 Link encap:Ethernet HWaddr 08:00:27:fc:bb:de 12 inet addr:192.168.56.10 Bcast:192.168.56.255 Mask:255.255.255.0 13 inet6 addr: fe80::a00:27ff:fefc:bbde/64 Scope:Link 14 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 15 RX packets:132 errors:0 dropped:0 overruns:0 frame:0 16 TX packets:91 errors:0 dropped:0 overruns:0 carrier:0 17 collisions:0 txqueuelen:1000 18 RX bytes:109552 (109.5 KB) TX bytes:23202 (23.2 KB) 解決方式\r在系統的路由表中添加一個靜態路由，該路由指定將流量發送到 8.8.8.8（Google DNS 服務器）的接口為 eth1。這樣，當系統嘗試訪問 8.8.8.8 時，將使用指定的接口進行通信。\n1route add -net 8.8.8.8 netmask 255.255.255.255 eth1 Reference\rhttps://superuser.com/questions/991711/vagrant-virtualbox-doesnt-setup-correct-gateway\r","date":"2022-05-19T13:02:00+00:00","updated":"2022-05-19T13:02:00+00:00"},{"objectID":"1652960160","permalink":"/post/k8s-delete-worker-node/kubernetes-delete-worker-node/","title":"[Kubernetes] 停止調度 / 刪除節點","content":"cordon、drain 和 delete 三個命令都會使 kubernetes node 停止被調度，本篇記錄如何優雅的刪除節點。\nDrain the Node\r使用 kubectl drain 將 Node 狀態變更為維護模式，該 Node 上面的 Pod 就會轉移到其他 Node 上。\n1kubectl drain worker3 --ignore-daemonsets --delete-local-data kubectl drain 操作會將指定節點上的 Pod 刪除，並在可調度節點上面起一個對應的 Pod。當舊 Pod沒有被正常刪除的情況下，新 Pod 不會起來。例如舊 Pod 一直處於Terminating 狀態。所以可以強制刪除該 Pod。\n1kubectl get pod all -A -o wide | grep worker3 2 3kubectl delete pods -n my-kafka-project my-cluster-zookeeper-2 --force Delete the Node\r1kubectl delete nodes worker3 delete 是一種暴力刪除 node 的方式，會強制關閉容器進程以驅逐 pod。 基于 node 的\r自註冊\r功能，恢復調度則重啟 kubelet 服務即可。\n1systemctl restart kubelet Drain v.s Cordon\rcordon 停止調度（不可調度，從 K8S 集群隔離）\r只會將 node 標識為SchedulingDisabled 不可調度狀態。新創建的資源，不會被調度到該節點。而舊有的 pod 不會受到影響，仍正常對外提供服務。\n1# 禁止調度 2kubectl cordon \u0026lt;nodeName\u0026gt; 3# 恢復調度 4kubectl uncordon \u0026lt;nodeName\u0026gt; drain 驅逐節點（先不可調度，然後排乾 Pod）\r會驅逐 Node 上的 pod 資源到其他節點重新創建。接著，將節點調為 SchedulingDisabled 不可調度狀態。\n1# 禁止調度 2kubectl drain \u0026lt;nodeName\u0026gt; --force --ignore-daemonsets --delete-local-data 3# 恢復調度 4kubectl uncordon \u0026lt;nodeName\u0026gt; --force 當一些 pod 不是經ReplicationController, ReplicaSet, Job, DaemonSet 或者 StatefulSet 管理的時候就需要用 \u0026ndash;force 來強制執行(例如 kube-proxy) --ignore-daemonsets 無視DaemonSet 管理下的 Pod。因為deamonset 會忽略 unschedulable 標籤，因此 deamonset 控制器控制的 pod 被刪除後可能馬上又在此節點上啟動起來，這樣就會成為死循環，因此這裡忽略daemonset。 --delete-local-data 如果有 mount local volumn 的 pod，會強制殺掉該 pod。 drain 驅逐流程：先在 Node 節點優雅關閉並刪除 pod，然後再在其他 Node 節點創建該 pod。所以為了確保 drain 驅逐 pod 過程中不中斷服務，必須保證要驅逐的 pod 副本數大於 1，並且採用了 anti-affinity 將這些 pod 調度到不同的 Node 節點上。 Reference\rhttps://www.cnblogs.com/kevingrace/p/14412254.html\r","date":"2022-05-19T11:36:00+00:00","updated":"2022-05-19T11:36:00+00:00"},{"objectID":"1652958900","permalink":"/post/database-mongodb-update-the-document/mongodb-update-new-field-to-old-document/","title":"MongoDB 一次為舊有資料加上新增欄位","content":"1db.collection1.update({\u0026#34;cameras\u0026#34;:{$exists:false}}, {$set:{\u0026#34;cameras\u0026#34;:{}}},false,true) 最後兩個 false, true 參數個別代表：\nUpsert: If set to true, creates a new document when no document matches the query criteria. Multi: If set to true, updates multiple documents that meet the query criteria. If set to false, updates one document. ","date":"2022-05-19T11:15:00+00:00","updated":"2022-05-19T11:15:00+00:00"},{"objectID":"1652951460","permalink":"/post/k8s-add-tls-to-istio-gw/add-https-to-istio-gw/","title":"為對外的 istio gateway 加上 https","content":"本篇文章記錄怎麼使用 cert-manager 為對外的 istio gateway 加上 https。\n憑證分類\r自簽憑證：某些不需要被公開存取、但希望達到資料傳輸能加密的內部服務，可以使用自簽憑證，Client 去存取的時候自己帶上 CA 憑證去驗證即可，例如 HashiCorp Vault, AWS RDS TLS 連線\u0026amp;hellip;等。\n第三方 CA 機構簽發憑證：如果是公開的網路服務，就必須透過正規的 CA 機構來簽發，如需要收費的 Digicert, SSL.com, Symantec\u0026amp;hellip;等，或是免費的 Let’s Encrypt。\ncert-manager\rcert-manager 是基於 Kubernetes 所開發的憑證管理工具，它可以可以幫忙發出來自各家的 TLS 憑證，例如上面所提到的 ACME (Let’s Encrypt), HashiCorp Vault, Venafi 或是自己簽發的憑證，而且它還可以確保 TLS 憑證一直維持在有效期限內。\nAbove Reference\rInstall\r1$ kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.4.0/cert-manager.yaml 2$ kubectl get pods --namespace cert-manager 3 NAME READY STATUS RESTARTS AGE 4 cert-manager-5c6866597-zw7kh 1/1 Running 0 2m 5 cert-manager-cainjector-577f6d9fd7-tr77l 1/1 Running 0 2m 6 cert-manager-webhook-787858fcdb-nlzsq 1/1 Running 0 2m Issuer\rIssuer 用來頒發憑證，分為兩種資源類型：\nissuer：只作用於特定 namespace ClusterIssuer：作用於整個 k8s 集群 cert-manager 有支援幾種的 issuer type：\nCA: 使用 x509 keypair 產生 certificate，存在 kubernetes secret Self …","date":"2022-05-19T09:11:00+00:00","updated":"2022-05-19T09:11:00+00:00"},{"objectID":"1652950680","permalink":"/post/devops-terraform-helm-release-set-array/terraform-helm-release-pass-array/","title":"[Terraform] helm_release Pass Array/List to Set's Value","content":"在 terraform 使用 helm_release，在傳入 set 參數時遇到有 chart 指定要 array 型態的值，但使用 [] 傳入 string array 卻會報無法 iterate array。在這邊紀錄一下解法。\nmetrics-server helm chart 描述\r這次使用 Terraform 實做安裝的 helm chart 是 metrics-server，\r官網\r提到可以使用 args 傳入啟動 server 的額外參數，啟動參數 --kubelet-insecure-tls 目的是防止 metrics server 訪問 kubelet 採集指標時報證書錯誤 x509: certificate signed by unknown authority 的問題。\n原本錯誤的範例\r1resource \u0026#34;helm_release\u0026#34; \u0026#34;metrics-server\u0026#34; { 2 count = var.package.metrics == true ? 1 : 0 3 4 name = \u0026#34;metrics-server\u0026#34; 5 namespace = \u0026#34;kube-system\u0026#34; 6 chart = \u0026#34;metrics-server/metrics-server\u0026#34; 7 set { 8 name = \u0026#34;args\u0026#34; 9 value = \u0026#34;[\\\u0026#34;--kubelet-insecure-tls\\\u0026#34;]\u0026#34; 10 } 11} 會丟以下錯誤\n1Error: template: metrics-server/templates/deployment.yaml:59:27: executing \u0026#34;metrics-server/templates/deployment.yaml\u0026#34; at \u0026lt;.Values.args\u0026gt;: range can\u0026#39;t iterate over [\u0026#34;--kubelet-insecure-tls\u0026#34;] 正確寫法\r將 [] 括號使用 {} 取代，且陣列項目不需要加上 \u0026quot;\u0026quot;。\n1resource \u0026#34;helm_release\u0026#34; \u0026#34;metrics-server\u0026#34; { 2 count = var.package.metrics == true ? 1 : 0 3 4 name = \u0026#34;metrics-server\u0026#34; 5 namespace = \u0026#34;kube-system\u0026#34; 6 chart = \u0026#34;metrics-server/metrics-server\u0026#34; 7 set { 8 name = \u0026#34;args\u0026#34; 9 value = \u0026#34;{--kubelet-insecure-tls}\u0026#34; 10 } 11} ","date":"2022-05-19T08:58:00+00:00","updated":"2022-05-19T08:58:00+00:00"},{"objectID":"1652950380","permalink":"/post/k8s-calico-running-but-unready/calico-running-but-unready/","title":"Calico Running but Unready (Ready 0/1)","content":"環境說明\rKubernetes 1.20.10 Calico 3.23 錯誤描述\r部署 calico 網絡後狀態雖為 Running，但 container 都無法成功運作，calico-controller 以及放在每個節點的 calico-node 皆無法初始化，如下圖\ncalico-kube-controllers 日誌 12022-05-17 01:11:09.503 [FATAL][1] main.go 120: Failed to initialize Calico datastore error=Get \u0026#34;https://10.254.0.1:443/apis/crd.projectcalico.org/v1/clusterinformations/default\u0026#34;: context deadline exceeded 22022-05-17 01:11:09.962 [INFO][1] main.go 94: Loaded configuration from environment config=\u0026amp;config.Config{LogLevel:\u0026#34;info\u0026#34;, WorkloadEndpointWorkers:1, ProfileWorkers:1, PolicyWorkers:1, NodeWorkers:1, Kubeconfig:\u0026#34;\u0026#34;, DatastoreType:\u0026#34;kubernetes\u0026#34;} 3W0822 01:11:09.963646 1 client_config.go:615] Neither --kubeconfig nor --master was specified. Using the inClusterConfig. This might not work. 42022-05-17 01:11:09.964 [INFO][1] main.go 115: Ensuring Calico datastore is initialized calico-node 日誌 12022-05-17 01:11:14.102 [WARNING][69] felix/health.go 211: Reporter is not ready. name=\u0026#34;async_calc_graph\u0026#34; 22022-05-17 01:11:14.102 [WARNING][69] felix/health.go 211: Reporter is not ready. name=\u0026#34;int_dataplane\u0026#34; 32022-05-17 01:11:14.103 [WARNING][69] felix/health.go 173: Health: not ready 42022-05-17 01:11:14.540 [INFO][69] felix/watchercache.go 180: Full resync is required ListRoot=\u0026#34;/calico/resources/v3/projectcalico.org/kubernetesendpointslices\u0026#34; 52022-05-17 01:11:14.544 [INFO][69] felix/watchercache.go 193: Failed to perform list of current data during resync ListRoot=\u0026#34;/calico/resources/v3/projectcalico.org/kubernetesendpointslices\u0026#34; error=resource does not exist: KubernetesEndpointSlice with error: the server could not find the requested resource 原因及解法\r原因是因為安裝的 Calico 版本不相容於 Kubernetes，依據 Calico 官網\r的描述，3.23 版最低支援的 k8s 版本為 1.21，故發生該錯誤。 解決方法是將 Calico 重新部署為 3.21 版，以相容於 k8s 1.20 的環境\n1curl -s https://docs.projectcalico.org/manifests/calico.yaml | kubectl delete -f - 2 configmap \u0026#34;calico-config\u0026#34; deleted 3 customresourcedefinition.apiextensions.k8s.io \u0026#34;bgpconfigurations.crd.projectcalico.org\u0026#34; deleted 4 customresourcedefinition.apiextensions.k8s.io \u0026#34;bgppeers.crd.projectcalico.org\u0026#34; deleted 5 customresourcedefinition.apiextensions.k8s.io \u0026#34;blockaffinities.crd.projectcalico.org\u0026#34; deleted 6 customresourcedefinition.apiextensions.k8s.io \u0026#34;caliconodestatuses.crd.projectcalico.org\u0026#34; deleted 7 customresourcedefinition.apiextensions.k8s.io \u0026#34;clusterinformations.crd.projectcalico.org\u0026#34; deleted 8 customresourcedefinition.apiextensions.k8s.io \u0026#34;felixconfigurations.crd.projectcalico.org\u0026#34; deleted 9 customresourcedefinition.apiextensions.k8s.io \u0026#34;globalnetworkpolicies.crd.projectcalico.org\u0026#34; deleted 10 customresourcedefinition.apiextensions.k8s.io \u0026#34;globalnetworksets.crd.projectcalico.org\u0026#34; deleted 11 customresourcedefinition.apiextensions.k8s.io \u0026#34;hostendpoints.crd.projectcalico.org\u0026#34; deleted 12 customresourcedefinition.apiextensions.k8s.io \u0026#34;ipamblocks.crd.projectcalico.org\u0026#34; deleted 13 customresourcedefinition.apiextensions.k8s.io \u0026#34;ipamconfigs.crd.projectcalico.org\u0026#34; deleted 14 customresourcedefinition.apiextensions.k8s.io \u0026#34;ipamhandles.crd.projectcalico.org\u0026#34; deleted 15 customresourcedefinition.apiextensions.k8s.io \u0026#34;ippools.crd.projectcalico.org\u0026#34; deleted 16 customresourcedefinition.apiextensions.k8s.io \u0026#34;ipreservations.crd.projectcalico.org\u0026#34; deleted 17 customresourcedefinition.apiextensions.k8s.io \u0026#34;kubecontrollersconfigurations.crd.projectcalico.org\u0026#34; deleted 18 customresourcedefinition.apiextensions.k8s.io \u0026#34;networkpolicies.crd.projectcalico.org\u0026#34; deleted 19 customresourcedefinition.apiextensions.k8s.io \u0026#34;networksets.crd.projectcalico.org\u0026#34; deleted 20 clusterrole.rbac.authorization.k8s.io \u0026#34;calico-kube-controllers\u0026#34; deleted 21 clusterrolebinding.rbac.authorization.k8s.io \u0026#34;calico-kube-controllers\u0026#34; deleted 22 clusterrole.rbac.authorization.k8s.io \u0026#34;calico-node\u0026#34; deleted 23 clusterrolebinding.rbac.authorization.k8s.io \u0026#34;calico-node\u0026#34; deleted 24 daemonset.apps \u0026#34;calico-node\u0026#34; deleted 25 serviceaccount \u0026#34;calico-node\u0026#34; deleted 26 deployment.apps \u0026#34;calico-kube-controllers\u0026#34; deleted 27 serviceaccount \u0026#34;calico-kube-controllers\u0026#34; deleted 28 error: unable to recognize \u0026#34;STDIN\u0026#34;: no matches for kind \u0026#34;PodDisruptionBudget\u0026#34; in version \u0026#34;policy/v1\u0026#34; 重新部署\n1curl -s https://docs.projectcalico.org/v3.21/manifests/calico.yaml | kubectl apply -f - 2 configmap/calico-config created 3 Warning: apiextensions.k8s.io/v1beta1 CustomResourceDefinition is deprecated in v1.16+, unavailable in v1.22+; use apiextensions.k8s.io/v1 CustomResourceDefinition 4 customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created 5 customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created 6 customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created 7 customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created 8 customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created 9 customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created 10 customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created 11 customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created 12 customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created 13 customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created 14 customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created 15 customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created 16 customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created 17 customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created 18 clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created 19 clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created 20 clusterrole.rbac.authorization.k8s.io/calico-node created 21 clusterrolebinding.rbac.authorization.k8s.io/calico-node created 22 daemonset.apps/calico-node created 23 serviceaccount/calico-node created 24 deployment.apps/calico-kube-controllers created 25 serviceaccount/calico-kube-controllers created 部署依照環境配置大概需要等幾分鐘不等，過程中可能會造成其他 Pod 被重啟，觀察下來是正常的情況\n全部完成後就可以看到 pod 皆正常運作了\nReference\rhttps://projectcalico.docs.tigera.io/archive/v3.21/getting-started/kubernetes/requirements\rhttps://github.com/opsnull/follow-me-install-kubernetes-cluster/issues/633\r","date":"2022-05-19T08:53:00+00:00","updated":"2022-05-19T08:53:00+00:00"},{"objectID":"1652723581","permalink":"/post/network-using-router-as-switch/using-router-as-switch/","title":"把 Router 當 Switch 用","content":"在公司架環境時，剛好遇到要讓幾台電腦都要直接連公司網段的情況，可是手上只有一台 router，如果把現有的 IP 網路線插在 WAN port 上，那麼插在 LAN port 的機器都會是虛擬網段，不符所需。後來 google 發現只要照下面二個步驟就可以讓 router 變身成 switch：\n連進 router 的 web console，把 dhcp server 功能關掉 把原先接在 WAN port 的網路線改接到 LAN port 以手上這台 totolink ac5 為例，只要將電腦跟公司內部 router 出來的實體網路線都插到中間三個 LAN port 即可。 補充一下基本知識\nHub、Switch、Router 共同點在於\r連結多台電腦、傳遞封包，延伸網路範圍、進行廣播。\nHub\r集線器用一個 port 直接連到網路上，大部分是有 DHCP server，其餘電腦接在這集線器上，就能自動拿到 IP。集線器所有 port 共享一個頻寬，同樣的封包會傳送至集線器所連結的每一台電腦，造成過多的廣播封包，影響網路傳輸的整體效能。\nL2 Switch\r交換器是一種負責網路橋接（network bridging）的網路硬體設備，會讀取網路卡的 MAC 位址來轉發資料，將資料準確地送達目的地。交換器的每個 port 都享有一個專屬的頻寬並具備資料交換功能，使得網路傳輸效能得於同一時間內所能傳輸的資料量較大\nL3 Switch\r如果再把路由表的功能加入 L2 Switch，那麼它就會變成 L3 Switch，可以為 VLAN 建立適當的路由表，讓效能更加提昇。L3 的交換器又稱為 IP Switch 或 Switch Router，透過專屬的 ASIC 晶片來解析第三層表頭（如 IP Header）以達到傳送目的，因此通常可以提高到每秒百萬封包的效能以及數十個高速乙太網路連接埠之容量。L3 Switch 的路由表可以對 VLAN 做更有效的管制，讓廣播封包不會無限制的傳送。\nL3 Swtich 比較專注在企業內網的 LAN 環境應用，通常會有提供大量的 Port 數，對於封包轉送的效能高。\nRouter\rRouter 通常定位在跨 WAN 的邊界連接用，可區隔不同的 IP 網段，可拿來當作 NAT、防火牆、負載平衡或 VoIP 等服務。通常不會有太多 Port (因為很少會有數十條 WAN 同時接進來)。雖軟體速度較硬體慢 (加上在網路協定中，越往上跑，封包要拆開越多層)，但在實際運用上較有彈性。\n","date":"2022-05-16T17:53:01+00:00","updated":"2022-05-16T17:53:01+00:00"},{"objectID":"1652723340","permalink":"/post/devops-install-kvm-libvertd-by-vagrant-on-centos/install-kvm-by-vagrant-on-centos/","title":"Install KVM (libvert) by Vagrant on CentOS","content":"上次紀錄了如何在 Ubuntu 透過 Virtualbox 使用 Vagrant，本篇文章記錄如何在 CentOS 透過 KVM 使用 Vagrant 自動化建立 VM。\nPrerequisite\r安裝 KVM，請參考之前\r文章記錄\r安裝 Vagrant\r安裝 Plugin\rvagrant-libvirt 用於支持 libvirt 1yum install -y libvirt-devel 2vagrant plugin install vagrant-libvirt 需要安裝好 libvirt-devel 才能安裝 vagrant-libvirt 插件，否則提示以下錯誤 1libvirt library not found in default locations (RuntimeError) vagrant-mutate 用於將官方的 Vagrant guest box 轉換成 KVM 格式 1vagrant plugin install vagrant-mutate 在 Vagrant 官方鏡像中，也有部分發行版直接提供了 libvirt 版本 定義 Vagrantfile\r1ENV[\u0026#34;LC_ALL\u0026#34;] = \u0026#34;en_US.UTF-8\u0026#34; 2 3MASTER=3 4WORKER=2 5 6Vagrant.configure(\u0026#34;2\u0026#34;) do |config| 7 8$script_ansible = \u0026lt;\u0026lt;EOF 9 yum install -y net-tools git centos-release-ansible-29 10 yum install -y ansible 11 localectl set-locale LANG=en_US.UTF-8 12 source /etc/locale.conf 13 mkdir -p /root/.ssh 14 cp /home/vagrant/.ssh/authorized_keys /root/.ssh/ 15 cp /home/vagrant/.ssh/id_rsa /root/.ssh/ 16EOF 17 18$script = \u0026lt;\u0026lt;EOF 19 yum install -y net-tools 20 localectl set-locale LANG=zh_TW.UTF-8 21 source /etc/locale.conf 22 mkdir -p /root/.ssh 23 cp /home/vagrant/.ssh/authorized_keys /root/.ssh/ 24EOF 25 26 config.ssh.insert_key = false 27 config.vm.provider \u0026#34;libvirt\u0026#34; do |v| 28 v.memory = 8192 29 v.cpus = 4 30 v.storage_pool_name = \u0026#34;images\u0026#34; 31 end 32 33 (1..MASTER).each do |i| 34 config.vm.define \u0026#34;master#{i}\u0026#34; do |node| 35 node.vm.box = \u0026#34;generic/centos7\u0026#34; 36 node.vm.hostname = \u0026#34;master#{i}\u0026#34; 37 # node.vm.network :private_network, ip: \u0026#34;192.168.56.2#{i}\u0026#34;, :netmask =\u0026gt; \u0026#34;255.255.255.0\u0026#34; 38 node.vm.network \u0026#34;public_network\u0026#34; 39 :dev=\u0026gt;\u0026#34;br0\u0026#34; 40 node.vm.synced_folder \u0026#34;/root/k8sInstaller\u0026#34;, \u0026#34;/etc/ansible/roles/k8sInstaller\u0026#34; 41 node.vm.provision \u0026#34;file\u0026#34;, source: \u0026#34;~/.vagrant.d/insecure_private_key\u0026#34;, destination: \u0026#34;~/.ssh/id_rsa\u0026#34; 42 node.vm.provision :shell, :inline =\u0026gt; $script_ansible 43 end 44 end 45 46(1..WORKER).each do |i| 47 config.vm.define \u0026#34;worker#{i}\u0026#34; do |node| 48 node.vm.box = \u0026#34;generic/centos7\u0026#34; 49 node.vm.hostname = \u0026#34;worker#{i}\u0026#34; 50 # node.vm.network :private_network, ip: \u0026#34;192.168.56.3#{i}\u0026#34;, :netmask =\u0026gt; \u0026#34;255.255.255.0\u0026#34; 51 node.vm.network \u0026#34;public_network\u0026#34; 52 :dev=\u0026gt;\u0026#34;br0\u0026#34; 53 node.vm.provision :shell, :inline =\u0026gt; $script 54 end 55 end 56 57end 更多 provider 設定可參考 vagrant-libvert\r文檔。\n啟動\r執行 vagrant up 指令需要額外指定 provider 如下：\n1vgrant up --provider=libvirt 或者在執行前設定 VAGRANT_DEFAULT_PROVIDER 環境變數：\n1export VAGRANT_DEFAULT_PROVIDER=libvirt 成功的話，可以看到虛擬機列表有五台用 vagrant 啟動的機器\nCall to virStoragePoolDefineXML failed: operation failed: Storage source conflict with pool: \u0026lsquo;images\u0026rsquo;\rCentOS 7 部署 kvm 使用 SELinux 增強模式，不能使用非默認的目錄來創建 VM 鏡像。Vagrant 會嘗試使用一個名為 default 的存儲池，如果不存在就會嘗試在 /var/lib/libvirt/images 上創建 defualt 存儲池。返回失敗的原因是已經在此為址創建了 images 儲存池。\n1virsh pool-list --all 2Name State Autostart 3------------------------------------------- 4 images active yes 直接定義 Vagrant 的存儲池使用images，在 Libvirt 配置中，有關 Porvider Options 中可以使用 storage_pool_name 設置 Libvirt 存儲池名字，也就是 box image 和 instance snapshoot 存儲位置。 在 Vagrantfile 中添加配置\n1Vagrant.configure(\u0026#34;2\u0026#34;) do |config| 2 ... 3 config.vm.provider :libvirt do |vm| 4 vm.storage_pool_name = \u0026#34;images\u0026#34; 5 end 6 ... 7end 然後再次執行命令 vagrant up --provider libvirt 就可以成功安裝。\nCall to virDomainCreateWithFlags failed: Unable to get index for interface eth0: No such device\r如果網路使用 public network，Vagrant 預設使用來當 bridge 的網卡是 eth0，而發生該錯誤訊息代表本地機器中找不到 eth0 網卡，請在設定檔中加上 dev 指定網卡名稱。\n1Vagrant.configure(\u0026#34;2\u0026#34;) do |config| 2 ... 3 config.vm.network \u0026#34;public_network\u0026#34; 4 :dev =\u0026gt; \u0026#34;br0\u0026#34; 5 ... 6end 其他參數可以參考 public network options\rReference\rhttps://huataihuang.gitbooks.io/cloud-atlas/content/virtual/vagrant/vagrant_libvirt_kvm.html\r","date":"2022-05-16T17:49:00+00:00","updated":"2022-05-16T17:49:00+00:00"},{"objectID":"1652607600","permalink":"/post/programming-apache-redirect-http-to-https/redirect-http-to-https-on-apache/","title":"Apache Redirect http to https","content":"目前手上有一個使用 php 開發的網頁，web server 使用 Apache (httpd) 實做。本篇記錄如何將 http 自動轉址到 https。\nmod_rewrite.c\r編輯 httpd config 檔，/etc/httpd/conf/httpd.conf，在 \u0026lt;VirtualHost *:80\u0026gt; 的區塊下新增以下 rewirte module：\n1\u0026lt;IfModule mod_rewrite.c\u0026gt; 2 RewriteEngine on 3 RewriteRule ^ - [E=protossl] 4 5 RewriteCond %{HTTPS} on 6 RewriteRule ^ - [E=protossl:s] 7 8 RewriteCond %{HTTPS} !=on 9 RewriteRule ^ https://www.ula.com/$1 [L,R=301] 10\u0026lt;/IfModule\u0026gt; 作業環境為 CentOS，重啟 httpd\n1sudo systemctl restart httpd Reference\rhttps://stackoverflow.com/questions/57341254/how-to-redirect-http-to-https-on-apache\r","date":"2022-05-15T09:40:00+00:00","updated":"2022-05-15T09:40:00+00:00"},{"objectID":"1652522460","permalink":"/post/devops-install-vagrunt-on-ubuntu/install-vagrunt-on-ubuntu/","title":"Vagrant on Ubuntu","content":"Vagrant 是由 HashiCorp 開源、使用 Ruby 開發的虛擬機器管理工具，用於管理如 VirtualBox、VMware、AWS 等 VM，主要好處是可以提供一個可配置、可移植和復用的虛擬機環境，可快速地使用設定檔 (Vagrantfile) 和 command line 自動化安裝、配置一台 VM，降低了開發者搭建環境的時間。\nPrerequisite\rDownload Provider Vagrant 的術語中，底層的虛擬機器軟體叫作 provider，預設 provider 是 VirtualBox，其他支援的 provider 可參考\r官網\r。本文使用官方推薦的 Virtualbox，依據\r官網下載步驟安裝\r。 Download Vagrant\r1$ vagrant version 2Installed Version: 2.2.19 3Latest Version: 2.2.19 4 5You\u0026amp;#39;re running an up-to-date version of Vagrant! Before We Start - Vagrant Basic\rVagrant 提供一個命令行工具 vagrant，可以直接操作虛擬機。\nBox：Vagrant 的虛擬機鏡像，可以透過在公開的 Vagrant Box Catalog\r上搜尋適合的 box 使用。 Provisioning：虛擬機實例啟動後的初始化 Vagrant 的工作流程大致如下：\n編寫設定檔 (Vagrantfile) 根據設定檔下載引入 Vagrant box 檔案。 Vagarnt 根據設定檔配置，開通並執行虛擬機器，讓它成為運行狀態。 建立 Vagrant 虛擬機\r初始化\r在公開的 box catalog 選定好想要的 box 後，透過 vagrant init 初始化，會在目錄下產生一個 Vagrantfile 檔案，建議創建一個專屬存放當下環境要用的目錄，vagrant 指令都在 Vagrantfile 所在的目錄執行，免得 Vagrant 搞錯成別台機器。\n直接在 init 指定 1vagrant init generic/centos7 透過 Vagrantfile 指定 若不加 box 名稱，可直接下 vagrant init，再去修改 Vagrantfile 中的相關 …","date":"2022-05-14T10:01:00+00:00","updated":"2022-05-14T10:01:00+00:00"},{"objectID":"1652126040","permalink":"/post/os-ubuntu-install-local-deb/ubuntu-install-deb-file/","title":"Ubuntu 20.04 安裝本地 deb 包","content":"apt 是 ubuntu 最常用的包命令，用於從 Ubuntu 存儲庫、PPA 和第三方 apt 存儲庫安裝、刪除和管理 package。從 Ubuntu 20.04 開始，apt 命令支持對本地 deb 文件的安裝。\ndpkg\r在以前都是使用 dpkg -i 來安裝本地 deb。但是 dpkg 不會自動安裝依賴包，因此安裝很容易出現依賴相關的錯誤。之後需要通過運行 sudo apt-get install -f 來安裝依賴。\napt/apt-get\r直接通過 apt/apt-get 來安裝本地 deb 包，只需要為 apt/apt-get 指定 deb 包的相對路徑或絕對路徑就行了，不能直接在 apt 命令後指定 deb 包的名字，必須要指定路徑，否則 apt 命令會嘗試從遠程倉庫中搜索 deb 包同名的 package，從而導致安裝失敗。如下範例：\n1sudo apt install ./PACKAGE_NAME.deb 2sudo apt install /home/ula/PACKAGE_NAME.deb Reference\rhttps://ubuntuhandbook.org/index.php/2021/04/install-deb-file-ubuntu-4-ways/\r","date":"2022-05-09T19:54:00+00:00","updated":"2022-05-09T19:54:00+00:00"},{"objectID":"1650747600","permalink":"/post/coldplay-biutyful/","title":"[Coldplay] Biutyful","content":"All I know is I love you so\n我所知道的是我如此愛你\nI hope that you get everything you want in this beautiful life\n我所希望的是你在這個美好生活中得到你想要的一切\nChange for your pocket, someone for the night\n換取你口袋的零錢，或者陪伴你度過一個夜晚\nI hope they name you a rocket and take you for a ride for free\n我希望他們能為你的火箭命名並免費帶你一程\nAnd if they tell you you\u0026rsquo;re nothin\u0026rsquo;, maybe you\u0026rsquo;d explain\nTo me you\u0026rsquo;re the summer sun after the rain\n如果他們告訴你一無是處，也許你可以解釋對我來說你是雨後的天晴\nYou were there when I needed something for the pain, you see\n你看，當我需要一些東西來止痛的時候你總在那裡\nWhen you love me, love me, love me\n當你愛我，愛我，愛我\nWhen you love me, love me, love me\n當你愛我，愛我，愛我\nWhen you love me, love me, love me\n當你愛我，愛我，愛我\nWhen you love me, love me, love me\n當你愛我，愛我，愛我\nI\u0026rsquo;m nowhere else\n我不在任何地方\nI\u0026rsquo;m on top of the world, man\nOn top of the world\n我在世界的頂端, 夥伴, 在世界的頂端\nAnd it\u0026rsquo;s so beautiful\n而這如此美麗\nAnd you\u0026rsquo;re so beautiful\n你也如此美麗\nI hope that you get everything you want in this beautiful life\n我所希望的是你在這個美好生活中得到你想要的一切\nWatermelon moon, so happy you\u0026rsquo;re alive\n西瓜月亮，真高興你活著\nAnd I feel like a river finally arrived at sea\n而我感覺像條河流，終於抵達海洋\n\u0026lsquo;Cause when you love me, love me, love me\nWhen you love me, love me, love me\nWhen you love me, love me, love me\nWhen you love me, love me, love me\nI know I\u0026rsquo;ll be on top of the world, man\nOn top of the world\nAnd it\u0026rsquo;s so beautiful\nAnd you\u0026rsquo;re so beautiful\nAnd it\u0026rsquo;s so beautiful\n\u0026lsquo;Cause when you love me, love me, love me\nWhen you love me, love me, love me\nWhen you love me, love me, love me\nWhen you love me, love me, love me\nI know I\u0026rsquo;ll be on top of the world, man\nOn top of the world\nAnd it\u0026rsquo;s so beautiful\nAnd you\u0026rsquo;re so beautiful\nAnd it\u0026rsquo;s so beautiful\nAnd you\u0026rsquo;re so beautiful\nAnd it\u0026rsquo;s so beautiful\n","date":"2022-04-23T21:00:00+00:00","updated":"2022-04-23T21:00:00+00:00"},{"objectID":"1650484740","permalink":"/post/database-mongodb-intro/install-mongodb-on-kubernetes/","title":"在 Kubernetes 上佈署 MongoDB","content":"紀錄一下 mongodb 的 kubernetes 佈署檔，分為 standalone 以及 replica set，會宣告 persistent volume 以儲存永久性資料 (如 database 資料、index 以及設定檔)。\n**假設已建立以 nfs 為基底的 storage class。\nStandalone\r1apiVersion: v1 2kind: PersistentVolumeClaim 3metadata: 4 name: test-mongo-pvc 5 namespace: test-mongo 6spec: 7 accessModes: 8 - ReadWriteOnce 9 resources: 10 requests: 11 storage: 10Gi 12 storageClassName: nfs 13 14--- 15 16kind: Deployment 17apiVersion: apps/v1 18metadata: 19 name: mongodb 20 namespace: test-mongo 21spec: 22 replicas: 1 23 selector: 24 matchLabels: 25 app: mongodb 26 template: 27 metadata: 28 labels: 29 app: mongodb 30 spec: 31 containers: 32 - name: mongodb 33 image: mongo:latest 34 command: [\u0026#34;mongod\u0026#34;, \u0026#34;--bind_ip_all\u0026#34;, \u0026#34;--noauth\u0026#34;, \u0026#34;--dbpath\u0026#34;, \u0026#34;/data/db\u0026#34;] 35 ports: 36 - containerPort: 27017 37 name: mongodb 38 protocol: TCP 39 imagePullPolicy: IfNotPresent 40 volumeMounts: 41 - name: mongodb-data 42 mountPath: /data/db 43 volumes: 44 - name: mongodb-data 45 persistentVolumeClaim: 46 claimName: test-mongo-pvc 47 48--- 49 50apiVersion: v1 51kind: Service 52metadata: 53 name: mongodb 54 namespace: test-mongo 55 labels: 56 name: mongodb 57spec: 58 ports: 59 - port: 27017 60 name: mongo-port 61 protocol: TCP 62 targetPort: 27017 63 selector: 64 app: mongodb 65 type: NodePort Replicaset\r(待補充)\n","date":"2022-04-20T19:59:00+00:00","updated":"2022-04-20T19:59:00+00:00"},{"objectID":"1650403140","permalink":"/post/programming-golang-sql-query/golang-sql-query/","title":"[Golang] 查詢 SQL DB 的幾種匯出方式","content":"簡易紀錄一下 sql query 回傳(印出)查詢結果的三種不同方法，三種方式回傳的格式都差不多，只是型態可能不一樣。本文使用的 database 是 Oracle。\n連線\r1package main 2 3import ( 4\t\u0026#34;database/sql\u0026#34; 5\t\u0026#34;fmt\u0026#34; 6 7\t_ \u0026#34;github.com/godror/godror\u0026#34; 8) 9 10const ( 11\tusername = \u0026#34;ula\u0026#34; 12\tpassword = \u0026#34;123456\u0026#34; 13\thost = \u0026#34;10.90.1.207\u0026#34; 14\tport = 1521 15\tsid = \u0026#34;emesHY\u0026#34; 16) 17 18func main() { 19 20\toralInfo := fmt.Sprintf(\u0026#34;%s/%s@%s:%d/%s\u0026#34;, username, password, host, port, sid) 21\tdb, err := sql.Open(\u0026#34;godror\u0026#34;, oralInfo) 22 23\terr = db.Ping() 24\tif err != nil { 25\tfmt.Println(\u0026#34;cant connect\u0026#34;) 26\tdb.Close() 27\t} 28 29\tsqlComand := fmt.Sprintf(\u0026#34;SELECT DISTINCT SERIAL_NUMBER,PRE_KPSN,THIS_KPSN,DATECODE,LOTCODE from EMESC.VP_ASSY_SUMMARY_SMT where serial_number = \u0026#39;TBCBB2039913\u0026#39;\u0026#34;) 30 31\trows, err := db.Query(sqlComand) 32\tif err != nil { 33\tfmt.Println(\u0026#34;Query Database Failed!\u0026#34;) 34\t} 35\tdefer rows.Close() 36 37\t// 插入以下其中一種作法 38} 作法一\r1columns, err := rows.Columns() 2if err != nil { 3\tfmt.Println(err) 4} 5count := len(columns) 6tableData := make([]map[string]interface{}, 0) 7values := make([]interface{}, count) 8valuePtrs := make([]interface{}, count) 9for rows.Next() { 10\tfor i := 0; i \u0026lt; count; i++ { 11\tvaluePtrs[i] = \u0026amp;values[i] 12\t} 13\trows.Scan(valuePtrs...) 14\tentry := make(map[string]interface{}) 15\tfor i, col := range columns { 16\tvar v interface{} 17\tval := values[i] 18\tb, ok := val.([]byte) 19\tif ok { 20\tv = string(b) 21\t} else { 22\tv = val 23\t} 24\tentry[col] = v 25\t} 26\ttableData = append(tableData, entry) 27} 28jsonData, err := json.Marshal(tableData) 29if err != nil { 30\tfmt.Println(err) 31} 32// fmt.Println(string(jsonData)) 33 34var result []map[string]interface{} 35 36if err := json.Unmarshal([]byte(jsonData), \u0026amp;result); err != nil { 37\tfmt.Println(\u0026#34;Can\u0026#39;t parse the json string\u0026#34;) 38} 39 40fmt.Println(result) 作法二\r1type Kpsn struct { 2\tSN string `json:\u0026#34;Serial_Number\u0026#34;` 3\tPre string `json:\u0026#34;Pre_KPSN\u0026#34;` 4\tThis string `json:\u0026#34;This_KPSN\u0026#34;` 5\tDC string `json:\u0026#34;Datecode\u0026#34;` 6\tLC string `json:\u0026#34;Lotcode\u0026#34;` 7} 8 9var kpsns []*Kpsn 10 11for rows.Next() { 12\tk := new(Kpsn) 13\trows.Scan(\u0026amp;k.SN, \u0026amp;k.Pre, \u0026amp;k.This, \u0026amp;k.DC, \u0026amp;k.LC) 14\tkpsns = append(kpsns, k) 15} 16jsonStr, _ := json.Marshal(\u0026amp;kpsns) 17 18var result []map[string]interface{} 19// var result []Kpsn 20 21if err := json.Unmarshal([]byte(jsonStr), \u0026amp;result); err != nil { 22\tfmt.Println(\u0026#34;Can\u0026#39;t parse the json string\u0026#34;) 23} 24 25fmt.Println(result) 作法三\r1var sn, pre, this, dc, lc string 2var result []map[string]interface{} 3 4for rows.Next() { 5\trows.Scan(\u0026amp;sn, \u0026amp;pre, \u0026amp;this, \u0026amp;dc, \u0026amp;lc) 6\tvar tmp map[string]interface{} 7\ttmp = make(map[string]interface{}) 8\ttmp[\u0026#34;SERIAL_NUMBER\u0026#34;] = sn 9\ttmp[\u0026#34;PRE_KPSN\u0026#34;] = pre 10\ttmp[\u0026#34;THIS_KPSN\u0026#34;] = this 11\ttmp[\u0026#34;DATECODE\u0026#34;] = dc 12\ttmp[\u0026#34;LOTCODE\u0026#34;] = lc 13\tresult = append(result, tmp) 14} 15fmt.Println(result) Reference\rhttps://stackoverflow.com/a/29164115/13318115\r","date":"2022-04-19T21:19:00+00:00","updated":"2022-04-19T21:19:00+00:00"},{"objectID":"1650046680","permalink":"/post/devops-gitlab-cicd-pipeline-on-k8s/build-cicd-pipeline-and-cd-to-k8s/","title":"創建 GitLab CICD pipeline 完成自動部屬到 Kubernetes","content":"前言\r在還沒接觸 CI/CD 時，一直有『這東西一定很難』的預設立場，直到開始撰寫第一個 .gitlab-ci.yml 時，心理認知的困難度仍沒消失。不過慶幸的是，網路上的教學真的很多，GitLab 社群也超給力的有著豐富的文檔跟範例。GitLab CI/CD 整體架構 (gitlab server、runner、excutor、pipeline、stage、job) 其實很單純，所以在實做時，可以把自動化需求切分，先從第一段 build 開始做，做成功後，再進階到 test 及 deploy。把困難的任務分段做，感覺就不那麼難以親近了！\n確認 CI/CD 需求\r前面文章\r已經建好了一個 runner，現在要實做 CI/CD pipeline。在開始之前要先確定開發者想透過 CICD 完成哪部分的自動化？以本次實做為例，我想要達成：\ncode 推上 GitLab 後，不須測試，因為在推上 repo 前，皆已在本地端測試完畢。 自動依據專案中的 Dockerfile 檔案 build 成 image，並上傳到 registry，如 DockerHub 或私有倉庫 自動依據專案中的 kubernetes.yaml 檔部屬到 kubernetes cluster 中 .gitlab-ci.yml\rGitLab CI/CD 透過放置在專案根目錄底下的 .gitlab-ci.yml 檔案來驅動，官網提供了許多 Template\r給初學者參考。 定義 pipeline 中會有哪些 stages (預計要有的構建階段)，依序設置。\n1stages: 2 - build 3 - deploy 接著在 Stage 中設置一個到多個 Job，來描述該階段所需完成的工作。\n1\u0026amp;lt;jobName\u0026amp;gt;: 2 stage: \u0026amp;lt;stageName\u0026amp;gt; 3 only: 4 - \u0026amp;lt;branchName\u0026amp;gt; 5 script: 6 - echo \u0026amp;#34;My first job\u0026amp;#34; 其中\nstage：指定該 Job 屬於哪一個 Stage only：指定在哪個 Branch 觸發時才會執行 script：需要執行的指令 配置 .gitlab-ci.yml\r確定需求後就可以開始寫 .gitlab-ci.yml 檔。\n先實做 build 階段\r1stages: 2 - …","date":"2022-04-15T18:18:00+00:00","updated":"2022-04-15T18:18:00+00:00"},{"objectID":"1649964780","permalink":"/post/k8s-kubelet-pull-image-backoff/image-pull-backoff-after-harbor-login/","title":"已登入 harbor 但 kubelet 仍會 ImagePullBackOff","content":"在 kubernetes 環境上拉取私有鏡像倉庫 harbor 的 image 時，一直卡在 ImagePullBackOff 的狀態，decribe pod 發現是權限問題導致拉取失敗。\n狀況說明\r錯誤訊息如下\n1Failed to pull image \u0026#34;10.1.5.142:4433/test/findkpsn:9d2e44d2\u0026#34;: rpc error: code = Unknown desc = Error response from daemon: unauthorized: unauthorized to access repository: test/findkpsn, action: pull: unauthorized to access repository: test/findkpsn, action: pull 然而實際上在本機上已經 docker login 成功過了，也可以直接使用 docker pull 拉取，但透過 k8s 拉取仍會失敗。\ndebug 思路\r確認在同樣 repo 的 project 下的其他 image 是否也發生同樣的情況 是，同樣 repo 的 project 的其他 image 也相同。 確認在不同的 repo 是否也發生同樣的情況 否，其他 repo 能正常夠過 kubectl 拉取，應能推斷部屬環境上沒問題。 原因\r結果是因為沒有把 project 公開 =__=\n意外發現\r仍然還是可以讓 project 維持在私有的狀況下，透過 kubectl 拉取。只要在定義資源時，加上 imagePullSecrets 的屬性，值指定為欲創建資源的 namespace 下的 kubernetes.io/dockerconfigjson 的 secret，即可拉取成功。\n創建 docker-registry secret\r1kubectl create secret docker-registry \u0026lt;secretName\u0026gt; \\ 2--docker-server=DOCKER_REGISTRY_SERVER \\ 3--docker-username=DOCKER_USER \\ 4--docker-password=DOCKER_PASSWORD -n \u0026lt;NAMESPACE\u0026gt; deployment 部屬檔\r在 spec.template.spec 下新增 imagePullSecrets\n1apiVersion: apps/v1 2kind: Deployment 3metadata: 4 name: test 5 namespace: test 6 labels: 7 app: test 8spec: 9 replicas: 1 10 selector: 11 matchLabels: 12 app: test 13 template: 14 metadata: 15 labels: 16 app: test 17 spec: 18 containers: 19 - name: test 20 image: 10.1.5.142:4433/test/test/findkpsn:9d2e44d2 21 imagePullSecrets: 22 - name: harbor 重新佈署\r加上 imagePullSecrets 後，就可以成功拉取私有專案的鏡像了!\nReference\rhttps://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod\r","date":"2022-04-14T19:33:00+00:00","updated":"2022-04-14T19:33:00+00:00"},{"objectID":"1649608560","permalink":"/post/devops-install-gitlab-runner-on-k8s-using-helm/install-gitlab-runner-on-k8s-by-helm/","title":"在 Kubernetes 上使用 helm 建立 GitLab Runner","content":"GitLab Runner 是一個獨立的程序，可以用以下三種方式安裝，請參考\r官網教學\r。\nGitLab Official Repositories RPM/deb packages Using Binaries Using Containers 承前一篇介紹，Runner 有 3 種類型，所有項目共享的 Shared Runner、指定 Group 共享的 Group Runner 和單個項目獨占的 Specific Runner，各有不同的 registration token。\nShared Runner：在 Admin Area \u0026amp;gt; Runners 中註冊 Group Runner：在指定的 group 的 Settings \u0026amp;gt; CI/CD \u0026amp;gt; Runners 中註冊 Specific Runner：在各自的 project 下的 Settings \u0026amp;gt; CI/CD \u0026amp;gt; Runners 中註冊 本篇記錄使用 helm chart 的方式安裝 Runner 於 Kubernetes 上的過程。\nPrerequisite\rGitLab Server (目前使用 14.9.2 版，用官方提供的 docker-compose 腳本安裝，可參考之前的\r文章記錄\r) Kubernetes 1.4+ (示範環境為 1.20.10) Helm (2 或 3 皆可，本文使用 helm3) GitLab server 與 Kubernetes 集群能互通 準備 helm chart\r1helm repo add gitlab https://charts.gitlab.io 2 helm repo list 3 NAME URL 4 gitlab\thttps://charts.gitlab.io/ 建立自簽憑證的 Kubernetes Secret\r將建立 gitlab 時創建的自簽憑證創建為 secret 資源，該憑證的檔名應採用 \u0026amp;lt;gitlab.hostname\u0026amp;gt;.crt 格式。\n1kubectl create secret generic \u0026amp;lt;SECRET_NAME\u0026amp;gt; \\ 2 --namespace \u0026amp;lt;NAMESPACE\u0026amp;gt; \\ 3 --from-file=\u0026amp;lt;CERTIFICATE_FILENAME\u0026amp;gt; …","date":"2022-04-10T16:36:00+00:00","updated":"2022-04-10T16:36:00+00:00"},{"objectID":"1649608440","permalink":"/post/devops-gitlab-cicd-intro/gitlab-cicd-intro/","title":"GitLab CICD 介紹","content":"CI/CD\rContinuous Integration 持續整合\r指在源代碼變更後的自動檢測(Lint)、構建(Build)和進行單元測試、集成測試(Test)的過程。目標是針對每個變動，能持續性的進行驗證並整合，以確保程式的品質。\nContinuous Delivery 持續交付\r指頻繁地將軟體的新版本，交付給 QA 或使用者，以供評審。如果評審通過，程式碼就進入生產階段。持續交付通常包含了一個手動的步驟，用來讓開發人員確認和部署到生產環境。\nContinuous Deployment 持續佈署\r持續部署是指當交付的程式碼通過評審之後，自動部署到生產環境中。持續部署不再需要開發人員手動部署成品到生產環境，只要程式碼提交的 PR 通過了，剩下的整個過程都會自動完成。\nGitlab CI/CD\rPipeline\r代表一次的完整的自動化任務，每次提交都會觸發一次。根據 GitLab 所述\npipelines are the top-level component of continuous integration, delivery, and deployment pipeline 使用存在 GitLab 項目的根目錄的 yaml 文件 (.gitlab-ci.yml) 定義。\nStage\r代表構建階段，一條 Pipeline 中可定義多個 Stages\n所有 Stages 會順序執行，即當一個 Stage 完成後，下一個 Stage 才會開始 只有當所有 Stages 完成後，該構建任務才會成功 如果任何一個 Stage 失敗，那麼後面的 Stages 不會執行，該構建任務失敗 Job\r代表構建工作，是 GitLab CI 中可以獨立控制並運行的最小單位，一個 Stage 中可以有多的 Jobs。\n相同 Stage 中的 Jobs 會並行執行 任一 Job 失敗，那麼 Stage 失敗，Pipeline 失敗 相同 Stage 中的 Jobs 都執行成功時，該 Stage 成功 Runner\rRunner 負責運行 job。需要先架好 Runner，並在 Gitlab 上登記，當 pipeline 運行時，Gitlab 會指派 jobs 給可使用的 runner。\n分為三種類型\rShared runners\rare available to all groups and projects in a GitLab instance. Group runners\rare available to all projects and subgroups in a group. It process jobs by using FIFO. Specific runners\rare associated with specific projects. Typically, specific runners are used for one project at a time. It process jobs by using FIFO. Reference\rhttps://www.bmc.com/blogs/continuous-delivery-continuous-deployment-continuous-integration-whats-difference/\rhttps://blog.tienyulin.com/ci-cd-concept/\rhttp://myblog-maurice.blogspot.com/2021/01/cicd.html\rhttps://docs.gitlab.com/ee/ci/runners/runners_scope.html\r","date":"2022-04-10T16:34:00+00:00","updated":"2022-04-10T16:34:00+00:00"},{"objectID":"1649606100","permalink":"/post/devops-install-harbor-by-docker-compose/install-harbor-by-docker-compose/","title":"使用 docker-compose 建立 Harbor 私有倉庫 (w/ https)","content":"默認情況下，Harbor 不附帶證書，可以直接使用 http 訪問。但在正式上線的環境中，建議配置 https。\n準備 https 所需證書\r在生產環境中，建議使用由受信任的第三方 CA 簽名的證書。在測試或開發環境中，則可以使用自簽的 CA。以下範例為使用 openssl 生成 CA，分為使用 DNS 或是直接使用 IP。\n使用域名(DNS)\r準備 x509 v3 服務檔，假設 harbor 伺服器使用的 DNS 為 harbor.ula.com。 1cat \u0026amp;gt; v3.ext \u0026amp;lt;\u0026amp;lt;-EOF 2authorityKeyIdentifier=keyid,issuer 3basicConstraints=CA:FALSE 4keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment 5extendedKeyUsage = serverAuth 6subjectAltName = @alt_names 7 8[alt_names] 9DNS.1=harbor.ula.com 10DNS.2=ula.com 11DNS.3=harbor 12EOF 準備金鑰及證書的 shell script 1vim prepare_ssl.sh 貼上以下內容\n1#!/bin/bash 2 3CURDIR=\u0026amp;#34;`pwd`\u0026amp;#34;/\u0026amp;#34;`dirname $0`\u0026amp;#34; 4HOSTNAME=$1 5DOMAIN=$2 6 7INSTALL_PATH=\u0026amp;#34;/data/certs\u0026amp;#34; 8mkdir -p $INSTALL_PATH 9 10cd $INSTALL_PATH 11# generate CA certificate private key 12openssl genrsa -out ca.key 4096 13 14# generate CA certificate 15openssl req -x509 -new -nodes -sha512 -days 3650 -subj \u0026amp;#34;/C=TW/ST=Taipei/L=Taipei/O=$HOSTNAME/OU=Personal/CN=$HOSTNAME.$DOMAIN\u0026amp;#34; -key …","date":"2022-04-10T15:55:00+00:00","updated":"2022-04-10T15:55:00+00:00"},{"objectID":"1648767135","permalink":"/post/k8s-setup-liveness-probe-to-restart-pod-when-showup-err-log/setup-liveness-probe-to-restart-when-pod-has-error-log/","title":"設定 liveness probe 監聽應用以重啟 pod","content":"目前手上有一個監聽 Oracle CDC 的程式跑在以 Debian 為基底的 kubernetes pod 中，會定期因為 Oracle 的錯誤訊息 ORA-12518: TNS 監聽程式無法分發客戶機連線的問題而斷線。此時雖然程式有 error log，但 Pod 的狀態仍然為 Running，只要重啟 Pod 即可重新正常運作。\nORA-12518\r首先順便解釋此錯誤的原因 The process of handing off a client connection to another process failed. 參考網路上其他分享：\nstackoverflow\rittutorial\rcnblogs\r從根本可以解決的方式如下：\nEdit /etc/systemd/system.conf file and Set DefaultTasksMax to ‘infinity’. dedicated server: 修改 oracle processes \u0026amp; sessions parameters shared server: 修改 oracle dispatcher parameters 然而\r因為 IT server 並不在我控管的範圍，所以只能自己手動重啟 Pod。原本是想說寫 cronJob 定期重啟 pod，但在找資料的過程中，發現在 stackoverflow crobjob 問題\r的解法中有人提出了直接使用 livenessprobe 解決。\n從設定 livenessProbe 解決\rKubelet 使用 liveness probe（存活探針）來確定何時重啟容器。當應用程序處於運行狀態但無法做進一步操作，liveness 探針將捕獲到 deadlock，重啟處於該狀態下的容器，使應用程序在存在 bug 的情況下依然能夠繼續運行下去。\nexec.Command：要在容器內執行的檢測命令，如果命令執行成功，將返回 0，kubelet 就會認為該容器是活著的並且很健康。如果返回非 0 值，kubelet 就會殺掉這個容器並重啟它。 periodSeconds：liveness probe 多久檢查一次 initialDelaySeconds：首次啟動 pod 後，要延遲多久後執行 liveness probe probe command 要寫啥?\r接下來又另一個問題來了，我的 probe 中的檢測命令要寫啥? 因為在手動重啟時，只能從 kubectl logs 為依據，查看有無錯誤訊息。然而現在 command 要執行在容器中，但容器裡面沒辦法直接使用 kubectl 取得應用的 stdout 的訊息。又去堆疊溢位(XD 找到了兩種解決方法。\n在容器裡 curl Kubernete API server\r設定 pod 連 kubernetes api server 請參考另外一篇\r文章記錄\r。 command 應該就會長成以下，如果 curl 回到的 output 會 grep 到 error 訊息，則重啟。\n1livenessProbe: 2 exec: 3 command: 4 - bash 5 - -c 6 - \u0026#34;curl -s --cacert /var/run/secrets/kubernetes.io/serviceaccount/ca.crt --header \u0026#34;Authorization: Bearer $(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\u0026#34; -X GET https://kubernetes.default.svc/api/v1/namespaces/converg-it/pods/converg-it-adapter-oracle-8575f54dc6-7lnv6/log?sinceSeconds=100 | grep \u0026#39;error\u0026#39;\u0026#34; 7 initialDelaySeconds: 120 8 periodSeconds: 60 在容器裡查看監聽的 port\r在目前跑的容器中，使用 ss 查看目前系統的 socket 狀態，可以發現到其實在正常連結的情況下能偵測到連線(establish state) oracle server 的監聽。\n那麼當發生連線異常時(ORA-12518)，就可以當作是重啟的條件。\n1livenessProbe: 2 exec: 3 command: 4 - bash 5 - -c 6 - \u0026#34;ss -an | grep -q \u0026#39;EST.*:1521 *$\u0026#39;\u0026#34; 7 initialDelaySeconds: 120 8 periodSeconds: 60 Reference\rhttps://jimmysong.io/kubernetes-handbook/guide/configure-pod-service-account.html\rhttps://stackoverflow.com/questions/49000280/monitor-and-take-action-based-on-pod-log-event\rhttps://stackoverflow.com/questions/57711963/kubernetes-liveness-probe-can-a-pod-monitor-its-own-stdout\r","date":"2022-03-31T22:52:15+00:00","updated":"2022-03-31T22:52:15+00:00"},{"objectID":"1648763832","permalink":"/post/k8s-curl-k8s-api-server-within-pod/curl-kubernetes-api-server-within-pod/","title":"在容器裡 curl Kubernetes API server","content":"連接 k8s 的 api-server 有三種方式：\nKubernetes Node 通過 kubectl proxy 中轉連接 通過授權驗證直接連接，例如 kubectl 和各種 client 就是這種情況 容器內部通過 ServiceAccount 連接 本文以第三種情況作範例。\nKubernetes API Server\r在 Kubernetes 集群被創建時，預設會在 default namespace 中創建 kubernetes 的服務，用於訪問 Kubernetes apiserver。因此，Pod 之間可以直接使用 kubernetes.default.svc 主機名來查詢 API server。\nService Account\rServiceAccount 是給執行在 Pod 的程式使用的身份認證，給 Pod 容器的程式訪問 API Server 時使用；ServiceAccount 僅侷限它所在的 namespace，每個 namespace 建立時都會自動建立一個 default service account；建立 Pod 時，如果沒有指定 Service Account，Pod 則會使用 default Service Account。\nService Account Secret\rSA 對應的 Secret 會自動掛載到 Pod 的 /var/run/secrets/kubernetes.io/serviceaccount/ 目錄中(包含 token、ca.crt、namespace)。\n創建 Role \u0026amp; Role Binding\r如果直接使用預設的 sa 訪問 api server 會遇到權限不足的問題\n此時需要建立角色開放存取 api 指定路徑的權限並綁定角色到 SA 上\n1# role\u0026amp;binding.yaml 2--- 3apiVersion: rbac.authorization.k8s.io/v1 4kind: Role 5metadata: 6 name: default-role 7 namespace: converg-it 8rules: 9 - apiGroups: [\u0026#34;\u0026#34;] 10 resources: 11 - pods 12 - pods/log 13 verbs: 14 - get 15 - list 16--- 17apiVersion: rbac.authorization.k8s.io/v1 18kind: RoleBinding 19metadata: 20 name: default-roldbinding 21 namespace: converg-it 22subjects: 23 - kind: ServiceAccount 24 name: default 25roleRef: 26 kind: Role 27 name: default-role 28 apiGroup: rbac.authorization.k8s.io 1kubectl -n converg-it apply -f role\u0026amp;binding.yaml 2role.rbac.authorization.k8s.io/default-role created 3rolebinding.rbac.authorization.k8s.io/default-roldbinding created curl API\r進入容器環境\n1kubectl exec -it -n converg-it converg-it-adapter-oracle-8575f54dc6-7lnv6 -- bash get target API\n1curl --cacert /var/run/secrets/kubernetes.io/serviceaccount/ca.crt --header \u0026#34;Authorization: Bearer $(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\u0026#34; -X GET https://kubernetes.default.svc/api/v1/namespaces/converg-it/pods/converg-it-adapter-oracle-8575f54dc6-7lnv6/log?sinceSeconds=300 Reference\rhttps://kubernetes.io/docs/tasks/run-application/access-api-from-pod/\r","date":"2022-03-31T21:57:12+00:00","updated":"2022-03-31T21:57:12+00:00"},{"objectID":"1648585140","permalink":"/post/devops-git-your-branch-and-origin-master-have-diverged/git-your-branch-and-origin-master-have-diverged/","title":"[Git] Your branch and 'origin/master' have diverged","content":"在 git status 的時後發現本地端跟遠端倉庫 commit 分岔 (diverged)，代表本地所在的分支跟遠端倉庫的分支各走各的路。\n問題\r1git status 2 On branch master 3 Your branch and \u0026amp;#39;origin/master\u0026amp;#39; have diverged, 4 and have 1 and 1 different commits each, respectively. 5 (use \u0026amp;#34;git pull\u0026amp;#34; to merge the remote branch into yours) 6 7 nothing to commit, working tree clean 原因排查\r查看各自的 commit log，在本地端的 log 中沒有發現 origin/master 遠端倉庫的 master HEAD，所以代表分支已經分歧。\n1git log --oneline origin/master 2 45b6977 (origin/master) v1.1 modified return body 3 2714cef add changelog 4 4a19b17 ula modified reverse rule 5 92d48af origin \u0026amp;#39;reverstapi\u0026amp;#39; version 6git log --oneline 7 733e302 (HEAD -\u0026amp;gt; master) v1.2 rebase from v1 and modified the sql command 8 2714cef add changelog 9 4a19b17 ula modified reverse rule 10 92d48af origin \u0026amp;#39;reverstapi\u0026amp;#39; version 11# 利用 git cherry origin/master 來看遠端與本地端 commit 的差別 12git cherry origin/master 13 + 733e302d65419093b386de9b286c96eecaa95200 14# 顯示有一個版本忘了提交 同步遠端與本地分支\r有以下四種狀況，主要分為合併兩分支與忽略某一方分支。合併又分為兩種 merge …","date":"2022-03-29T20:19:00+00:00","updated":"2022-03-29T20:19:00+00:00"},{"objectID":"1648158720","permalink":"/post/k8s-istio-404-not-found/istio-404-not-found/","title":"Istio 沒掛，但正確的設置 gateway 跟 virtual service 後，卻一直 404 not found","content":"手上有一個寫好的 API 要對外釋出，在設置完 istio 資源之後，curl istio ingress gateway/targetAPI 卻一直回傳 404 Not Found，照理來說這個 API 如果找不到資料回傳的 404 訊息應該是 {\u0026quot;error\u0026quot;:\u0026quot;Record Not Found, the serial number doesn't exist\u0026quot;}，用這篇文章記錄問題跟解決方式。\n問題排查\r在相同的 istio ingress gateway 上的 API 皆正常運作，排除 istio 本身可能會有問題 用同樣的配置檔，部屬在另外一個 K8s 環境上的 istio，發現運作正常，排除配置檔有誤的問題 用其他 API 部屬，也一樣直接 404 Not Found，排除原先 API 本身可能有誤的問題 查看 istio ingress gateway 的 log 1[2022-03-24T06:23:07.653Z] \u0026#34;GET /api/convergence/findRecord/TBCC32008806 HTTP/1.1\u0026#34; 200 - \u0026#34;-\u0026#34; \u0026#34;-\u0026#34; 0 9491 5 4 \u0026#34;10.1.5.32\u0026#34; \u0026#34;PostmanRuntime/7.29.0\u0026#34; \u0026#34;3655dd39-9c2c-9c14-ac5c-53fc7547a155\u0026#34; \u0026#34;10.1.5.41\u0026#34; \u0026#34;10.244.64.149:8080\u0026#34; outbound|8080||converg-api.converg-api.svc.cluster.local 10.244.128.48:42104 10.244.128.48:8080 10.1.5.32:39159 - http-Sbups 2--- 3[2022-03-24T06:26:32.759Z] \u0026#34;GET /api/convergence/grabreflow/TBCBB2039913 HTTP/1.1\u0026#34; 404 - \u0026#34;-\u0026#34; \u0026#34;-\u0026#34; 0 18 1 1 \u0026#34;10.1.5.32\u0026#34; \u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:98.0) Gecko/20100101 Firefox/98.0\u0026#34; \u0026#34;61998a8c-0c79-94a0-9c84-cb03a2f10390\u0026#34; \u0026#34;10.1.5.41\u0026#34; \u0026#34;10.244.128.19:8080\u0026#34; outbound|8080||reverseapi.converg-apipost.svc.cluster.local 10.244.128.48:36740 10.244.128.48:8080 10.1.5.32:15717 - http-XbhqV 上面 200 的是正常運作的 API，下面 404 是新設置的 API，所以其實 404 這個 istio gw \u0026amp; virtual service 其實是有運作的，得出來的結論就是，istio 找不到新設置的 API 去路由，極大可能是跟其他 URL 規則衝突。\n解決\r終於發現前面最後一個設置的 API 沒有設 prefix，所以 istio 就直接監聽 /，導致後面怎麼設新的 API，都認不到!!!!!!!!\n1kubectl get virtualservices.networking.istio.io -n converg-apipost converg-apipost-vs -o yaml 2apiVersion: networking.istio.io/v1beta1 3kind: VirtualService 4metadata: 5 creationTimestamp: \u0026#34;2021-08-04T10:04:00Z\u0026#34; 6 generation: 9 7 labels: 8 protocol: http 9 name: converg-apipost-vs 10 namespace: converg-apipost 11 resourceVersion: \u0026#34;241532937\u0026#34; 12 selfLink: /apis/networking.istio.io/v1beta1/namespaces/converg-apipost/virtualservices/converg-apipost-vs 13 uid: 15049a96-f97f-4b00-bee9-001fafcdd2ff 14spec: 15 gateways: 16 - converg-apipost-gw 17 hosts: 18 - \u0026#39;*\u0026#39; 19 http: 20 - match: 21 - uri: 22 prefix: \u0026#34;\u0026#34; 23 name: http-OuFqP 24 route: 25 - destination: 26 host: reverseapi 27 port: 28 number: 8080 把 prefix 加上去後，原本後加的 API 就成功運作了 ✌️\n1[root@k8sm1 ~]# curl 10.1.5.41/api/convergence/grabreflow/test 2{\u0026#34;error\u0026#34;:\u0026#34;Record Not Found, the serial number doesn\u0026#39;t exist\u0026#34;} ","date":"2022-03-24T21:52:00+00:00","updated":"2022-03-24T21:52:00+00:00"},{"objectID":"1647969180","permalink":"/post/devops-terraform-upgrade-to-v1.1.7/terraform-upgrade-to-1-1-7/","title":"Terraform 從 0.14 升級到 1.1.7 問題排查","content":"手上有 Terraform 0.14 版跑的腳本，最近發現 Terraform 已經升級到 1.1.7 了，便打算在升級的環境下，跑 0.14 版跑成功的腳本，看看是否有誤，紀錄一下遇到的問題以及解法。\nTerraform 安裝\r1yum install -y yum-utils 2yum-config-manager --add-repo https://rpm.releases.hashicorp.com/RHEL/hashicorp.repo 3yum -y install terraform 4terraform version 問題\rError: Kubernetes cluster unreachable: invalid configuration: no configuration has been provided, try setting KUBERNETES_MASTER environment variable\r加上 KUBE_CONFIG_PATH 變數解決。\n1export KUBE_CONFIG_PATH=$HOME/.kube/config Error: failed to download \u0026ldquo;xxxx\u0026rdquo;\r安裝 helm 並加入相對應的 repo\n1[ -f /usr/local/bin/helm ] || (cd /tmp \u0026amp;\u0026amp; curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 \u0026amp;\u0026amp; chmod 755 get_helm.sh \u0026amp;\u0026amp; /tmp/get_helm.sh) 2helm repo add stable https://charts.helm.sh/stable 3helm repo add prometheus-community https://prometheus-community.github.io/helm-charts 4helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx 5helm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/ 6helm repo update Error: Failed to create Ingress \u0026lsquo;xxxingress\u0026rsquo; because: the server could not find the requested resource (post ingresses.extensions)\r將原先的 kubernetes_ingress provider 改成 kubernetes_ingress_v1，另外 service name \u0026amp; port 的參數格式有變，請參考\r官網\r。\nWarning: Helm release \u0026ldquo;xxx\u0026rdquo; was created but has a failed status. Use the helm command to investigate the error, correct it, then run Terraform again.\r基本上 terraform 安裝沒什麼問題，直接下 kubectl get pod 在相對應的 namespace 查看出問題的 pod 是哪些，並進一步 debug。\n","date":"2022-03-22T17:13:00+00:00","updated":"2022-03-22T17:13:00+00:00"},{"objectID":"1647960540","permalink":"/post/k8s-namespace-delete-terminating-status/kubernetes-namespace-delete-terminating/","title":"Kubernetes namespace 一直 delete 不成功的原因 (卡在 terminating status)","content":"最近在刪除 namespace 的時候總是會卡在 Terminating 的狀態，一直不疑有他的直接使用網路上常看的解決方法將 spec.finalizers 清空。但因為每次刪、每次卡，就連完全無任何資源的命名空間也是卡！仔細看後才發現原來是有其他元件錯誤，進而造成影響。\n原因排查\r先使用清空 finalizer 的方法強制刪除 namespace，\n1kubectl proxy 2Starting to serve on 127.0.0.1:8001 另開一個 terminal\n1cat \u0026lt;\u0026lt;EOF | curl -X PUT \\ 2 localhost:8001/api/v1/namespaces/test/finalize \\ 3 -H \u0026#34;Content-Type: application/json\u0026#34; \\ 4 --data-binary @- 5{ 6 \u0026#34;kind\u0026#34;: \u0026#34;Namespace\u0026#34;, 7 \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, 8 \u0026#34;metadata\u0026#34;: { 9 \u0026#34;name\u0026#34;: \u0026#34;test\u0026#34; 10 }, 11 \u0026#34;spec\u0026#34;: { 12 \u0026#34;finalizers\u0026#34;: null 13 } 14} 15EOF 會回傳下面結果\n1{ 2 \u0026#34;kind\u0026#34;: \u0026#34;Namespace\u0026#34;, 3 \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, 4 \u0026#34;metadata\u0026#34;: { 5 \u0026#34;name\u0026#34;: \u0026#34;test\u0026#34;, 6 \u0026#34;uid\u0026#34;: \u0026#34;353766ea-be97-4ccf-9275-0b39bd651afe\u0026#34;, 7 \u0026#34;resourceVersion\u0026#34;: \u0026#34;1359585\u0026#34;, 8 \u0026#34;creationTimestamp\u0026#34;: \u0026#34;2022-03-21T04:18:38Z\u0026#34;, 9 \u0026#34;deletionTimestamp\u0026#34;: \u0026#34;2022-03-21T04:18:51Z\u0026#34;, 10 \u0026#34;labels\u0026#34;: { 11 \u0026#34;kubernetes.io/metadata.name\u0026#34;: \u0026#34;test\u0026#34; 12 }, 13 \u0026#34;managedFields\u0026#34;: [ 14 { 15 \u0026#34;manager\u0026#34;: \u0026#34;kubectl-create\u0026#34;, 16 \u0026#34;operation\u0026#34;: \u0026#34;Update\u0026#34;, 17 \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, 18 \u0026#34;time\u0026#34;: \u0026#34;2022-03-21T04:18:38Z\u0026#34;, 19 \u0026#34;fieldsType\u0026#34;: \u0026#34;FieldsV1\u0026#34;, 20 \u0026#34;fieldsV1\u0026#34;: { 21 \u0026#34;f:metadata\u0026#34;: { 22 \u0026#34;f:labels\u0026#34;: { 23 \u0026#34;.\u0026#34;: {}, 24 \u0026#34;f:kubernetes.io/metadata.name\u0026#34;: {} 25 } 26 } 27 } 28 }, 29 { 30 \u0026#34;manager\u0026#34;: \u0026#34;kube-controller-manager\u0026#34;, 31 \u0026#34;operation\u0026#34;: \u0026#34;Update\u0026#34;, 32 \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, 33 \u0026#34;time\u0026#34;: \u0026#34;2022-03-21T04:18:57Z\u0026#34;, 34 \u0026#34;fieldsType\u0026#34;: \u0026#34;FieldsV1\u0026#34;, 35 \u0026#34;fieldsV1\u0026#34;: { 36 \u0026#34;f:status\u0026#34;: { 37 \u0026#34;f:conditions\u0026#34;: { 38 \u0026#34;.\u0026#34;: {}, 39 \u0026#34;k:{\\\u0026#34;type\\\u0026#34;:\\\u0026#34;NamespaceContentRemaining\\\u0026#34;}\u0026#34;: { 40 \u0026#34;.\u0026#34;: {}, 41 \u0026#34;f:lastTransitionTime\u0026#34;: {}, 42 \u0026#34;f:message\u0026#34;: {}, 43 \u0026#34;f:reason\u0026#34;: {}, 44 \u0026#34;f:status\u0026#34;: {}, 45 \u0026#34;f:type\u0026#34;: {} 46 }, 47 \u0026#34;k:{\\\u0026#34;type\\\u0026#34;:\\\u0026#34;NamespaceDeletionContentFailure\\\u0026#34;}\u0026#34;: { 48 \u0026#34;.\u0026#34;: {}, 49 \u0026#34;f:lastTransitionTime\u0026#34;: {}, 50 \u0026#34;f:message\u0026#34;: {}, 51 \u0026#34;f:reason\u0026#34;: {}, 52 \u0026#34;f:status\u0026#34;: {}, 53 \u0026#34;f:type\u0026#34;: {} 54 }, 55 \u0026#34;k:{\\\u0026#34;type\\\u0026#34;:\\\u0026#34;NamespaceDeletionDiscoveryFailure\\\u0026#34;}\u0026#34;: { 56 \u0026#34;.\u0026#34;: {}, 57 \u0026#34;f:lastTransitionTime\u0026#34;: {}, 58 \u0026#34;f:message\u0026#34;: {}, 59 \u0026#34;f:reason\u0026#34;: {}, 60 \u0026#34;f:status\u0026#34;: {}, 61 \u0026#34;f:type\u0026#34;: {} 62 }, 63 \u0026#34;k:{\\\u0026#34;type\\\u0026#34;:\\\u0026#34;NamespaceDeletionGroupVersionParsingFailure\\\u0026#34;}\u0026#34;: { 64 \u0026#34;.\u0026#34;: {}, 65 \u0026#34;f:lastTransitionTime\u0026#34;: {}, 66 \u0026#34;f:message\u0026#34;: {}, 67 \u0026#34;f:reason\u0026#34;: {}, 68 \u0026#34;f:status\u0026#34;: {}, 69 \u0026#34;f:type\u0026#34;: {} 70 }, 71 \u0026#34;k:{\\\u0026#34;type\\\u0026#34;:\\\u0026#34;NamespaceFinalizersRemaining\\\u0026#34;}\u0026#34;: { 72 \u0026#34;.\u0026#34;: {}, 73 \u0026#34;f:lastTransitionTime\u0026#34;: {}, 74 \u0026#34;f:message\u0026#34;: {}, 75 \u0026#34;f:reason\u0026#34;: {}, 76 \u0026#34;f:status\u0026#34;: {}, 77 \u0026#34;f:type\u0026#34;: {} 78 } 79 } 80 } 81 }, 82 \u0026#34;subresource\u0026#34;: \u0026#34;status\u0026#34; 83 } 84 ] 85 }, 86 \u0026#34;spec\u0026#34;: {}, 87 \u0026#34;status\u0026#34;: { 88 \u0026#34;phase\u0026#34;: \u0026#34;Terminating\u0026#34;, 89 \u0026#34;conditions\u0026#34;: [ 90 { 91 \u0026#34;type\u0026#34;: \u0026#34;NamespaceDeletionDiscoveryFailure\u0026#34;, 92 \u0026#34;status\u0026#34;: \u0026#34;True\u0026#34;, 93 \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2022-03-21T04:18:56Z\u0026#34;, 94 \u0026#34;reason\u0026#34;: \u0026#34;DiscoveryFailed\u0026#34;, 95 \u0026#34;message\u0026#34;: \u0026#34;Discovery failed for some groups, 1 failing: unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: the server is currently un able to handle the request\u0026#34; 96 }, 97 { 98 \u0026#34;type\u0026#34;: \u0026#34;NamespaceDeletionGroupVersionParsingFailure\u0026#34;, 99 \u0026#34;status\u0026#34;: \u0026#34;False\u0026#34;, 100 \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2022-03-21T04:18:57Z\u0026#34;, 101 \u0026#34;reason\u0026#34;: \u0026#34;ParsedGroupVersions\u0026#34;, 102 \u0026#34;message\u0026#34;: \u0026#34;All legacy kube types successfully parsed\u0026#34; 103 }, 104 { 105 \u0026#34;type\u0026#34;: \u0026#34;NamespaceDeletionContentFailure\u0026#34;, 106 \u0026#34;status\u0026#34;: \u0026#34;False\u0026#34;, 107 \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2022-03-21T04:18:57Z\u0026#34;, 108 \u0026#34;reason\u0026#34;: \u0026#34;ContentDeleted\u0026#34;, 109 \u0026#34;message\u0026#34;: \u0026#34;All content successfully deleted, may be waiting on finalization \u0026#34; 110 }, 111 { 112 \u0026#34;type\u0026#34;: \u0026#34;NamespaceContentRemaining\u0026#34;, 113 \u0026#34;status\u0026#34;: \u0026#34;False\u0026#34;, 114 \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2022-03-21T04:18:57Z\u0026#34;, 115 \u0026#34;reason\u0026#34;: \u0026#34;ContentRemoved\u0026#34;, 116 \u0026#34;message\u0026#34;: \u0026#34;All content successfully removed\u0026#34; 117 }, 118 { 119 \u0026#34;type\u0026#34;: \u0026#34;NamespaceFinalizersRemaining\u0026#34;, 120 \u0026#34;status\u0026#34;: \u0026#34;False\u0026#34;, 121 \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2022-03-21T04:18:57Z\u0026#34;, 122 \u0026#34;reason\u0026#34;: \u0026#34;ContentHasNoFinalizers\u0026#34;, 123 \u0026#34;message\u0026#34;: \u0026#34;All content-preserving finalizers finished\u0026#34; 124 } 125 ] 126 } 127} 可以看到最後的 Terminating 的區塊有解釋原因。\n解決方法\r按照原因解決即可成功刪除 namespace，本文遇到的問題是因為安裝 metric server 時失敗，可以看到 apiservice 其中有出現 false 狀態。\n1kubectl get apiservice 2NAME SERVICE AVAILABLE AGE 3v1. Local True 9d 4v1.admissionregistration.k8s.io Local True 9d 5v1.apiextensions.k8s.io Local True 9d 6v1.apps Local True 9d 7v1.authentication.k8s.io Local True 9d 8v1.authorization.k8s.io Local True 9d 9v1.autoscaling Local True 9d 10v1.batch Local True 9d 11v1.certificates.k8s.io Local True 9d 12v1.coordination.k8s.io Local True 9d 13v1.crd.projectcalico.org Local True 9d 14v1.discovery.k8s.io Local True 9d 15v1.events.k8s.io Local True 9d 16v1.monitoring.coreos.com Local True 3d23h 17v1.networking.k8s.io Local True 9d 18v1.node.k8s.io Local True 9d 19v1.policy Local True 9d 20v1.rbac.authorization.k8s.io Local True 9d 21v1.scheduling.k8s.io Local True 9d 22v1.storage.k8s.io Local True 9d 23v1alpha1.kafka.strimzi.io Local True 3h3m 24v1alpha1.monitoring.coreos.com Local True 3d23h 25v1beta1.batch Local True 9d 26v1beta1.discovery.k8s.io Local True 9d 27v1beta1.events.k8s.io Local True 9d 28v1beta1.flowcontrol.apiserver.k8s.io Local True 9d 29v1beta1.kafka.strimzi.io Local True 3h3m 30v1beta1.metrics.k8s.io kube-system/metrics-server False (MissingEndpoints) 3d1h 31v1beta1.node.k8s.io Local True 9d 32v1beta1.policy Local True 9d 33v1beta1.storage.k8s.io Local True 9d 34v1beta2.core.strimzi.io Local True 3h3m 35v1beta2.flowcontrol.apiserver.k8s.io Local True 9d 36v1beta2.kafka.strimzi.io Local True 3h3m 37v2.autoscaling Local True 9d 38v2beta1.autoscaling Local True 9d 39v2beta2.autoscaling Local True 9d 先暫時移除臨時裝的 metric server\n1helm uninstall metrics-server --namespace kube-system 就可以成功 delete 了\n1[root@node ~]# kubectl create ns test 2namespace/test created 3[root@node ~]# 4[root@node ~]# kubectl delete ns test 5namespace \u0026#34;test\u0026#34; deleted 6[root@node ~# Warning\n另外關於 metric-server 的 debug，紀錄在另外一篇\r文章\r\u0026lt;i class=\u0026quot;fa fa-external-link-alt\u0026quot;\u0026gt;\u0026lt;/i\u0026gt;\r。\n","date":"2022-03-22T14:49:00+00:00","updated":"2022-03-22T14:49:00+00:00"},{"objectID":"1647958860","permalink":"/post/data-debezium-1.5-snapshot-mode-schema-only-not-work/debezium-snapshot-mode-schema-only-not-work/","title":"Debezium 1.5 snapshot.mode schema_only not work","content":"使用 Debezium 1.5 連上 connector 後，發現資料會從資料表的最舊資料一筆資料開始送，查官網後發現有 snapshot.mode 的參數可以設置，但設置完畢後卻發現有 error，本篇記錄解法。\nsnapshot.mode\rDebezium 1.5 版本的 Oracle connector 的配置中，用 snapshot.mode 配置項表示快照模式。默認為 initial 模式，當連接器啟動的時候，會執行一次數據庫初始的一致性快照任務。 第二種快照模式是 schema_only。在這種情況下，connector 仍會捕獲相關表的結構，但它不會在啟動時產生 READ 事件創建完整數據集。故如果只對從現在開始的數據更改感興趣，而不對所有記錄的完整當前狀態感興趣，須將模式改為 schema_only。\n錯誤\r在 connector 的設定項加上 \u0026quot;snapshot.mode\u0026quot;: \u0026quot;schema_only\u0026quot; 後，發現 connector 會報以下錯誤：\n12022-03-22 05:06:45,029 ERROR || WorkerSourceTask{id=oracle2-0} Task threw an uncaught and unrecoverable exception. Task i s being killed and will not recover until manually restarted [org.apache.kafka.connect.runtime.WorkerTask] 2org.apache.kafka.connect.errors.ConnectException: An exception occurred in the change event producer. This connector will be stopped. 3 at io.debezium.pipeline.ErrorHandler.setProducerThrowable(ErrorHandler.java:42) 4 at io.debezium.connector.oracle.logminer.LogMinerStreamingChangeEventSource.execute(LogMinerStreamingChangeEventSour ce.java:208) 5 at io.debezium.pipeline.ChangeEventSourceCoordinator.streamEvents(ChangeEventSourceCoordinator.java:152) 6 at io.debezium.pipeline.ChangeEventSourceCoordinator.lambda$start$0(ChangeEventSourceCoordinator.java:119) 7 at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) 8 at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264) 9 at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) 10 at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) 11 at java.base/java.lang.Thread.run(Thread.java:834) 12Caused by: io.debezium.DebeziumException: Supplemental logging not configured for table EMESHY.EMESP.tp_sn_log. Use command : ALTER TABLE EMESP.tp_sn_log ADD SUPPLEMENTAL LOG DATA (ALL) COLUMNS 13 at io.debezium.connector.oracle.logminer.LogMinerHelper.checkSupplementalLogging(LogMinerHelper.java:407) 14 at io.debezium.connector.oracle.logminer.LogMinerStreamingChangeEventSource.execute(LogMinerStreamingChangeEventSour ce.java:132) 15 ... 7 more 錯誤排查\r按照錯誤原因是指定的表沒有設置 Supplemental Log，但理應不可能，因為在預設的 snapshot mode initial 時是可連接成功的，還是按照指示進入資料庫設置，\n然結果不出所料的回報已經設置了，使用網路上的另一個 command 查看 ALL_LOG_GROUP 的配置：\n一樣是顯示在我指定的 TP_SN_LOG 表有設置補充日誌。 無意間在 community 發現也有人遇到一樣的錯誤(\rgitter thread\r)，發現可能是 bug，其實可以看到錯誤訊息的內容把原本應該大寫的表名 TP_SN_LOG 轉成小寫 tp_sn_log。查看 jira issue\r提到，照理來說 oracle 標識符在查詢中不區分大小寫，除非它們在字符串謂詞中顯式使用，即 “WHERE TABLE_NAME = \u0026rsquo;table\u0026rsquo;”，或者標識符是用雙引號創建的。但我在送 config 時就是使用大寫押~~怪!\n解決方式\r暫時在 connector 設定項加上 database.tablename.case.insensitive 參數，值設為 false，即可順利解決。超怪 =___=，非大小寫不敏感不就是指敏感的意思嘛!!! 只能當作是 1.5 的 bug 了。\n附上最終的 config\n1{ 2 \u0026#34;name\u0026#34;:\u0026#34;oracle2\u0026#34;, 3 \u0026#34;connector.class\u0026#34;:\u0026#34;io.debezium.connector.oracle.OracleConnector\u0026#34;, 4 \u0026#34;tasks.max\u0026#34;:\u0026#34;1\u0026#34;, 5 \u0026#34;database.hostname\u0026#34;:\u0026#34;10.90.1.207\u0026#34;, 6 \u0026#34;database.port\u0026#34;:\u0026#34;1521\u0026#34;, 7 \u0026#34;database.user\u0026#34;:\u0026#34;logminer\u0026#34;, 8 \u0026#34;database.password\u0026#34;:\u0026#34;logminer\u0026#34;, 9 \u0026#34;database.dbname\u0026#34;:\u0026#34;EMESHY\u0026#34;, 10 \u0026#34;database.server.name\u0026#34;:\u0026#34;oracle\u0026#34;, 11 \u0026#34;database.history.kafka.bootstrap.servers\u0026#34;:\u0026#34;kafka:9092\u0026#34;, 12 \u0026#34;database.history.kafka.topic\u0026#34;:\u0026#34;tpsn2\u0026#34;, 13 \u0026#34;database.connection.adapter\u0026#34;:\u0026#34;logminer\u0026#34;, 14 \u0026#34;table.include.list\u0026#34;:\u0026#34;EMESP.TP_SN_LOG\u0026#34;, 15 \u0026#34;log.mining.strategy\u0026#34;:\u0026#34;online_catalog\u0026#34;, 16 \u0026#34;snapshot.mode\u0026#34;: \u0026#34;schema_only\u0026#34;, 17 \u0026#34;database.tablename.case.insensitive\u0026#34;: \u0026#34;false\u0026#34; 18} Reference\rhttps://debezium.io/documentation/reference/1.5/connectors/oracle.html#oracle-snapshots\rhttps://gitter.im/debezium/user?at=6034b854e634904e60ba19a4\r","date":"2022-03-22T14:21:00+00:00","updated":"2022-03-22T14:21:00+00:00"},{"objectID":"1647902005","permalink":"/post/data-install-strimzi-kafka-operator-to-watch-all-namespace/install-strimzi-kafka-operator-to-watch-all-namespace/","title":"安裝監聽於所有 namespace 的 strimzi kafka operator, 便於在不同 namespace 下安裝不同座 Kafka cluster","content":"以下紀錄如何隔離 kafka operator 與建立的 kafka cluster 的命名空間，預計是可以在一座 kubernetes cluster 上只需安裝一個 kafka operator 來建立多個 kafka cluster。\n安裝 operator\r下載 CRD 資源\n1wget https://github.com/strimzi/strimzi-kafka-operator/releases/download/0.28.0/strimzi-0.28.0.tar.gz -P /tmp \u0026amp;\u0026amp; tar zxvf /tmp/strimzi-0.28.0.tar.gz -C /tmp 修改 strimzi-0.28.0/install/cluster-operator/060-Deployment-strimzi-cluster-operator.yaml 檔案中的 STRIMZI_NAMESPACE 的值，改為監聽所有 \u0026quot;*\u0026quot; namespace。\n1# vim /tmp/strimzi-0.28.0/install/cluster-operator/060-Deployment-strimzi-cluster-operator.yaml 2apiVersion: apps/v1 3kind: Deployment 4spec: 5 # ... 6 template: 7 spec: 8 # ... 9 serviceAccountName: strimzi-cluster-operator 10 containers: 11 - name: strimzi-cluster-operator 12 image: strimzi/operator:0.16.2 13 imagePullPolicy: IfNotPresent 14 env: 15 - name: STRIMZI_NAMESPACE 16 value: \u0026#34;*\u0026#34; 17 # ... 以上可以用下面一行 command 解決\n1sed -z \u0026#39;s/ valueFrom:\\n fieldRef:\\n fieldPath: metadata.namespace/ value: \\\u0026#34;*\\\u0026#34;/\u0026#39; -i /tmp/strimzi-0.28.0/install/cluster-operator/060-Deployment-strimzi-cluster-operator.yaml 建立 ClusterRoleBindings，使可授權 Cluster Operator 存取 cluster-wide 的所有 namespaces\n1kubectl create clusterrolebinding strimzi-cluster-operator-namespaced --clusterrole=strimzi-cluster-operator-namespaced --serviceaccount system-kafka-operator:strimzi-cluster-operator 2kubectl create clusterrolebinding strimzi-cluster-operator-entity-operator-delegation --clusterrole=strimzi-entity-operator --serviceaccount system-kafka-operator:strimzi-cluster-operator 3kubectl create clusterrolebinding strimzi-cluster-operator-topic-operator-delegation --clusterrole=strimzi-topic-operator --serviceaccount system-kafka-operator:strimzi-cluster-operator 正式佈署 operator\n1kubectl apply -f /tmp/strimzi-0.28.0/install/cluster-operator -n system-kafka-operator 查看資源\n1[root@node ~]# kubectl get all -n system-kafka-operator 2NAME READY STATUS RESTARTS AGE 3pod/strimzi-cluster-operator-7548bd689f-xlr9j 1/1 Running 0 29m 4 5NAME READY UP-TO-DATE AVAILABLE AGE 6deployment.apps/strimzi-cluster-operator 1/1 1 1 29m 7 8NAME DESIRED CURRENT READY AGE 9replicaset.apps/strimzi-cluster-operator-7548bd689f 1 1 1 29m 安裝 kafka cluster\r準備 kafka.yaml\n1apiVersion: kafka.strimzi.io/v1beta2 2kind: Kafka 3metadata: 4 name: my-cluster 5 namespace: kafka 6spec: 7 kafka: 8 version: 3.1.0 9 replicas: 3 10 listeners: 11 - name: plain 12 port: 9092 13 type: internal 14 tls: false 15 - name: tls 16 port: 9093 17 type: internal 18 tls: true 19 - name: external 20 port: 9094 21 type: loadbalancer 22 tls: false 23 config: 24 default.replication.factor: 3 25 num.partitions: 1 26 offsets.topic.replication.factor: 3 27 transaction.state.log.replication.factor: 3 28 transaction.state.log.min.isr: 2 29 log.message.format.version: \u0026#34;3.1\u0026#34; 30 storage: 31 class: nfs 32 type: jbod 33 volumes: 34 - id: 0 35 type: persistent-claim 36 size: 10Gi 37 deleteClaim: false 38 zookeeper: 39 replicas: 3 40 storage: 41 class: nfs 佈署 cluster\n1kubectl apply -f kafka.yaml 查看資源\n1[root@node terraform]# kubectl get all -n kafka 2NAME READY STATUS RESTARTS AGE 3pod/my-cluster-kafka-0 1/1 Running 0 25s 4pod/my-cluster-kafka-1 1/1 Running 0 25s 5pod/my-cluster-kafka-2 1/1 Running 0 25s 6pod/my-cluster-zookeeper-0 1/1 Running 0 52s 7pod/my-cluster-zookeeper-1 1/1 Running 0 52s 8pod/my-cluster-zookeeper-2 1/1 Running 0 52s 9 10NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE 11service/my-cluster-kafka-0 LoadBalancer 10.109.143.19 10.1.5.151 9094:32608/TCP 27s 12service/my-cluster-kafka-1 LoadBalancer 10.96.7.88 10.1.5.154 9094:32057/TCP 27s 13service/my-cluster-kafka-2 LoadBalancer 10.99.44.55 10.1.5.152 9094:31265/TCP 27s 14service/my-cluster-kafka-bootstrap ClusterIP 10.105.207.155 \u0026lt;none\u0026gt; 9091/TCP,9092/TCP,9093/TCP 27s 15service/my-cluster-kafka-brokers ClusterIP None \u0026lt;none\u0026gt; 9090/TCP,9091/TCP,9092/TCP,9093/TCP 27s 16service/my-cluster-kafka-external-bootstrap LoadBalancer 10.96.136.137 10.1.5.153 9094:30262/TCP 27s 17service/my-cluster-zookeeper-client ClusterIP 10.107.192.225 \u0026lt;none\u0026gt; 2181/TCP 53s 18service/my-cluster-zookeeper-nodes ClusterIP None \u0026lt;none\u0026gt; 2181/TCP,2888/TCP,3888/TCP 53s 19 20NAME READY AGE 21statefulset.apps/my-cluster-kafka 3/3 25s 22statefulset.apps/my-cluster-zookeeper 3/3 52s Reference\rhttps://strimzi.io/docs/0.16.2/full.html#deploying-cluster-operator-to-watch-whole-cluster-deploying-co\r","date":"2022-03-21T22:33:25+00:00","updated":"2022-03-21T22:33:25+00:00"},{"objectID":"1647898080","permalink":"/post/k8s-helm-install-metrics-server/kubernets-install-metrics-server-by-helm/","title":"使用 helm 安裝 Metrics Server","content":"Metrics Server 通過 kubelet（cAdvisor）獲取監控數據，主要作用是為 kube-scheduler、HPA(Horizontal Pod Autoscaler)等 k8s 核心組件，以及 kubectl top 命令和 Dashboard 等 UI 組件提供數據來源，可以用來看 node 或 pod 的資源 (CPU \u0026amp; Memory) 消耗。須注意的是，Metric Server 是 in memory 的 monitor，只可以查詢當前的度量數據，並不保存歷史數據。\n安裝\r1helm repo add metrics-server https://kubernetes-sigs.github.io/metrics-server/ 2helm upgrade --install metrics-server metrics-server/metrics-server --namespace kube-system 3Release \u0026#34;metrics-server\u0026#34; does not exist. Installing it now. 4NAME: metrics-server 5LAST DEPLOYED: Mon Mar 21 16:32:14 2022 6NAMESPACE: kube-system 7STATUS: deployed 8REVISION: 1 9TEST SUITE: None 10NOTES: 11*********************************************************************** 12* Metrics Server * 13*********************************************************************** 14 Chart version: 3.8.2 15 App version: 0.6.1 16 Image tag: k8s.gcr.io/metrics-server/metrics-server:v0.6.1 17*********************************************************************** 連線失敗問題\r可以發現使用預設值安裝完後 deployment 一直無法 ready，查看 deployment 資訊\n1# kubectl describe deployments.apps -n kube-system metrics-server 2Name: metrics-server 3Namespace: kube-system 4CreationTimestamp: Mon, 21 Mar 2022 16:32:17 +0800 5Labels: app.kubernetes.io/instance=metrics-server 6 app.kubernetes.io/managed-by=Helm 7 app.kubernetes.io/name=metrics-server 8 app.kubernetes.io/version=0.6.1 9 helm.sh/chart=metrics-server-3.8.2 10Annotations: deployment.kubernetes.io/revision: 1 11 meta.helm.sh/release-name: metrics-server 12 meta.helm.sh/release-namespace: kube-system 13Selector: app.kubernetes.io/instance=metrics-server,app.kubernetes.io/name=metrics-server 14Replicas: 1 desired | 1 updated | 1 total | 0 available | 1 unavailable 15StrategyType: RollingUpdate 16MinReadySeconds: 0 17RollingUpdateStrategy: 25% max unavailable, 25% max surge 18Pod Template: 19 Labels: app.kubernetes.io/instance=metrics-server 20 app.kubernetes.io/name=metrics-server 21 Service Account: metrics-server 22 Containers: 23 metrics-server: 24 Image: k8s.gcr.io/metrics-server/metrics-server:v0.6.1 25 Port: 4443/TCP 26 Host Port: 0/TCP 27 Args: 28 --secure-port=4443 29 --cert-dir=/tmp 30 --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname 31 --kubelet-use-node-status-port 32 --metric-resolution=15s 33 Liveness: http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3 34 Readiness: http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3 35 Environment: \u0026lt;none\u0026gt; 36 Mounts: 37 /tmp from tmp (rw) 38 Volumes: 39 tmp: 40 Type: EmptyDir (a temporary directory that shares a pod\u0026#39;s lifetime) 41 Medium: 42 SizeLimit: \u0026lt;unset\u0026gt; 43 Priority Class Name: system-cluster-critical 44Conditions: 45 Type Status Reason 46 ---- ------ ------ 47 Available False MinimumReplicasUnavailable 48 Progressing True ReplicaSetUpdated 49OldReplicaSets: \u0026lt;none\u0026gt; 50NewReplicaSet: metrics-server-7d76b744cd (1/1 replicas created) 51Events: 52 Type Reason Age From Message 53 ---- ------ ---- ---- ------- 54 Normal ScalingReplicaSet 3m50s deployment-controller Scaled up replica set metrics-server-7d76b744cd to 1 查看 Pod log\n1kubectl logs -f -n kube-system metrics-server-7d76b744cd-fv9ns 連線失敗問題修正\r加上 --kubelet-insecure-tls 啟動參數\n1kubectl patch -n kube-system deployment metrics-server --type=json -p \u0026#39;[{\u0026#34;op\u0026#34;:\u0026#34;add\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/spec/template/spec/containers/0/args/-\u0026#34;,\u0026#34;value\u0026#34;:\u0026#34;--kubelet-insecure-tls\u0026#34;}]\u0026#39; 測試\r查看節點資源消耗\n1[root@node ~]# kubectl top nodes 2NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% 3node 312m 7% 2769Mi 36% 4rockyw 234m 2% 3335Mi 21% 5rockyw2 199m 2% 2727Mi 17% Reference\r在找資料的時候發現以下兩篇針對 metric server 的介紹，值得拜讀。\nKubernetes Monitoring 101 — Core pipeline \u0026amp; Services Pipeline\rKubernetes 自動彈性伸縮\r","date":"2022-03-21T21:28:00+00:00","updated":"2022-03-21T21:28:00+00:00"},{"objectID":"1647454980","permalink":"/post/devops-containerd-crictl-configure-private-img-registry/containerd-configure-private-harbor/","title":"Containerd (crictl) 配置私有鏡像倉庫","content":"前幾天將 k8s 升級到 1.23 版，使用 containerd 當 CRI，立馬就遇到要拉取私有 image registry 的狀況，本文紀錄配置過程。\n配置 containerd config.toml\r依據遠端私有倉庫是否加密，有不同配置：\n- 私有倉庫帶有 tls 加密 (https)\r1vim /etc/containerd/config.toml 在 [plugins.\u0026quot;io.containerd.grpc.v1.cri\u0026quot;.registry.mirror] 下加上私有倉庫位址/URL\n1[plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry.mirrors.\u0026#34;harbor.nexmasa.com\u0026#34;] 2 endpoint = [\u0026#34;https://harbor.nexmasa.com\u0026#34;] 另外再接著加上 [plugins.\u0026quot;io.containerd.grpc.v1.cri\u0026quot;.registry.configs] 的項目\n1[plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry.configs] 2 [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry.configs.\u0026#34;harbor.nexmasa.com\u0026#34;.tls] 3 ca_file = \u0026#34;/etc/containerd/harbor.nexmasa.com/ca.pem\u0026#34; 4 cert_file = \u0026#34;/etc/containerd/harbor.nexmasa.com/cert.pem\u0026#34; 5 key_file = \u0026#34;/etc/containerd/harbor.nexmasa.com/key.pem\u0026#34; 6 [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry.configs.\u0026#34;harbor.nexmasa.com\u0026#34;.auth] 7 username = \u0026#34;xxxxxx\u0026#34; 8 password = \u0026#39;xxxxxx\u0026#39; - 私有倉庫無加密 (http) 或是想略過憑證檢查\r在 tls 配置項下，以 insecure_skip_verify = true 參數取代上面配置的金鑰及憑證。\n1[plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry.configs] 2 [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry.configs.\u0026#34;harbor.nexmasa.com\u0026#34;.tls] 3 insecure_skip_verify = true 重啟 containerd\r1systemctl restart containerd 查看配置\n1[root@node ~]# crictl info 2{ 3 \u0026#34;status\u0026#34;: { 4 \u0026#34;conditions\u0026#34;: [ 5 { 6 \u0026#34;type\u0026#34;: \u0026#34;RuntimeReady\u0026#34;, 7 \u0026#34;status\u0026#34;: true, 8 \u0026#34;reason\u0026#34;: \u0026#34;\u0026#34;, 9 \u0026#34;message\u0026#34;: \u0026#34;\u0026#34; 10 }, 11 { 12 \u0026#34;type\u0026#34;: \u0026#34;NetworkReady\u0026#34;, 13 \u0026#34;status\u0026#34;: true, 14 \u0026#34;reason\u0026#34;: \u0026#34;\u0026#34;, 15 \u0026#34;message\u0026#34;: \u0026#34;\u0026#34; 16 } 17 ] 18 }, 19. 20. 21. 22\u0026#34;config\u0026#34;: { 23. 24. 25. 26 \u0026#34;registry\u0026#34;: { 27 \u0026#34;mirrors\u0026#34;: { 28 \u0026#34;docker.io\u0026#34;: { 29 \u0026#34;endpoint\u0026#34;: [ 30 \u0026#34;https://registry-1.docker.io\u0026#34; 31 ] 32 }, 33 \u0026#34;harbor.nexmasa.com\u0026#34;: { 34 \u0026#34;endpoint\u0026#34;: [ 35 \u0026#34;https://harbor.nexmasa.com\u0026#34; 36 ] 37 } 38 }, 39 \u0026#34;configs\u0026#34;: { 40 \u0026#34;harbor.nexmasa.com\u0026#34;: { 41 \u0026#34;auth\u0026#34;: { 42 \u0026#34;username\u0026#34;: \u0026#34;xxxxx\u0026#34;, 43 \u0026#34;password\u0026#34;: \u0026#34;xxxxx\u0026#34;, 44 \u0026#34;auth\u0026#34;: \u0026#34;\u0026#34;, 45 \u0026#34;identitytoken\u0026#34;: \u0026#34;\u0026#34; 46 }, 47 \u0026#34;tls\u0026#34;: { 48 \u0026#34;insecure_skip_verify\u0026#34;: false, 49 \u0026#34;caFile\u0026#34;: \u0026#34;/etc/containerd/harbor.nexmasa.com/ca.pem\u0026#34;, 50 \u0026#34;certFile\u0026#34;: \u0026#34;/etc/containerd/harbor.nexmasa.com/cert.pem\u0026#34;, 51 \u0026#34;keyFile\u0026#34;: \u0026#34;/etc/containerd/harbor.nexmasa.com/key.pem\u0026#34; 52 } 53 } 54 }, 55 \u0026#34;auths\u0026#34;: null, 56 \u0026#34;headers\u0026#34;: null 57 }, 測試\r拉取鏡像\n1crictl pull harbor.nexmasa.com/test/hello-world@sha256:90659bf80b44ce6be8234e6ff90a1ac34acbeb826903b02cfa0da11c82cbc042 Reference\rhttps://github.com/containerd/containerd/blob/main/docs/cri/registry.md\r","date":"2022-03-16T18:23:00+00:00","updated":"2022-03-16T18:23:00+00:00"},{"objectID":"1647287580","permalink":"/post/database-mongodb-oom-crash/mongodb-oom-crash/","title":"Mongodb 可能是讓機器 OOM crash 的元兇 ?!","content":"問題\r手上有三台各有 160GB 的機器，但卻會在跑一段時間後輪流死當。觀察記憶體消耗之後，發現兇手就是 MongoDB!!!\n原因\rMongoDB 使用記憶體映射存儲引擎 (Memory Mapped Storage Engine) WiredTiger，是 MongoDB 3.2 版之後的默認引擎，它會把磁盤 IO 操作轉換成記憶體操作，如果是讀操作，記憶體中的數據起到緩存的作用，如果是寫操作，記憶體還可以把隨機的寫操作轉換成順序的寫操作，總之可以大幅度提升性能。 截自 Mongodb doucument\r1With WiredTiger, MongoDB utilizes both the WiredTiger internal cache and the filesystem cache. 2Starting in MongoDB 3.4, the default WiredTiger internal cache size is the larger of either: 3- 50% of (RAM - 1 GB), or 4- 256 MB. 因為伺服器記憶體大小為 160GB，一個 Mongodb 就可能會占掉近 80GB 的記憶體。 此時服務器上若還有跑其他 Mongodb 或應用程序的話，就會導致記憶體不足而退出。\n解決方法\r在 mongodb 啟動時設定\rrun on baremetal 1mongod --wiredTigerCacheSizeGB 10 run on docker 1docker run --name mymongo -d mongo --wiredTigerCacheSizeGB 10 run on kubernetes 1cat \u0026lt;\u0026lt;EOF | kubectl apply -f - 2kind: Deployment 3apiVersion: apps/v1 4metadata: 5 name: mongodb 6spec: 7 replicas: 1 8 selector: 9 matchLabels: 10 app: mongodb 11 template: 12 metadata: 13 labels: 14 app: mongodb 15 spec: 16 containers: 17 - image: mongo 18 name: mongodb 19 ports: 20 - containerPort: 27017 21 name: mongodb 22 protocol: TCP 23 imagePullPolicy: IfNotPresent 24 command: [\u0026#34;docker-entrypoint.sh\u0026#34;] 25 args: [\u0026#34;mongod\u0026#34;, \u0026#34;--wiredTigerCacheSizeGB\u0026#34;, \u0026#34;10\u0026#34;] 26 restartPolicy: Always 27--- 28apiVersion: v1 29kind: Service 30metadata: 31 name: mongodb 32 labels: 33 name: mongodb 34spec: 35 ports: 36 - port: 27017 37 name: mongo-port 38 protocol: TCP 39 targetPort: 27017 40 selector: 41 app: mongodb 42 type: NodePort 43EOF mongodb 運行時設定\r如果 mongodb 已經啟動的話，可以使用下面方式動態設定\n1db.adminCommand({setParameter: 1, wiredTigerEngineRuntimeConfig: \u0026#34;cache_size=10G\u0026#34;}) 查看設定結果\r1db.serverStatus().wiredTiger.cache[\u0026#39;maximum bytes configured\u0026#39;]/1024/1024/1024 Reference\rhttps://docs.mongodb.com/manual/reference/program/mongod/\rhttps://stackoverflow.com/questions/64809287/how-to-set-wiredtigercachesize-in-mongodb-when-deployed-in-kubernetes/64859613#64859613\rhttps://blog.csdn.net/LuyaoYing001/article/details/75576820\r","date":"2022-03-14T19:53:00+00:00","updated":"2022-03-14T19:53:00+00:00"},{"objectID":"1647183360","permalink":"/post/k8s-1.23-installation/install-kubernetes-123-on-rocky-linux/","title":"在 Rocky Linux 8 安裝 Kubernetes 1.23 (containerd as cri)","content":"kubernetes 1.22 版之後，就不再支持 Docker 作為 container runtime 以及管理容器及鏡像的工具了。可以使用 containerd 取代 docker 的 container runtime；以及 crictl 作為 CRI(Container Runtime Interface)，另外 podman 也可以用來管理容器和鏡像。本篇記錄基於 containerd \u0026amp;amp; crictl 使用 kubeadm 部屬 Kubernetes 集群的過程。\n系統環境配置 (所有節點)\r最小系統資源需求\r每台機器 4 GiB 以上 RAM master control plane 節點至少需要有兩個以上的 vCPU 集群中所有機器之間的完整網絡連接 (can be private or public) Server Type Hostname Spec master node.ulatest.com 4 vCPU, 8G RAM worker rockyw.ulatest.com 8 vCPU, 16G RAM worker rockyw2.ulatest.com 8 vCPU, 16G RAM 配置 /etc/hosts\r1cat /etc/hosts 2127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 3::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 4 510.1.5.130 nexdata nexdata.ulatest.com 610.1.5.146 node node.ulatest.com 710.1.5.147 rockyw rockyw.ulatest.com 810.1.5.148 rockyw2 rockyw2.ulatest.com 9 1010.1.5.130 nfs nfs.ulatest.com 更新軟體套件\r1yum update -y 系統配置\r停用防火牆\r1systemctl stop firewalld 2systemctl disable firewalld 關閉 SELINUX\r1sed -i …","date":"2022-03-13T14:56:00+00:00","updated":"2022-03-13T14:56:00+00:00"},{"objectID":"1647182760","permalink":"/post/k8s-install-kubernetes-dashboard/kubernets-dashboard-installation/","title":"安裝 Kubernetes Dashboard - 單集群可視化管理","content":"Kubernetes Dashboard 是由官方維護的 Kubernetes 集群 WEB UI 管理工具，能查看 Kubernetes Cluster 上資源分佈與使用狀況，也可以創建或者修改 Kubernetes 資源，讓使用者透過 Web UI 介面取代指令的管理 Kubernetes。\n安裝\r安裝非常簡單，只要透過下面 command 即可部屬。\n1kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.5.1/aio/deploy/recommended.yaml 確認安裝結果\n1[root@ula ~]# kubectl get all -n kubernetes-dashboard 2NAME READY STATUS RESTARTS AGE 3pod/dashboard-metrics-scraper-799d786dbf-zhvlc 1/1 Running 0 24s 4pod/kubernetes-dashboard-fb8648fd9-wd4t5 1/1 Running 0 24s 5 6NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE 7service/dashboard-metrics-scraper ClusterIP 10.111.121.48 \u0026lt;none\u0026gt; 8000/TCP 24s 8service/kubernetes-dashboard ClusterIP 10.107.248.188 \u0026lt;none\u0026gt; 443/TCP 25s 9 10NAME READY UP-TO-DATE AVAILABLE AGE 11deployment.apps/dashboard-metrics-scraper 1/1 1 1 24s 12deployment.apps/kubernetes-dashboard 1/1 1 1 25s 13 14NAME DESIRED CURRENT READY AGE 15replicaset.apps/dashboard-metrics-scraper-799d786dbf 1 1 1 24s 16replicaset.apps/kubernetes-dashboard-fb8648fd9 1 1 1 24s 訪問\r將 kubernetes-dashboard 服務暴露 NodePort\n1kubectl edit svc -n kubernetes-dashboard kubernetes-dashboard 將原本 type: ClusterIP 改成 type: NodePort。完成後就可以使用 https://NodeIP:nodePort 地址訪問 dashboard。\n1[root@ula ~]# kubectl get svc -n kubernetes-dashboard kubernetes-dashboard 2NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE 3kubernetes-dashboard NodePort 10.107.248.188 \u0026lt;none\u0026gt; 443:31302/TCP 44m 由於 Dashboard 默認使用 https，其證書不受瀏覽器信任，所以訪問時加上 https 強制跳轉就可以了。\n登入\r登錄 Dashboard 支持 Kubeconfig 和 Token 兩種認證方式，Kubeconfig 中也依賴 token 字段，所以生成 token 這一步是必不可少的。下面紀錄使用 token 的方式登錄。\n建立 service account \u0026amp; role binding\r準備 yaml 檔 sc-ula.yaml\n1kind: ClusterRoleBinding 2apiVersion: rbac.authorization.k8s.io/v1beta1 3metadata: 4 name: ula 5 annotations: 6 rbac.authorization.kubernetes.io/autoupdate: \u0026#34;true\u0026#34; 7roleRef: 8 kind: ClusterRole 9 name: cluster-admin # k8s 預設建立的角色 10 apiGroup: rbac.authorization.k8s.io 11subjects: 12- kind: ServiceAccount 13 name: ula 14 namespace: kube-system 15 16 --- 17 apiVersion: v1 18 kind: ServiceAccount 19 metadata: 20 name: ula 21 namespace: kube-system 22 labels: 23 kubernetes.io/cluster-service: \u0026#34;true\u0026#34; 24 addonmanager.kubernetes.io/mode: Reconcile 使用 kubectl apply 建立\n1kubectl apply -f sc-ula.yaml 取得 Server Account Token\r查看 service account secret\n1kubectl get sa ula -n kube-system -o=yaml 2apiVersion: v1 3kind: ServiceAccount 4metadata: 5 creationTimestamp: \u0026#34;2022-03-14T03:12:41Z\u0026#34; 6 name: ula 7 namespace: kube-system 8 resourceVersion: \u0026#34;332586\u0026#34; 9 uid: f048e242-5947-4945-8310-432628e635b7 10secrets: 11- name: ula-token-qwddr 取得 token 字段，並使用 base64 decode\n1kubectl get secret ula-token-qwddr -o jsonpath={.data.token} -n kube-system | base64 -d 2eyJhbGciOiJSUzI1NiIsImtpZCI6IjVadWpvSGEtR2tMR2ZFMGVHaGloWjJNOFdRcn............... 然後在 dashboard 登錄頁面上直接使用上面得到的 token 字符串即可登錄，這樣就可以擁有管理員權限操作整個 kubernetes 集群的對象，也可以新建一個指定操作權限的用戶。\nReference\rhttps://github.com/kubernetes/dashboard\rhttps://kubernetes.io/zh/docs/tasks/access-application-cluster/web-ui-dashboard/\r","date":"2022-03-13T14:46:00+00:00","updated":"2022-03-13T14:46:00+00:00"},{"objectID":"1646862852","permalink":"/post/data-install-kafka-by-docker-compose/install-kafka-by-docker-compose/","title":"使用 docker compose 安裝 kafka","content":"安裝單節點 Kafka\r1. 準備 docker-compose.yaml 檔案\r1version: \u0026#39;3\u0026#39; 2services: 3 zookeeper: 4 restart: always 5 image: confluentinc/cp-zookeeper:latest 6 environment: 7 ZOOKEEPER_CLIENT_PORT: 2181 8 ZOOKEEPER_TICK_TIME: 2000 9 ports: 10 - 22181:2181 11 volumes: 12 - ./zoo/data:/var/lib/zookeeper/data 13 - ./zoo/log:/var/lib/zookeeper/log 14 15 kafka: 16 restart: always 17 image: confluentinc/cp-kafka:latest 18 depends_on: 19 - zookeeper 20 ports: 21 - 29092:29092 22 volumes: 23 - ./kafka/data:/var/lib/kafka/data 24 environment: 25 KAFKA_BROKER_ID: 1 26 KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181 27 KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://10.13.1.100:29092 28 KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT 29 KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT 30 KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1 在使用 docker 或是公有雲部屬 kafka 時，需要用到 KAFKA_ADVERTISED_LISTENER 參數。其中第一個值是真正建立 kafka broker 用的，第二個數值是用於對外發布的服務端口。 KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR 用來配置 __consumer_offsets 副本數。 2. 啟動 container\r1docker-compose up -d 2Creating network \u0026#34;kafka_default\u0026#34; with the default driver 3Creating kafka_zookeeper_1 ... done 4Creating kafka_kafka_1 ... done 3. 查看服務\r1docker ps 2CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 39afa79af69e7 confluentinc/cp-kafka:latest \u0026#34;/etc/confluent/dock…\u0026#34; 39 minutes ago Up 39 minutes 9092/tcp, 0.0.0.0:29092-\u0026gt;29092/tcp, :::29092-\u0026gt;29092/tcp kafka_kafka_1 42445639c8af9 confluentinc/cp-zookeeper:latest \u0026#34;/etc/confluent/dock…\u0026#34; 39 minutes ago Up 39 minutes 2888/tcp, 3888/tcp, 0.0.0.0:22181-\u0026gt;2181/tcp, :::22181-\u0026gt;2181/tcp kafka_zookeeper_1 4. 檢查運作\r使用下載 kafka 套件提供的 script 測試\nlist topic，若無錯誤訊息則代表建立並連線 kafka broker 成功。 1[root@ula bin]# ./kafka-topics.sh --list --bootstrap-server 10.13.1.100:29092 2 3[root@ula bin]# produce message 1[root@ula bin]# ./kafka-console-producer.sh --bootstrap-server 10.13.1.100:29092 --topic test 2\u0026gt;life 3\u0026gt;is 4\u0026gt;what 5\u0026gt;you 6\u0026gt;make 7\u0026gt;it 8\u0026gt;! 9\u0026gt;^C[root@ula bin]# consume message 1[root@ula bin]# 2[root@ula bin]# ./kafka-console-consumer.sh --bootstrap-server 10.13.1.100:29092 --topic test --from-beginning 3life 4is 5what 6you 7make 8it 9! 10^CProcessed a total of 7 messages 安裝 Kafka Cluster\r1--- 2version: \u0026#39;3\u0026#39; 3services: 4 zookeeper-1: 5 image: confluentinc/cp-zookeeper:latest 6 environment: 7 ZOOKEEPER_CLIENT_PORT: 2181 8 ZOOKEEPER_TICK_TIME: 2000 9 ports: 10 - 22181:2181 11 zookeeper-2: 12 image: confluentinc/cp-zookeeper:latest 13 environment: 14 ZOOKEEPER_CLIENT_PORT: 2181 15 ZOOKEEPER_TICK_TIME: 2000 16 ports: 17 - 32181:2181 18 kafka-1: 19 image: confluentinc/cp-kafka:latest 20 depends_on: 21 - zookeeper-1 22 - zookeeper-2 23 ports: 24 - 29092:29092 25 environment: 26 KAFKA_BROKER_ID: 1 27 KAFKA_ZOOKEEPER_CONNECT: zookeeper-1:2181,zookeeper-2:2181 28 KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-1:9092,PLAINTEXT_HOST://10.13.1.100:29092 29 KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT 30 KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT 31 KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 2 32 kafka-2: 33 image: confluentinc/cp-kafka:latest 34 depends_on: 35 - zookeeper-1 36 - zookeeper-2 37 ports: 38 - 39092:39092 39 environment: 40 KAFKA_BROKER_ID: 2 41 KAFKA_ZOOKEEPER_CONNECT: zookeeper-1:2181,zookeeper-2:2181 42 KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-2:9092,PLAINTEXT_HOST://10.13.1.100:39092 43 KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT 44 KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT 45 KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 2 ","date":"2022-03-09T21:54:12+00:00","updated":"2022-03-09T21:54:12+00:00"},{"objectID":"1646335260","permalink":"/post/os-linux-eat-all-memory/linux-eat-all-memory/","title":"Linux 吃掉了記憶體 ?!","content":"前言\r在觀察用於儲存 k8s 的 nfs server 時，發現記憶體的 cache/buffer 值非常高，但細看 top 卻無任何應用程式佔用記憶體空間。查了一下網路，發現是 linux 系統本身的機制。\n原因\rLinux 會借用未使用的記憶體來做磁碟快取，可以讓應用載入更快並且執行更加流暢，提高 IO 性能。如果有其他應用要用到記憶體時，系統會從磁碟快取中拿走一塊被借用的記憶體，不會用到 swap。\nfree 命令的記憶體空間解析\r1[root@nfs ~]# free -h 2 total used free shared buff/cache available 3Mem: 188G 6.4G 1.0G 138M 181G 181G 4Swap: 4.0G 0B 4.0G 5[root@nfs ~]# total 記憶體總數 used 已經使用的記憶體 free 空閒的記憶體數 shared 多個進程共享的記憶體總額 buffer 作為 buffer cache 的緩存 cache 作為 page cache 的緩存 available = free + buff/cache 上表中 something 代表的正是 free 命令中 buffers/cached 使用的記憶體。由於這個記憶體實際上是從作業系統的角度使用的，所以如果用戶想要使用它，那麼它可以被用戶的應用快速地回收和使用。\nbuffer \u0026amp; cache\rbuffer cache 緩衝\rBuffer cache 也叫塊緩衝，是對物理磁盤上的一個磁盤塊進行的緩衝，其大小爲通常爲 1k。它是爲了緩衝寫操作然後一次性將很多改動寫入硬盤，避免頻繁寫硬盤，提高寫入效率。\npage cache 快取\rpage cache 頁緩衝/文件緩衝，由若干個磁盤塊組成(也即由若干個bufferCache組成，物理上不一定連續)，通常為 4K、在 64 位系統上爲 8k。它是爲了給讀操作提供緩衝，避免頻繁讀硬盤，提高讀取效率。\n簡單說來，buffer cache 用來緩存磁盤數據、優化磁盤的 I/O；page cache 用來緩存文件數據、優化文件系統的 I/O。在有文件系統的情況下，對文件操作，那麼數據會緩存到 page cache，如果直接採用 dd 等工具對磁盤進行讀寫，那麼數據會緩存到 buffer cache。\n如何清除 cache\r一般來說使用者是不需要去管這些 cache 何時會被清除的，所以 Linux 也沒有專門的指令來做這件事，不過依然有提供一個 proc file system 介面 /proc/sys/vm/drop_caches，可以強制 kernel 清理快取。drop_caches 的值可以是 0-3 之間的數字，代表不同涵義：\n0：不釋放(系統默認值)；不釋放内存，由操作系統自動管理\n1：釋放 pagecache.\n1# sync \u0026amp;\u0026amp; echo 1 \u0026gt; /proc/sys/vm/drop_caches 2：釋放 dentries(目錄緩存) 和 inodes (文件元數據)\n1# sync \u0026amp;\u0026amp; echo 2 \u0026gt; /proc/sys/vm/drop_caches 3：釋放所有緩存 pagecache, dentries and inodes.\n1# sync \u0026amp;\u0026amp; echo 3 \u0026gt; /proc/sys/vm/drop_caches 基本上執行這些指令沒有什麼好處，所以除非你很明確知道你想幹嘛（例如為了避免 cache 影響實驗結果），否則建議不要隨便手動清除快取記憶體。另外，也建議在做這個動作前先執行 sync 讓檔案寫入操作先完成，否則可能會造成意想不到的結果。\nReference\rhttps://www.linuxatemyram.com/\rhttps://medium.com/hungys-blog/clear-linux-memory-cache-manually-90bec95ea003\r","date":"2022-03-03T19:21:00+00:00","updated":"2022-03-03T19:21:00+00:00"},{"objectID":"1646053200","permalink":"/post/smangus/","title":"司馬庫斯兩日遊🤎","content":"去年疫情悶了半年，十一月初就早早訂了 228 的司馬庫斯的櫻花祭旅遊。因為不想舟車勞頓的開三四個小時的山路，就決定邀請大姊男友家一起直接包八人小團出遊！每次出遊前一個禮拜就會忍不住每天都看好幾次的氣象預報，但結果是，不是前一天看的預報都不準 🤣\n然超級幸運的是，原先連續兩個禮拜每天都在下雨的北部，竟然就這麼剛剛好的在連假前一天正式放晴 ☀️ ☀️ 行前每天都在祈禱大晴天，甚至還在回嘉的前一天按照網路上的不科學習俗畫了烏龜燒 XD 但儘管完全沒根據，還是很神奇的兩天都是一點雲都沒有的超級好天氣!!(故意沒畫雲) amazing~~ 🥳 😎 第一晚夜宿新竹市區\r因為想多留一點時間在司馬庫斯，所以第一個晚上決定先上新竹住一晚。準時的約了旅行社安排的晚上七點，但連假國道塞好塞滿，所以最後十點才到飯店。城隍廟夜市幾乎都關了，所幸還能遇到開得很晚的碳烤，便買了幾道菜配酒，不亦樂乎！ 然後第一天晚上的旅館爛到不知道如何評論 🙃 下次非車程很遠的旅行還是自助遊最好了，如今已經不像大學時的窮遊可以享受省錢的快樂啦 🤪\n合興車站\r隔天一早八點半便從飯店出發，早上飯店附餐是很神奇的便當 =_=? 吃完便當配菜就跟姊們去附近的全家解決。路上買了司機推薦的新竹溫媽媽客家菜包跟草仔粿，米香很香，內餡也好吃！ 對比之下，我的構圖還是有待加強 TVT 辛苦我姊了 到第一個景點時，雲就都散了，天氣超好! 軍艦岩\r前往軍艦岩前經過了一個觀景台，在馬里光部落附近。 在軍艦岩有兩座吊橋，春天大部分的樹葉都變綠了，但還能看到些許紅黃褐色的葉子穿插，呈現山巒的層次美。河水量充沛，且非常乾淨，好美哇~~ 司馬庫斯\r一個小時後，大概一點就到司馬庫斯了！先吃個從合興車站帶上來的山豬肉便當(意外好吃)。在三點入住前到處在部落逛逛，趁著前一批遊客離開、下一批遊客到來前，在大道跟部落入口拍照，提早來真的是太對了 🥺 😇 大概十分鐘到上面的觀景台，路途上有一隻可愛的黃牛 🐮，觀景台可以看到整個司馬庫斯部落。 然後三點準時跟司機 check-in 小木屋。這次因為沒有馬上就訂好，所以只能住靠近大門的伯特利屋 雅房，但慶幸的是住在二樓，而且衛浴就房間在旁邊！大家在房間內各自休息，嗑了一些帶來的零食跟早上沒吃完的菜包充飢，還睡了一小覺補眠，夕陽灑進來，剛剛好的溫度，太舒服啦 😆 從二樓拍出去的風景，紀錄司馬庫斯太陽下山的光影變 …","date":"2022-02-28T21:00:00+08:00","updated":"2022-02-28T21:00:00+08:00"},{"objectID":"1645634220","permalink":"/post/os-kvm-install-rocky-linux/kvm-install-rocky-linux/","title":"[KVM] Install Rocky Linux","content":"安裝 KVM 套件\r請參考之前的\r文章\r安裝 Rocky Linux VM\r於官網下載 iso 映像檔\r1wget --no-check-certificate https://download.rockylinux.org/pub/rocky/8/isos/x86_64/Rocky-8.5-x86_64-minimal.iso 開始安裝\r1virt-install --name rockym \\ 2--disk path=/var/lib/libvirt/images/rockym,size=60,format=qcow2 \\ 3--vcpus 4 --memory 16384 \\ 4--network bridge=br0 \\ 5--graphics none --os-type linux --os-variant=rhl8.0 \\ 6--location /var/lib/libvirt/images/Rocky-8.5-x86_64-minimal.iso \\ 7--extra-args \u0026amp;#39;console=ttyS0\u0026amp;#39; 3) Installation source 選項配置\r1Installation 2 31) [x] Language settings 2) [x] Time settings 4 (English (United States)) (Asia/Taipei timezone) 53) [x] Installation source 4) [x] Software selection 6 (Local media) (Server) 75) [!] Installation Destination 6) [x] Kdump 8 (Automatic partitioning (Kdump is enabled) 9 selected) 107) [ ] Network configuration 8) [!] Root password 11 (Not connected) (Root account is disabled.) 129) [!] User creation 13 (No user will be created) 14 15Please make a selection from the above …","date":"2022-02-23T16:37:00+00:00","updated":"2022-02-23T16:37:00+00:00"},{"objectID":"1645568520","permalink":"/post/os-kvm-shrink-image/kvm-shrink-image/","title":"[KVM] 為 image 瘦身","content":"問題說明\r在建立 vm 後，發現其使用的 qcow2 image 檔案大小超級大。使用 qemu-img info 查看 vm 真正的使用容量僅 2G 但實際上卻佔用了啟動時劃分的 disk size 如範例的 60G。\n1[root@nexdata images]# qemu-img info rockym 2image: rockym 3file format: qcow2 4virtual size: 60G (64424509440 bytes) 5disk size: 2.1G 6cluster_size: 65536 7Format specific information: 8 compat: 1.1 9 lazy refcounts: true 10 refcount bits: 16 11 corrupt: false 解決方式\r透過 convert 轉換 qcow2 檔案縮小：\n1qemu-img convert -p -f qcow2 ./vm-disk-original.qcow2 -O qcow2 ./vm-disk-shrinked.qcow2 Warning\n以防萬一，請先將 VM 關機後再操作。\nReference\rhttps://serverfault.com/questions/881595/kvm-guest-qcow2-larger-than-disk-size\r","date":"2022-02-22T22:22:00+00:00","updated":"2022-02-22T22:22:00+00:00"},{"objectID":"1645560060","permalink":"/post/programming-lamp-apache-mariadb-php-on-linux/deploy-lamp-on-linux/","title":"佈署 LAMP (Apache + MariaDB + PHP Web Server on Linux)","content":"系統環境\rCentOS 7.9 PHP 7.2.34 Apache/2.4.6 1[root@ewrula ~]# httpd -v 2Server version: Apache/2.4.6 (CentOS) 3Server built: Jan 25 2022 14:08:43 MariaDB 5.5.68 1[root@ewrula ~]# mysql -V 2mysql Ver 15.1 Distrib 5.5.68-MariaDB, for Linux (x86_64) using readline 5.1 安裝\rApache\r1yum install -y httpd 2systemctl start httpd 3systemctl enable httpd 安裝完畢並啟動後 Apache 已經可以存取了\nMariaDB\r1yum install mariadb-server mariadb 2systemctl start mariadb 3systemctl enable mariadb 設定 MariaDB 的 root 密碼：\n1/usr/bin/mysql_secure_installation 會跳出提示，預設是空密碼，故請按照指示 enter for none\n1Enter current password for root (enter for none): 2OK, successfully used password, moving on... 3 4Setting the root password ensures that nobody can log into the MariaDB 5root user without the proper authorization. 6 7New password: password 8Re-enter new password: password 9Password updated successfully! 10Reloading privilege tables.. 11 ... Success! 測試連線 MariaDB：\n1mysql -u root -p php 7.2\r目前 php 最高的穩定版本是 7.2，若直接採用 CentOS 中的 yum 安裝 yum -y install php，版本是5.4，因此需要手動更新 rpm。\n1rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm 2rpm -Uvh https://mirror.webtatic.com/yum/el7/webtatic-release.rpm 7.2 版本名為 72w，除了 php 庫外還需安裝其他常用套件：\n1yum -y install php72w php72w-bcmath php72w-cli php72w-common php72w-dba php72w-devel php72w-embedded php72w-enchant php72w-fpm php72w-gd php72w-imap php72w-interbase php72w-intl php72w-ldap php72w-mbstring php72w-mcrypt php72w-mysqlnd php72w-odbc php72w-opcache php72w-pdo php72w-pdo_dblib php72w-pear php72w-pecl-apcu php72w-pecl-apcu-devel php72w-pecl-imagick php72w-pecl-imagick-devel php72w-pecl-xdebug php72w-pgsql php72w-phpdbg php72w-process php72w-pspell php72w-recode php72w-snmp php72w-soap php72w-tidy php72w-xml php72w-xmlrpc 查看已安裝套件:\n1[root@ewrula ~]# php -m 2[PHP Modules] 3apcu 4bcmath 5bz2 6calendar 7Core 8ctype 9curl 10date 11dba 12dom 13enchant 14exif 15fileinfo 16filter 17ftp 18gd 19gettext 20gmp 21hash 22iconv 23imagick 24imap 25interbase 26intl 27json 28ldap 29libxml 30mbstring 31mysqli 32mysqlnd 33odbc 34openssl 35pcntl 36pcre 37PDO 38pdo_dblib 39PDO_Firebird 40pdo_mysql 41PDO_ODBC 42pdo_pgsql 43pdo_sqlite 44pgsql 45Phar 46posix 47pspell 48readline 49recode 50Reflection 51session 52shmop 53SimpleXML 54snmp 55soap 56sockets 57SPL 58sqlite3 59standard 60sysvmsg 61sysvsem 62sysvshm 63tidy 64tokenizer 65wddx 66xdebug 67xml 68xmlreader 69xmlrpc 70xmlwriter 71xsl 72Zend OPcache 73zip 74zlib 75 76[Zend Modules] 77Xdebug 78Zend OPcache 79 80[root@ewrula ~]# 配置\r設定防火牆\r1firewall-cmd --zone=public --permanent --add-port=80/tcp 2firewall-cmd --zone=public --permanent --add-port=443/tcp 3service firewalld restart 設定 apache conf 檔\r1vim /etc/httpd/conf/httpd.conf 主要要設定網頁路徑所在，詳細的設定項說明可以參考鳥哥的\r文章\r權限設定\r1chown -R apache.apache /var/www/blog 2chmod -R 755 /var/www/blog 3sudo chcon -t httpd_sys_rw_content_t /var/www/blog -R 重啟 server\r1systemctl restart httpd 解決 QueryException (2002) SQLSTATE[HY000] [2002] Permission denied 的錯誤\r這是 SELinux 對 httpd 運行服務的安全控管機制，請先查詢 httpd 運行時的設定\n1getsebool -a | grep httpd 如果 httpd_can_network_connect_db 的設定是 Off (關閉)，請透過下列指令予以開啟\n1setsebool -P httpd_can_network_connect_db 1 執行過程：\n1[root@ewrula ewr]# getsebool -a | grep httpd 2httpd_anon_write --\u0026gt; off 3httpd_builtin_scripting --\u0026gt; on 4httpd_can_check_spam --\u0026gt; off 5httpd_can_connect_ftp --\u0026gt; off 6httpd_can_connect_ldap --\u0026gt; off 7httpd_can_connect_mythtv --\u0026gt; off 8httpd_can_connect_zabbix --\u0026gt; off 9httpd_can_network_connect --\u0026gt; off 10httpd_can_network_connect_cobbler --\u0026gt; off 11httpd_can_network_connect_db --\u0026gt; off 12httpd_can_network_memcache --\u0026gt; off 13httpd_can_network_relay --\u0026gt; off 14httpd_can_sendmail --\u0026gt; off 15httpd_dbus_avahi --\u0026gt; off 16httpd_dbus_sssd --\u0026gt; off 17httpd_dontaudit_search_dirs --\u0026gt; off 18httpd_enable_cgi --\u0026gt; on 19httpd_enable_ftp_server --\u0026gt; off 20httpd_enable_homedirs --\u0026gt; off 21httpd_execmem --\u0026gt; off 22httpd_graceful_shutdown --\u0026gt; on 23httpd_manage_ipa --\u0026gt; off 24httpd_mod_auth_ntlm_winbind --\u0026gt; off 25httpd_mod_auth_pam --\u0026gt; off 26httpd_read_user_content --\u0026gt; off 27httpd_run_ipa --\u0026gt; off 28httpd_run_preupgrade --\u0026gt; off 29httpd_run_stickshift --\u0026gt; off 30httpd_serve_cobbler_files --\u0026gt; off 31httpd_setrlimit --\u0026gt; off 32httpd_ssi_exec --\u0026gt; off 33httpd_sys_script_anon_write --\u0026gt; off 34httpd_tmp_exec --\u0026gt; off 35httpd_tty_comm --\u0026gt; off 36httpd_unified --\u0026gt; off 37httpd_use_cifs --\u0026gt; off 38httpd_use_fusefs --\u0026gt; off 39httpd_use_gpg --\u0026gt; off 40httpd_use_nfs --\u0026gt; off 41httpd_use_openstack --\u0026gt; off 42httpd_use_sasl --\u0026gt; off 43httpd_verify_dns --\u0026gt; off 44[root@ewrula ewr]# setsebool -P httpd_can_network_connect_db 1 45[root@ewrula ewr]# 46[root@ewrula ewr]# 47[root@ewrula ewr]# 48[root@ewrula ewr]# getsebool -a | grep httpd 49httpd_anon_write --\u0026gt; off 50httpd_builtin_scripting --\u0026gt; on 51httpd_can_check_spam --\u0026gt; off 52httpd_can_connect_ftp --\u0026gt; off 53httpd_can_connect_ldap --\u0026gt; off 54httpd_can_connect_mythtv --\u0026gt; off 55httpd_can_connect_zabbix --\u0026gt; off 56httpd_can_network_connect --\u0026gt; off 57httpd_can_network_connect_cobbler --\u0026gt; off 58httpd_can_network_connect_db --\u0026gt; on 59httpd_can_network_memcache --\u0026gt; off 60httpd_can_network_relay --\u0026gt; off 61httpd_can_sendmail --\u0026gt; off 62httpd_dbus_avahi --\u0026gt; off 63httpd_dbus_sssd --\u0026gt; off 64httpd_dontaudit_search_dirs --\u0026gt; off 65httpd_enable_cgi --\u0026gt; on 66httpd_enable_ftp_server --\u0026gt; off 67httpd_enable_homedirs --\u0026gt; off 68httpd_execmem --\u0026gt; off 69httpd_graceful_shutdown --\u0026gt; on 70httpd_manage_ipa --\u0026gt; off 71httpd_mod_auth_ntlm_winbind --\u0026gt; off 72httpd_mod_auth_pam --\u0026gt; off 73httpd_read_user_content --\u0026gt; off 74httpd_run_ipa --\u0026gt; off 75httpd_run_preupgrade --\u0026gt; off 76httpd_run_stickshift --\u0026gt; off 77httpd_serve_cobbler_files --\u0026gt; off 78httpd_setrlimit --\u0026gt; off 79httpd_ssi_exec --\u0026gt; off 80httpd_sys_script_anon_write --\u0026gt; off 81httpd_tmp_exec --\u0026gt; off 82httpd_tty_comm --\u0026gt; off 83httpd_unified --\u0026gt; off 84httpd_use_cifs --\u0026gt; off 85httpd_use_fusefs --\u0026gt; off 86httpd_use_gpg --\u0026gt; off 87httpd_use_nfs --\u0026gt; off 88httpd_use_openstack --\u0026gt; off 89httpd_use_sasl --\u0026gt; off 90httpd_verify_dns --\u0026gt; off 91[root@ewrula ewr]# systemctl restart httpd Reference\rhttps://ithelp.ithome.com.tw/questions/10186688\r","date":"2022-02-22T20:01:00+00:00","updated":"2022-02-22T20:01:00+00:00"},{"objectID":"1645470840","permalink":"/post/os-virtualbox-vm-efail/virtualbox-efail-error/","title":"VirtualBox 安裝 VM 錯誤 (E_FAIL 0x80004005) 之解決","content":"在 Win10 使用 VirtualBox 安裝 VM 時遇到下述錯誤，紀錄一下解決方式。\nError:\r1Failed to open a session for the virtual machine xxx. 2Call to WHvSetupPartition failed: ERROR_SUCCESS (Last=0xc000000d/87) (VERR_NEM_VM_CREATE_FAILED). 3Result Code: E_FAIL (0x80004005) 4Component: ConsoleWrap 5Interface: IConsole {872da645-4a9b-1727-bee2-5585105b9eed} Reason\r許多虛擬化應用程式依賴處理器上的硬體虛擬化擴充，包括 Intel VT-x 和 AMD-V，一次僅能允許一個軟體使用。若要使用其他虛擬化軟體如 VirtualBOX 或 VMware，必須停用 Hyper-V 虛擬機器監控程式、裝置防護和認證防護。\nSolution:\r1Windows Features 2---------------------- 3Disabled -\u0026gt; Hyper-V 4Enabled -\u0026gt; Virtual Machine Platform 5Enabled -\u0026gt; Windows Hypervisor Platform 6Disabled -\u0026gt; Windows Sandbox 7 8Elevated Powershell/Cmd 9------------------------------- 10bcdedit /set hypervisorlaunchtype off 11 12BIOS 13----- 14Enabled -\u0026gt; Virtualization Technology (VTx) 15Enabled -\u0026gt; Virtualization Technology for Directed I/O (VTd) 16Disabled -\u0026gt; HP Hypervisor Other Windows Features (Possibly irrelevant, can view img below)\nReference\rhttps://forums.virtualbox.org/viewtopic.php?f=6\u0026t=93443\rhttps://docs.microsoft.com/zh-tw/troubleshoot/windows-client/application-management/virtualization-apps-not-work-with-hyper-v\r","date":"2022-02-21T19:14:00+00:00","updated":"2022-02-21T19:14:00+00:00"},{"objectID":"1644560400","permalink":"/post/kubernets-ingress-invalid-ingressclass/","title":"Ingress does not contain a valid IngressClass","content":"問題描述\rnginx ingress 從原本 deprecated 的 stable/nginx-ingress helm chart 改為 ingress-nginx/ingress-nginx chart 後，發現 ingress resource 的 nginx 網頁 404 not found，查看 ingress nginx controller log 發現有 ingress does not contain a valid IngressClass 的錯誤。\n完整 log 如下：\n1[root@testm terraform]# kubectl logs -f -n ingress-nginx ingress-nginx-controller-5c5bf8c854-7pcf7 2------------------------------------------------------------------------------- 3NGINX Ingress controller 4 Release: v1.1.1 5 Build: a17181e43ec85534a6fea968d95d019c5a4bc8cf 6 Repository: https://github.com/kubernetes/ingress-nginx 7 nginx version: nginx/1.19.9 8 9------------------------------------------------------------------------------- 10 11W0209 05:43:07.359176 7 client_config.go:615] Neither --kubeconfig nor --master was specified. Using thesterConfig. This might not work. 12I0209 05:43:07.359883 7 main.go:223] \u0026#34;Creating API client\u0026#34; host=\u0026#34;https://10.96.0.1:443\u0026#34; 13I0209 05:43:07.379993 7 main.go:267] \u0026#34;Running in Kubernetes cluster\u0026#34; major=\u0026#34;1\u0026#34; minor=\u0026#34;20\u0026#34; git=\u0026#34;v1.20.15\u0026#34; \u0026#34;clean\u0026#34; commit=\u0026#34;8f1e5bf0b9729a899b8df86249b56e2c74aebc55\u0026#34; platform=\u0026#34;linux/amd64\u0026#34; 14I0209 05:43:07.644539 7 main.go:104] \u0026#34;SSL fake certificate created\u0026#34; file=\u0026#34;/etc/ingress-controller/ssl/defake-certificate.pem\u0026#34; 15I0209 05:43:07.675915 7 ssl.go:531] \u0026#34;loading tls certificate\u0026#34; path=\u0026#34;/usr/local/certificates/cert\u0026#34; key=\u0026#34;/ual/certificates/key\u0026#34; 16I0209 05:43:07.727278 7 nginx.go:255] \u0026#34;Starting NGINX Ingress controller\u0026#34; 17I0209 05:43:07.828276 7 event.go:282] Event(v1.ObjectReference{Kind:\u0026#34;ConfigMap\u0026#34;, Namespace:\u0026#34;ingress-nginxe:\u0026#34;ingress-nginx-controller\u0026#34;, UID:\u0026#34;c6943575-18fe-471a-aa44-5f251af7a277\u0026#34;, APIVersion:\u0026#34;v1\u0026#34;, ResourceVersion:\u0026#34;104FieldPath:\u0026#34;\u0026#34;}): type: \u0026#39;Normal\u0026#39; reason: \u0026#39;CREATE\u0026#39; ConfigMap ingress-nginx/ingress-nginx-controller 18I0209 05:43:08.929877 7 nginx.go:297] \u0026#34;Starting NGINX process\u0026#34; 19I0209 05:43:08.930002 7 leaderelection.go:248] attempting to acquire leader lease ingress-nginx/ingress-cler-leader... 20I0209 05:43:08.930504 7 nginx.go:317] \u0026#34;Starting validation webhook\u0026#34; address=\u0026#34;:8443\u0026#34; certPath=\u0026#34;/usr/local/icates/cert\u0026#34; keyPath=\u0026#34;/usr/local/certificates/key\u0026#34; 21I0209 05:43:08.930926 7 controller.go:155] \u0026#34;Configuration changes detected, backend reload required\u0026#34; 22I0209 05:43:08.945063 7 leaderelection.go:258] successfully acquired lease ingress-nginx/ingress-controllder 23I0209 05:43:08.945130 7 status.go:84] \u0026#34;New leader elected\u0026#34; identity=\u0026#34;ingress-nginx-controller-5c5bf8c854- 24I0209 05:43:09.015595 7 controller.go:172] \u0026#34;Backend successfully reloaded\u0026#34; 25I0209 05:43:09.015708 7 controller.go:183] \u0026#34;Initial sync, sleeping for 1 second\u0026#34; 26I0209 05:43:09.015766 7 event.go:282] Event(v1.ObjectReference{Kind:\u0026#34;Pod\u0026#34;, Namespace:\u0026#34;ingress-nginx\u0026#34;, Namress-nginx-controller-5c5bf8c854-7pcf7\u0026#34;, UID:\u0026#34;f84b09e4-7d7d-40bf-ae66-f2eb72ab7a59\u0026#34;, APIVersion:\u0026#34;v1\u0026#34;, ResourceV:\u0026#34;104846\u0026#34;, FieldPath:\u0026#34;\u0026#34;}): type: \u0026#39;Normal\u0026#39; reason: \u0026#39;RELOAD\u0026#39; NGINX reload triggered due to a change in configurat 27W0209 05:43:26.721706 7 controller.go:988] Error obtaining Endpoints for Service \u0026#34;monitoring/prometheus-or-prometheus\u0026#34;: no object matching key \u0026#34;monitoring/prometheus-operator-prometheus\u0026#34; in local store 28W0209 05:43:26.721831 7 controller.go:1083] Service \u0026#34;monitoring/prometheus-operator-grafana\u0026#34; does not havactive Endpoint. 29W0209 05:43:26.722981 7 controller.go:988] Error obtaining Endpoints for Service \u0026#34;monitoring/prometheus-or-alertmanager\u0026#34;: no object matching key \u0026#34;monitoring/prometheus-operator-alertmanager\u0026#34; in local store 30I0209 05:43:26.781310 7 admission.go:149] processed ingress via admission controller {testedIngressLengthtedIngressTime:0.06s renderingIngressLength:1 renderingIngressTime:0.001s admissionTime:18.0kBs testedConfiguraze:0.061} 31I0209 05:43:26.781370 7 main.go:101] \u0026#34;successfully validated configuration, accepting\u0026#34; ingress=\u0026#34;monitorinetheus-ingress\u0026#34; 32I0209 05:43:26.784116 7 admission.go:149] processed ingress via admission controller {testedIngressLengthtedIngressTime:0.063s renderingIngressLength:1 renderingIngressTime:0s admissionTime:18.0kBs testedConfiguratio0.063} 33I0209 05:43:26.784155 7 main.go:101] \u0026#34;successfully validated configuration, accepting\u0026#34; ingress=\u0026#34;monitorinana-ingress\u0026#34; 34I0209 05:43:26.785125 7 admission.go:149] processed ingress via admission controller {testedIngressLengthtedIngressTime:0.063s renderingIngressLength:1 renderingIngressTime:0.002s admissionTime:18.1kBs testedConfigurize:0.065} 35I0209 05:43:26.785171 7 main.go:101] \u0026#34;successfully validated configuration, accepting\u0026#34; ingress=\u0026#34;monitorintmanager-ingress\u0026#34; 36I0209 05:43:26.785238 7 admission.go:149] processed ingress via admission controller {testedIngressLengthtedIngressTime:0.064s renderingIngressLength:1 renderingIngressTime:0s admissionTime:18.0kBs testedConfiguratio0.064} 37I0209 05:43:26.785259 7 main.go:101] \u0026#34;successfully validated configuration, accepting\u0026#34; ingress=\u0026#34;istio-sysali-ingress\u0026#34; 38I0209 05:43:26.931088 7 store.go:420] \u0026#34;Ignoring ingress because of error while validating ingress class\u0026#34; s=\u0026#34;monitoring/grafana-ingress\u0026#34; error=\u0026#34;ingress does not contain a valid IngressClass\u0026#34; 39I0209 05:43:26.931132 7 store.go:420] \u0026#34;Ignoring ingress because of error while validating ingress class\u0026#34; s=\u0026#34;monitoring/prometheus-ingress\u0026#34; error=\u0026#34;ingress does not contain a valid IngressClass\u0026#34; 40I0209 05:43:26.931149 7 store.go:420] \u0026#34;Ignoring ingress because of error while validating ingress class\u0026#34; s=\u0026#34;monitoring/alertmanager-ingress\u0026#34; error=\u0026#34;ingress does not contain a valid IngressClass\u0026#34; 41I0209 05:43:26.945691 7 store.go:420] \u0026#34;Ignoring ingress because of error while validating ingress class\u0026#34; s=\u0026#34;istio-system/kiali-ingress\u0026#34; error=\u0026#34;ingress does not contain a valid IngressClass\u0026#34; 原 Ingress 配置\r以 grafana-ing.tf 為範例 (使用 terraform 自動部署工具安裝)\n1resource \u0026#34;kubernetes_ingress\u0026#34; \u0026#34;grafana_ingress\u0026#34; { 2 count = var.package.prometheus == true ? 1 : 0 3 4 metadata { 5 name = \u0026#34;grafana-ingress\u0026#34; 6 namespace = \u0026#34;monitoring\u0026#34; 7 } 8 9 spec { 10 rule { 11 host = \u0026#34;${var.prometheus-options.host_grafana}.${var.domain}\u0026#34; 12 http { 13 path { 14 backend { 15 service_name = \u0026#34;prometheus-operator-grafana\u0026#34; 16 service_port = 80 17 } 18 path = \u0026#34;/\u0026#34; 19 } 20 } 21 } 22 } 23 24 depends_on = [ 25 helm_release.ingress-nginx, 26 ] 27} 發生原因\r原因是在 ingress v1.0.0 版之後，ingress 需要加上 ingress class，請參考 github 的 #7341\rpull request，如果沒有，controller 會丟 Ignoring ingress because of error while validating ingress class\u0026quot; ingress=\u0026quot;k8sNamespace/ingressResourceName\u0026quot; error=\u0026quot;ingress does not contain a valid IngressClass\u0026quot; 的錯誤。\nInfo\nAn Ingress Class is basically a category which specify who needs to serve and manage the Ingress, this is necessary since in a cluster you can have more than one Ingress controller, each one with its rules and configurations.\n解決方法\r在 ingress resource 中的 metadata 欄位加上 annotations: kubernetes.io/ingress.class: \u0026quot;nginx\u0026quot; 即可。\n1resource \u0026#34;kubernetes_ingress\u0026#34; \u0026#34;grafana_ingress\u0026#34; { 2 count = var.package.prometheus == true ? 1 : 0 3 4 metadata { 5 name = \u0026#34;grafana-ingress\u0026#34; 6 namespace = \u0026#34;monitoring\u0026#34; 7 annotations = { 8 \u0026#34;kubernetes.io/ingress.class\u0026#34; = \u0026#34;nginx\u0026#34; 9 } 10 } 11 12 spec { 13 rule { 14 host = \u0026#34;${var.prometheus-options.host_grafana}.${var.domain}\u0026#34; 15 http { 16 path { 17 backend { 18 service_name = \u0026#34;prometheus-operator-grafana\u0026#34; 19 service_port = 80 20 } 21 path = \u0026#34;/\u0026#34; 22 } 23 } 24 } 25 } 26 27 depends_on = [ 28 helm_release.ingress-nginx, 29 ] 30} 注意\r上述方法僅適用於 kubernetes 1.22 版以前。k8s 1.22 版以後請使用 ingressClassName 於 spec 區塊下。請參考\r官網\r說明。\nReference\rhttps://forum.linuxfoundation.org/discussion/859965/exercise-7-nginx-update-requires-change-to-yaml\rhttps://registry.terraform.io/providers/hashicorp/kubernetes/latest/docs/resources/ingress\rhttps://stackoverflow.com/questions/65979766/ingress-with-nginx-controller-not-working-address-missing\r","date":"2022-02-11T06:20:00+00:00","updated":"2022-02-11T06:20:00+00:00"},{"objectID":"1644434460","permalink":"/post/blog-hexo-add-search-function-next8/hexo-add-search-bar/","title":"[Hexo] Add Search Function under NexT8 Theme","content":"文章愈來愈多啦，只用目錄跟標籤找有如大海撈針，所以來幫部落格添加本站搜索的功能！在這邊記錄一下。\nnpm 安裝套件\rHexo 沒有本地搜尋的功能，但 NexT 主題已經內建搜尋功能的選項，不過需額外安裝套件。建議直接安裝 NexT 釋出的 hexo-generator-searchdb 套件會比較穩定。\n1npm install hexo-generator-searchdb 修改 config 檔\r可以直接在 themes/_config.yml 找到 search 的相關設置，將 enable: false 改成 true。\n1# Local Search 2# Dependencies: https://github.com/theme-next/hexo-generator-searchdb 3local_search: 4 enable: false 5 # If auto, trigger search by changing input. 6 # If manual, trigger search by pressing enter key or search button. 7 trigger: auto 8 # Show top n results per article, show all results by setting to -1 9 top_n_per_article: 1 10 # Unescape html strings to the readable one. 11 unescape: false 12 # Preload the search data when the page loads. 13 preload: false 重新啟動 hexo server\r就簡簡單單的大功告成了，讚嘆 NexT!\nReference\rhttps://home.gamer.com.tw/artwork.php?sn=5273365\r","date":"2022-02-09T19:21:00+00:00","updated":"2022-02-09T19:21:00+00:00"},{"objectID":"1644353160","permalink":"/post/os-kvm-guest-cannot-access-internet/kvm-guest-cannot-access-internet/","title":"[KVM] guest can't access Internet","content":"狀況\r在 CentOS 建立了幾台 KVM 虛擬機，但是在系統重啟後發現這幾台機器都無法連網了。只能與 host 互 ping，無法連網、無法 ping 同網段的其他主機、KVM guest 與 guest 間也不認得。\n其中一個 VM 的網路設定： （CentOS 7.9）\nHost IP: 10.1.5.130 Guest IP: 10.1.5.141 Host Network: Bridge (br0) Guest KVM Network Ineterface: eth0 Ping results:\n1root@host:~$ ping 10.1.5.141 2PING 10.0.10.13 (10.0.10.13) 56(84) bytes of data. 364 bytes from 10.0.10.13: icmp_seq=1 ttl=64 time=0.207 ms 4 5root@host:~$ ping 1.1.1.1 6PING 1.1.1.1 (1.1.1.1) 56(84) bytes of data. 764 bytes from 1.1.1.1: icmp_seq=1 ttl=58 time=119 ms 8 9root@guest:~$ ping 10.1.5.130 10PING 10.0.10.2 (10.0.10.2) 56(84) bytes of data. 1164 bytes from 10.0.10.2: icmp_seq=1 ttl=64 time=0.257 ms 12 13root@guest:~$ ping 1.1.1.1 14PING 1.1.1.1 (1.1.1.1) 56(84) bytes of data. 15 16--- 1.1.1.1 ping statistics --- 179 packets transmitted, 0 received, 100% packet loss, time 7999ms 查看雙方的網路設定皆無異常，重啟 server 或 kvm 的網卡皆無效。\n1[root@host ~]# ifconfig 2br0: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500 3 inet 10.1.5.130 netmask 255.255.255.0 broadcast 10.1.5.255 4 inet6 fe80::3c9b:4cc9:bd11:8919 prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt; 5 ether 0c:c4:7a:b7:39:66 txqueuelen 1000 (Ethernet) 6 RX packets 5179086 bytes 284472155 (271.2 MiB) 7 RX errors 0 dropped 79272 overruns 0 frame 0 8 TX packets 81432 bytes 13931170 (13.2 MiB) 9 TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 10 11br-3138c45de2ae: flags=4099\u0026lt;UP,BROADCAST,MULTICAST\u0026gt; mtu 1500 12 inet 172.18.0.1 netmask 255.255.0.0 broadcast 172.18.255.255 13 ether 02:42:c1:cf:65:cc txqueuelen 0 (Ethernet) 14 RX packets 0 bytes 0 (0.0 B) 15 RX errors 0 dropped 0 overruns 0 frame 0 16 TX packets 0 bytes 0 (0.0 B) 17 TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 18 19docker0: flags=4099\u0026lt;UP,BROADCAST,MULTICAST\u0026gt; mtu 1500 20 inet 172.17.0.1 netmask 255.255.0.0 broadcast 172.17.255.255 21 ether 02:42:fc:40:fa:e4 txqueuelen 0 (Ethernet) 22 RX packets 0 bytes 0 (0.0 B) 23 RX errors 0 dropped 0 overruns 0 frame 0 24 TX packets 0 bytes 0 (0.0 B) 25 TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 26 27ens1f0: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500 28 ether 0c:c4:7a:b7:39:66 txqueuelen 1000 (Ethernet) 29 RX packets 4745249 bytes 350711453 (334.4 MiB) 30 RX errors 0 dropped 39639 overruns 0 frame 0 31 TX packets 117171 bytes 12372319 (11.7 MiB) 32 TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 33 34lo: flags=73\u0026lt;UP,LOOPBACK,RUNNING\u0026gt; mtu 65536 35 inet 127.0.0.1 netmask 255.0.0.0 36 inet6 ::1 prefixlen 128 scopeid 0x10\u0026lt;host\u0026gt; 37 loop txqueuelen 1000 (Local Loopback) 38 RX packets 567 bytes 64847 (63.3 KiB) 39 RX errors 0 dropped 0 overruns 0 frame 0 40 TX packets 567 bytes 64847 (63.3 KiB) 41 TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 42 43vnet1: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500 44 inet6 fe80::fc54:ff:fe12:db6c prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt; 45 ether fe:54:00:12:db:6c txqueuelen 1000 (Ethernet) 46 RX packets 2674 bytes 254695 (248.7 KiB) 47 RX errors 0 dropped 0 overruns 0 frame 0 48 TX packets 94305 bytes 5469690 (5.2 MiB) 49 TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 50 51[root@host ~]# nmcli con show 52NAME UUID TYPE DEVICE 53bridge-br0 a2d389db-dfb8-4fb4-b5e1-c98fa4f08f71 bridge br0 54br-3138c45de2ae 3e78cb65-cc77-4a9f-b485-a672a2631b86 bridge br-3138c45de2ae 55bridge-slave-ens1f0 de082284-5730-4648-8353-804805894e46 ethernet ens1f0 56vnet1 bb2d992a-72f9-4b29-b4b6-0553f7a46772 tun vnet1 57 58[root@host ~]# cat /etc/sysconfig/network-scripts/ifcfg-bridge-br0 59STP=yes 60BRIDGING_OPTS=priority=32768 61TYPE=Bridge 62PROXY_METHOD=none 63BROWSER_ONLY=no 64BOOTPROTO=none 65DEFROUTE=yes 66IPV4_FAILURE_FATAL=no 67IPV6INIT=yes 68IPV6_AUTOCONF=yes 69IPV6_DEFROUTE=yes 70IPV6_FAILURE_FATAL=no 71IPV6_ADDR_GEN_MODE=stable-privacy 72NAME=bridge-br0 73UUID=a2d389db-dfb8-4fb4-b5e1-c98fa4f08f71 74DEVICE=br0 75ONBOOT=yes 76IPADDR=10.1.5.130 77PREFIX=24 78GATEWAY=10.1.5.254 79DNS1=10.1.1.3 80 81######################################### 82 83[root@guest ~]# ifconfig 84eth0: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500 85 inet 10.1.5.141 netmask 255.255.255.0 broadcast 10.1.5.255 86 inet6 fe80::432b:25e4:8f2:328 prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt; 87 ether 52:54:00:12:db:6c txqueuelen 1000 (Ethernet) 88 RX packets 786 bytes 76712 (74.9 KiB) 89 RX errors 0 dropped 0 overruns 0 frame 0 90 TX packets 167 bytes 16793 (16.3 KiB) 91 TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 92 93lo: flags=73\u0026lt;UP,LOOPBACK,RUNNING\u0026gt; mtu 65536 94 inet 127.0.0.1 netmask 255.0.0.0 95 inet6 ::1 prefixlen 128 scopeid 0x10\u0026lt;host\u0026gt; 96 loop txqueuelen 1000 (Local Loopback) 97 RX packets 32 bytes 2592 (2.5 KiB) 98 RX errors 0 dropped 0 overruns 0 frame 0 99 TX packets 32 bytes 2592 (2.5 KiB) 100 TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 101 102 103[root@guest ~]# cat /etc/sysconfig/network-scripts/ifcfg-eth0 104TYPE=\u0026#34;Ethernet\u0026#34; 105PROXY_METHOD=\u0026#34;none\u0026#34; 106BROWSER_ONLY=\u0026#34;no\u0026#34; 107BOOTPROTO=\u0026#34;static\u0026#34; 108DEFROUTE=\u0026#34;yes\u0026#34; 109IPV4_FAILURE_FATAL=\u0026#34;no\u0026#34; 110IPV6INIT=\u0026#34;yes\u0026#34; 111IPV6_AUTOCONF=\u0026#34;yes\u0026#34; 112IPV6_DEFROUTE=\u0026#34;yes\u0026#34; 113IPV6_FAILURE_FATAL=\u0026#34;no\u0026#34; 114IPV6_ADDR_GEN_MODE=\u0026#34;stable-privacy\u0026#34; 115HWADDR=\u0026#34;52:54:00:12:db:6c\u0026#34; 116NAME=\u0026#34;eth0\u0026#34; 117UUID=\u0026#34;c43aa0f0-7327-4807-975b-9e1dc5e46332\u0026#34; 118DEVICE=\u0026#34;eth0\u0026#34; 119ONBOOT=\u0026#34;yes\u0026#34; 120IPADDR=\u0026#34;10.1.5.141\u0026#34; 121PREFIX=\u0026#34;24\u0026#34; 122GATEWAY=\u0026#34;10.1.5.254\u0026#34; 123DNS1=\u0026#34;10.1.1.3\u0026#34; 原因 \u0026amp; 解決方法\rThe problem is that Docker (which is installed on host machine) changes the default policy for the FORWARD chain in iptables to DROP. 😑\nTo fix the issue, a rule to allow traffic has to be added. Running this command added the required rule:\n1# from host 2iptables -I FORWARD -i br0 -o br0 -j ACCEPT Reference\rhttps://askubuntu.com/questions/1134115/kvm-guest-cant-access-internet\rhttps://www.reddit.com/r/linuxadmin/comments/bdy6sz/kvm_guest_cant_access_internet/\r","date":"2022-02-08T20:46:00+00:00","updated":"2022-02-08T20:46:00+00:00"},{"objectID":"1644346680","permalink":"/post/os-centos-crash-and-analysis-kexec-kdump/centos-crash-analysis/","title":"[CentOS] Crash Analysis (kexec \u0026 kdump)","content":"原理\rkexec（kernel execution）是 Linux 核心的一種機制，允許從當前執行的核心啟動新核心。kexec 會繞過系統韌體 (BIOS or UEFI) 的初始化，並直接將新核心載入到主記憶體執行，可以實現系統的快速重啟。\nkdump 是一種基於 kexec 實現的核心崩潰轉儲技術。當系統崩潰時，kdump 使用 kexec 啟動另一個核心並獲得記憶體轉儲，並使用它來匯出和儲存記憶體轉儲來保持系統一致性，最終會匯出一個記憶體映像（也稱為 vmcore），該映像可用於除錯和確定崩潰的原因。\ncrash 是一個被廣泛應用的核心崩潰轉儲檔案分析工具，可以通過 crash 分析 vmcore 檔案以分析出核心崩潰的原因。\n安裝\r1. 檢視系統核心\r1[root@kvm2 ~]# uname -r 23.10.0-1127.el7.x86_64 另外說明一下目前使用的 CentOS 版本為 7.9\n1[root@kvm2 ~]# cat /etc/centos-release 2CentOS Linux release 7.9.2009 (Core) 2. 安裝 kexec \u0026amp;amp; crash\r1yum install crash kexec-tools -y kdump 通常在安裝 CentOS 時就會預設開啟了\nNote\n網路上有些教學會寫需要修改 grub 的 crashkernel 預留記憶體大小、以及更新 kdump.conf 配置，但也可以不配置、不更新；皆保持預設。 在 Linux 4.15 中預設使用 crashkernel=auto，kernel 將通過 memblock_find_in_range 自動計算核心的記憶體大小和起始位置，但是有些核心可能不支持，需要手動指定。 如果要自己手動設定，可參考\r這篇教學\r\u0026amp;lt;i class=\u0026amp;quot;fa fa-external-link-alt\u0026amp;quot;\u0026amp;gt;\u0026amp;lt;/i\u0026amp;gt;\r3. 安裝 kernel-debuginfo\r使用 crash 除錯核心轉儲檔案，需要安裝 crash 工具和核心除錯工具 kernel-debuginfo。下載連結 http://debuginfo.centos.org/7/x86_64/\r1wget …","date":"2022-02-08T18:58:00+00:00","updated":"2022-02-08T18:58:00+00:00"},{"objectID":"1643219160","permalink":"/post/programming-python-golang-selenium-screenshot-specific-area/screenshot-specific-area-by-selenium/","title":"[Python \u0026 Golang] Selenium Screenshot to the Specific Area","content":"本篇文章紀錄如何使用 python 以及 golang 改寫的 selenium，螢幕截圖指定網址的特定範圍並存成圖片。\n目標\r預計爬取 selenium 官網\r的 project 頁面，並擷取指定範圍存成圖片。 Python Sample Code\r1from PIL import Image 2import time 3from selenium import webdriver 4from selenium.webdriver.chrome.options import Options 5 6print(\u0026#34;開始爬取\u0026#34;) 7 8if __name__ == \u0026#34;__main__\u0026#34;: 9 options = Options() 10 options.add_argument(\u0026#39;--headless\u0026#39;) 11 options.add_argument(\u0026#39;--no-sandbox\u0026#39;) 12 options.add_argument(\u0026#39;lang=zh_TW.UTF-8\u0026#39;) 13 driver = webdriver.Chrome(\u0026#39;./chromedriver\u0026#39;, options=options) 14 driver.set_window_size(1400, 1500) # 設定視窗大小 15 16 driver.get(\u0026#34;https://www.selenium.dev/projects/\u0026#34;) 17 time.sleep(1) 18 19 driver.save_screenshot(\u0026#34;./scrnsht.png\u0026#34;) 20 21 # crop curve table only 22 ele = driver.find_elements_by_xpath(\u0026#34;/html/body/div/main/div[1]/div\u0026#34;) 23 24 left = ele[0].location[\u0026#39;x\u0026#39;] 25 top = ele[0].location[\u0026#39;y\u0026#39;] 26 right = left + ele[0].size[\u0026#39;width\u0026#39;] 27 bottom = top + ele[0].size[\u0026#39;height\u0026#39;] 28 29 im = Image.open(\u0026#34;./scrnsht.png\u0026#34;) 30 im = im.crop((left, top, right, bottom)) 31 im.save(\u0026#34;./crop.png\u0026#34;) 32 33 driver.quit() 34 35print(\u0026#34;爬取完成\u0026#34;) Golang(tebeka/selenium) Sample Code\r1package main 2 3import ( 4\t\u0026#34;fmt\u0026#34; 5\t\u0026#34;image\u0026#34; 6\t\u0026#34;image/png\u0026#34; 7\t\u0026#34;io/ioutil\u0026#34; 8 9\t\u0026#34;os\u0026#34; 10\t\u0026#34;time\u0026#34; 11 12\t\u0026#34;github.com/tebeka/selenium\u0026#34; 13\t\u0026#34;github.com/tebeka/selenium/chrome\u0026#34; 14) 15 16func main() { 17 18\topts := []selenium.ServiceOption{ 19\tselenium.Output(os.Stderr), // Output debug information to STDERR 20\t} 21\tservice, err := selenium.NewChromeDriverService(\u0026#34;/home/nexdata/chromedriver\u0026#34;, 9515, opts...) 22\tif err != nil { 23\tfmt.Printf(\u0026#34;Error starting the ChromeDriver server: %v\u0026#34;, err) 24\t} 25\tdefer service.Stop() 26 27\t// call browser 28\tcaps := selenium.Capabilities{ 29\t\u0026#34;browserName\u0026#34;: \u0026#34;chrome\u0026#34;, 30\t} 31\t// set chrome arguments 32\tchromeCaps := chrome.Capabilities{ 33\tArgs: []string{ 34\t\u0026#34;--headless\u0026#34;, // do not open the browser (run in background) 35\t\u0026#34;--no-sandbox\u0026#34;, // allow non-root to execute chrome 36\t\u0026#34;--disable-deb-shm-usage\u0026#34;, 37\t\u0026#34;--window-size=1400,1500\u0026#34;, 38\t//\u0026#34;--start-maximized\u0026#34;, 39\t}, 40\t} 41\tcaps.AddChrome(chromeCaps) 42 43\t// connect to the webdriver instance which running locally 44\twd, err := selenium.NewRemote(caps, \u0026#34;http://127.0.0.1:9515/wd/hub\u0026#34;) 45\tif err != nil { 46\tfmt.Printf(\u0026#34;connect to the webDriver faild: %v\u0026#34;, err) 47\t} 48\t// delay closing Chrome 49\tdefer wd.Quit() 50 51\t// connect to the target website 52\tif err := wd.Get(\u0026#34;https://www.selenium.dev/projects/\u0026#34;); err != nil { 53\tfmt.Printf(\u0026#34;connect to the reflow server failed: %v\u0026#34;, err) 54\t} 55 56\ttime.Sleep(time.Duration(1) * time.Second) 57 58\tele, err := wd.FindElement(selenium.ByXPATH, \u0026#34;/html/body/div/main/div[1]/div\u0026#34;) 59\tif err != nil { 60\tfmt.Printf(\u0026#34;target element doesn\u0026#39;t exist!\u0026#34;) 61\t} 62\tscrnsht, _ := wd.Screenshot() 63\tioutil.WriteFile(\u0026#34;scrnsht.png\u0026#34;, scrnsht, 0666) 64\tloc, _ := ele.Location() 65\tsz, _ := ele.Size() 66\t// fmt.Println(loc) 67\t// fmt.Println(sz) 68\tfile, _ := os.Open(\u0026#34;./scrnsht.png\u0026#34;) 69\tdefer file.Close() 70\timg, _ := png.Decode(file) 71\tsub_image := img.(interface { 72\tSubImage(r image.Rectangle) image.Image 73\t}).SubImage(image.Rect(loc.X, loc.Y, loc.X+sz.Width, loc.Y+sz.Height)) 74\tfile, _ = os.Create(\u0026#34;./crop.png\u0026#34;) 75\tpng.Encode(file, sub_image) 76 77\tfmt.Println(\u0026#34;爬取完成\u0026#34;) 78} Result\rscrnsht.png\rcrop.png\r","date":"2022-01-26T17:46:00+00:00","updated":"2022-01-26T17:46:00+00:00"},{"objectID":"1643045220","permalink":"/post/devops-selenium-chrome-docker-img/build-selenium-chrome-docker-image/","title":"[Docker] 包 Selenium Chrome Docker Image","content":"紀錄一下把 golang 改寫的 selenium 爬蟲程式包成 docker image 遇到的問題。\n最終的 Dockerfile\rChrome 跟 chromedrive 竟然都可以透過 apk 安裝，方便又輕鬆 👍\n1FROM golang:1.15.3-alpine3.12 AS build 2WORKDIR / 3COPY . . 4RUN CGO_ENABLED=0 go build -o /grabreflow -ldflags=\u0026amp;#34;-s -w\u0026amp;#34; 5 6 7FROM alpine:edge 8WORKDIR / 9COPY ./config /config 10COPY ./view /view 11COPY --from=build /grabreflow /grabreflow 12RUN apk add --no-cache chromium chromium-chromedriver \u0026amp;amp;\u0026amp;amp; apk add wqy-zenhei --update-cache --repository https://nl.alpinelinux.org/alpine/edge/testing 13ENTRYPOINT [ \u0026amp;#34;/grabreflow\u0026amp;#34; ] 踩坑紀錄\r1. 使用 selenium 官方 docker image standalone-chrome 時無法讀寫檔案\rDockerfile\r1FROM golang:1.15.3-alpine3.12 AS build 2WORKDIR / 3COPY . . 4RUN CGO_ENABLED=0 go build -o /grabreflow -ldflags=\u0026amp;#34;-s -w\u0026amp;#34; 5 6FROM selenium/standalone-chrome 7WORKDIR / 8COPY ./config /config 9COPY ./view /view 10COPY --from=build /grabreflow /grabreflow 11COPY ./chromedriver /chromedriver 12ENTRYPOINT [ \u0026amp;#34;/grabreflow\u0026amp;#34; ] 1docker build -t …","date":"2022-01-24T17:27:00+00:00","updated":"2022-01-24T17:27:00+00:00"},{"objectID":"1642590180","permalink":"/post/data-oracle-11g-xe-cdc-debezium/install-oracle-11g-and-establish-cdc-by-debezium/","title":"[Oracle] Install Oracle 11g XE and Establish CDC by Debezium","content":"Debezium\r簡介\rDebezium 是一個由 RedHat 開源的基於資料庫變更日誌的實時變更數據捕獲（CDC）工具，構建在 Apache Kafka 之上，是 Apache Kafka Connect 的 Source Connector，可以實時獲取行級別（row-level）資料的更改事件（INSERT、UPDATE 和 DELETE）並同步到 Kafka。目前支援的常見資料庫有 MySQL(binlog)、Oracle(logminer or xstream)、MongoDB(change streams)、PostgreSQL(logical replication stream mode)、SQL Server(transaction log)\u0026amp;hellip;等。本文範例是使用 Oracle 的 logminer 日誌透過 Debezium 獲取指定資料庫的變更事件。\nCDC\rCDC 全稱是 Change Data Capture 變更數據捕獲，它是一個比較廣義的概念，只要能捕獲變更的資料，都可以稱為 CDC，主要有基於查詢的 CDC 和基於日誌的 CDC。\n基於查詢的 CDC 基於日誌的 CDC 概念 每次捕獲變更發起 Select 查詢進行全表掃描，過濾出查詢之間變更的資料 讀取資料儲存系統的 log ，例如 Mysql 裡面的 binlog持續監控 開源產品 Sqoop, Kafka JDBC Source Canal, Maxwell, Debezium 執行模式 Batch Streaming 捕獲所有資料的變化 X O 低延遲，不增加資料庫負載 X O 不侵入業務（LastUpdated欄位） X O 捕獲刪除事件和舊記錄的狀態 X O 捕獲舊記錄的狀態 X O 安裝 Oracle 11g Express Edition\rCreate Container\r1docker run -d -it --name oracle -p 1521:1521 -e ORACLE_ALLOW_REMOTE=true -v oracle:/u01/app/oracle --restart=always wnameless/oracle-xe-11g-r2 Connect to the Database\r1$ docker ps 2CONTAINER ID …","date":"2022-01-19T11:03:00+00:00","updated":"2022-01-19T11:03:00+00:00"},{"objectID":"1641830580","permalink":"/post/programming-golang-tebeka-selenium/golang-tebeka-selenium/","title":"[golang] 使用 tebeka selenium 爬蟲模擬操控網頁","content":"最近碰到一個需從網頁去擷取圖片的需求，原本拿到的 sample code 是由 python 的 selenium 爬的，但後來發現有大神將此包改寫成 golang tebeka/selenium\r，所以就試著改寫看看，在此紀錄一下。\n目標網頁爬取需求\r目標網頁是一個爐溫監控的網站，任務是爬取指定產品所經的迴焊爐的生產歷史紀錄。需要模擬的步驟如下\n查詢頁面需先輸入的參數有： 下拉式選單選取線別 文字框時間範圍 （開始與結束） 文字框產品編號 點選查詢按鈕 回傳搜尋結果表格，點選表格的每一列的任意位置/欄位 彈出一個 modal 視窗，內容為被點選列的詳細爐溫圖表 擷取螢幕並裁切圖片到目標範圍存到本機 點選 OK 按鈕以關閉 modal 視窗 繼續點選下列，重複 3～6 步驟，直到表格的最後一列資訊 運行環境\rUbuntu v20.04 Golang v1.15 Google Chrome v96.0.4664.110 事前準備\r須在 ubuntu 下載 Chrome 以及相對應的 chromedriver。\n更新系统 1sudo apt-get update 安裝相關的必要套件 1sudo apt-get install libxss1 libappindicator1 libindicator7 2# 安裝 xvfb 以便可以用 headless 模式（跑在背景，不開啟瀏覽器）運行 Chrome 3sudo apt-get install xvfb 下載安裝包 1wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb 安装 chrome 1sudo apt-get install dpkg 2# 安装chrome，安裝過程中由於缺少一些依賴而報錯是正常的 3sudo dpkg -i google-chrome*.deb 4#自動安裝上一步缺少的依賴 5sudo apt-get install -f 下載 chromedriver 確認安裝的 Chrome 版本，並下載與之匹配的 chromedriver。 1wget -N …","date":"2022-01-10T16:03:00+00:00","updated":"2022-01-10T16:03:00+00:00"},{"objectID":"1641478200","permalink":"/post/programming-golang-mod-replace-for-fork-repo/go-mod-replace-fork-repo/","title":"[golang]  go mod replace 解决 fork repo 匯入問題","content":"開發程式時使用 github 上各大神開發的第三方套件，有時候有自己的額外需求需要進行改造，所以 fork 到自己的 github 修改後，再 import 到自己的專案中。但當在進行構建的時候，報錯如下：\n1go get: github.com/bbb/xxx@v1.0.2: parsing go.mod: 2 module declares its path as: github.com/aaa/xxx 3 but was required as: github.com/bbb/xxx 解决方式\r使用 replace 將新的 package 去替換另一個 package，他們可以是不同的 package，也可以是同一個 package 的不同版本。 基本語法：\n1$ go mod edit -replace=old[@v]=new@v 新的 package 後面的 version 不可省略，可以是 release 版本號或是 git 的提交號（commit-id）。 （edit所有操作都需要版本 tag）\n也可以直接编辑 go.mod 文件：\n1module test 2 3go 1.16 4 5require ( 6\tgithub.com/gin-contrib/cors v1.3.1 7\tgithub.com/gin-gonic/gin v1.7.7 8\tgithub.com/sirupsen/logrus v1.8.1 9\tgithub.com/spf13/viper v1.10.1 10\tgithub.com/tebeka/selenium v0.9.9 11\tgopkg.in/alecthomas/kingpin.v2 v2.2.6 12) 13 14replace github.com/tebeka/selenium =\u0026gt; github.com/ulahsieh/selenium v0.9.10-0.20220105013444-c7d3f285d0e7 15// 注意版本號是用空格隔開 完成後在程式下面跑 go mod tidy 獲取新的套件並取代舊的，就大功告成啦！\nReference\rhttps://github.com/Bpazy/blog/issues/164\rhttps://www.cnblogs.com/sunsky303/p/12150575.html\r","date":"2022-01-06T14:10:00+00:00","updated":"2022-01-06T14:10:00+00:00"},{"objectID":"1641416700","permalink":"/post/programming-golang-notepad-highlight/golang-highlight-on-notepad/","title":"為 Notepad++ 加上 golang 語法高亮","content":"原生 Notepad++ 沒有支援 golang 的語法，在此紀錄一下要怎麼在 Notepad 中加入自定義的 golang 程式語言的語法高亮(使用暗黑模式 Obsidian)。目前 notepad 使用的版本為 v8.1.9.3。\n定義使用者自訂語言\r在工具列中點選 語言\u0026amp;gt;自訂程式語法\u0026amp;gt;開啟自訂語法樣式資料夾，在該資料下新增一個 userDefineLang-Go-Obsidian.xml 檔案，內容為以下 xml 代碼。\n1\u0026amp;lt;NotepadPlus\u0026amp;gt; 2 \u0026amp;lt;UserLang name=\u0026amp;#34;Go\u0026amp;#34; ext=\u0026amp;#34;go\u0026amp;#34; udlVersion=\u0026amp;#34;2.1\u0026amp;#34;\u0026amp;gt; 3 \u0026amp;lt;Settings\u0026amp;gt; 4 \u0026amp;lt;Global caseIgnored=\u0026amp;#34;no\u0026amp;#34; allowFoldOfComments=\u0026amp;#34;yes\u0026amp;#34; foldCompact=\u0026amp;#34;no\u0026amp;#34; forcePureLC=\u0026amp;#34;0\u0026amp;#34; decimalSeparator=\u0026amp;#34;0\u0026amp;#34; /\u0026amp;gt; 5 \u0026amp;lt;Prefix Keywords1=\u0026amp;#34;no\u0026amp;#34; Keywords2=\u0026amp;#34;no\u0026amp;#34; Keywords3=\u0026amp;#34;no\u0026amp;#34; Keywords4=\u0026amp;#34;no\u0026amp;#34; Keywords5=\u0026amp;#34;no\u0026amp;#34; Keywords6=\u0026amp;#34;no\u0026amp;#34; Keywords7=\u0026amp;#34;no\u0026amp;#34; Keywords8=\u0026amp;#34;no\u0026amp;#34; /\u0026amp;gt; 6 \u0026amp;lt;/Settings\u0026amp;gt; 7 \u0026amp;lt;KeywordLists\u0026amp;gt; 8 \u0026amp;lt;Keywords name=\u0026amp;#34;Comments\u0026amp;#34;\u0026amp;gt;00// 01 02 03/* 04*/\u0026amp;lt;/Keywords\u0026amp;gt; 9 \u0026amp;lt;Keywords name=\u0026amp;#34;Numbers, prefix1\u0026amp;#34;\u0026amp;gt;\u0026amp;lt;/Keywords\u0026amp;gt; 10 \u0026amp;lt;Keywords name=\u0026amp;#34;Numbers, prefix2\u0026amp;#34;\u0026amp;gt;0X 0x 0\u0026amp;lt;/Keywords\u0026amp;gt; 11 \u0026amp;lt;Keywords …","date":"2022-01-05T21:05:00+00:00","updated":"2022-01-05T21:05:00+00:00"},{"objectID":"1637516160","permalink":"/post/kubernets-ca-expired/","title":"Unable to connect to the server: x509: certificate has expired or is not yet valid","content":"在下 kubectl 時出現 Unable to connect to the server: x509: certificate has expired or is not yet valid 的錯誤，原因是 kubernetes apiserver 證書已過期，kubernetes 的 apiServer 與 kubelet 的訪問授權證書是一年，官方表示通過這種方式，讓用戶不斷的升級版本。\n目前有幾種解決方式：\n重新生成證書取代過期的證書 (本次作法) 升級集群以自動更新證書 部屬一套新的環境，將業務遷移過去 去掉證書驗證功能 (不安全且不科學，需要自己改 source code) 查看證書的有效日期\r透過 openssl 直接查證書內容\n1$ openssl x509 -in /etc/kubernetes/pki/apiserver.crt -noout -text | grep Not 2 Not Before: Nov 17 04:48:20 2020 GMT 3 Not After : Nov 17 04:48:20 2021 GMT 或是透過 kubeadm 檢查 Kubernetes 環境證書\n1$ kubeadm alpha certs check-expiration 2[check-expiration] Reading configuration from the cluster... 3[check-expiration] FYI: You can look at this config file with \u0026#39;kubectl -n kube-system get cm kubeadm-config -oyaml\u0026#39; 4[check-expiration] Error reading configuration from the Cluster. Falling back to default configuration 5 6W1118 09:51:35.880390 7092 configset.go:348] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io] 7CERTIFICATE EXPIRES RESIDUAL TIME CERTIFICATE AUTHORITY EXTERNALLY MANAGED 8admin.conf Nov 17, 2021 04:48 UTC \u0026lt;invalid\u0026gt; no 9apiserver Nov 17, 2021 04:48 UTC \u0026lt;invalid\u0026gt; ca no 10apiserver-etcd-client Nov 17, 2021 04:48 UTC \u0026lt;invalid\u0026gt; etcd-ca no 11apiserver-kubelet-client Nov 17, 2021 04:48 UTC \u0026lt;invalid\u0026gt; ca no 12controller-manager.conf Nov 17, 2021 04:48 UTC \u0026lt;invalid\u0026gt; no 13etcd-healthcheck-client Nov 17, 2021 04:48 UTC \u0026lt;invalid\u0026gt; etcd-ca no 14etcd-peer Nov 17, 2021 04:48 UTC \u0026lt;invalid\u0026gt; etcd-ca no 15etcd-server Nov 17, 2021 04:48 UTC \u0026lt;invalid\u0026gt; etcd-ca no 16front-proxy-client Nov 17, 2021 04:48 UTC \u0026lt;invalid\u0026gt; front-proxy-ca no 17scheduler.conf Nov 17, 2021 04:48 UTC \u0026lt;invalid\u0026gt; no 18 19CERTIFICATE AUTHORITY EXPIRES RESIDUAL TIME EXTERNALLY MANAGED 20ca Nov 15, 2030 04:48 UTC 8y no 21etcd-ca Nov 15, 2030 04:48 UTC 8y no 22front-proxy-ca Nov 15, 2030 04:48 UTC 8y no 經查看 k8s master 組件證書都過期了。\n更新證書\r備份舊有的配置文件與證書 1$ cp -rf /etc/kubernetes /etc/kubernets.bak 更新證書 1$ kubeadm alpha certs renew all 2[renew] Reading configuration from the cluster... 3[renew] FYI: You can look at this config file with \u0026#39;kubectl -n kube-system get cm kubeadm-config -oyaml\u0026#39; 4[renew] Error reading configuration from the Cluster. Falling back to default configuration 5 6W1118 11:11:52.322016 26585 configset.go:348] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io] 7certificate embedded in the kubeconfig file for the admin to use and for kubeadm itself renewed 8certificate for serving the Kubernetes API renewed 9certificate the apiserver uses to access etcd renewed 10certificate for the API server to connect to kubelet renewed 11certificate embedded in the kubeconfig file for the controller manager to use renewed 12certificate for liveness probes to healthcheck etcd renewed 13certificate for etcd nodes to communicate with each other renewed 14certificate for serving etcd renewed 15certificate for the front proxy client renewed 16certificate embedded in the kubeconfig file for the scheduler manager to use renewed 重新生成配置文件\n這些配置文件中包含證書，所以需要重新生成\n1$ rm -rf /etc/kubernetes/*.conf 2$ kubeadm init phase kubeconfig all --apiserver-advertise-address 10.1.5.21 更新配置身份認證的 $HOME/.kube/config 檔案\n將重新生成於 /etc/kubernetes 下的 admin.conf 檔案覆蓋原先的 ~/.kube/config 1$ cp -i /etc/kubernetes/admin.conf $HOME/.kube/config 2$ chown $(id -u):$(id -g) $HOME/.kube/config 重新啟動 kubelet \u0026amp; docker service 1$ systemctl restart kubelet 2$ systemctl restart docker 重新使用 kubectl 訪問集群 1$ kubectl get nodes 2NAME STATUS ROLES AGE VERSION 3k8sm1 Ready master 365d v1.17.13 4k8sm2 Ready master 365d v1.17.13 5k8sm3 Ready master 365d v1.17.13 6k8sw1 Ready \u0026lt;none\u0026gt; 365d v1.17.13 7k8sw2 Ready \u0026lt;none\u0026gt; 365d v1.17.13 8k8sw3 Ready \u0026lt;none\u0026gt; 365d v1.17.13 如果是多 master，上面的步驟在每個 master 都要做 Reference\rhttps://stackoverflow.com/questions/56320930/renew-kubernetes-pki-after-expired\rhttps://cloud.tencent.com/developer/article/1832411\r","date":"2021-11-21T17:36:00+00:00","updated":"2021-11-21T17:36:00+00:00"},{"objectID":"1636369380","permalink":"/post/os-scp-exclude-specific-file/linux-scp-exclude-files/","title":"[Linux] SCP 傳送排除特定檔案或資料夾 (如 .git)","content":"Scp all files and folders to the remote server without copying .git \u0026amp; other dot files/folders:\n1cd sourceFolder 2scp -r [!.]* root@remoteServer:/root/targetForder The command means transfer all the files/folders * under current directory except [!] the files\u0026rsquo;/folders\u0026rsquo; named starting with .\n","date":"2021-11-08T11:03:00+00:00","updated":"2021-11-08T11:03:00+00:00"},{"objectID":"1635713400","permalink":"/post/blog-hexo-dark-mode/hexo-darkmode/","title":"[Hexo] 設定可切換的 Dark Mode","content":"在 NexT 主題中引入 Darkmode.js 以支持網頁的 Dark Mode。\n打開 .\\themes\\next\\layout\\_scripts 文件夾內的 vendors.njk 文件，在末尾添加以下代碼：\n1\u0026lt;script src=\u0026#34;https://cdn.jsdelivr.net/npm/darkmode-js@1.5.7/lib/darkmode-js.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 2\u0026lt;script src=\u0026#34;https://cdn.jsdelivr.net/npm/darkmode-js@1.5.7/lib/darkmode-js.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 3\u0026lt;script\u0026gt; 4 function addDarkmodeWidget() { 5 const options = { 6 bottom: \u0026#39;64px\u0026#39;, // default: \u0026#39;32px\u0026#39; 7 right: \u0026#39;unset\u0026#39;, // default: \u0026#39;32px\u0026#39; 8 left: \u0026#39;32px\u0026#39;, // default: \u0026#39;unset\u0026#39; 9 time: \u0026#39;1s\u0026#39;, // default: \u0026#39;0.3s\u0026#39; 10 mixColor: \u0026#39;#fff\u0026#39;, // default: \u0026#39;#fff\u0026#39; 11 backgroundColor: \u0026#39;#fff\u0026#39;, // default: \u0026#39;#fff\u0026#39; 12 buttonColorDark: \u0026#39;#100f2c\u0026#39;, // default: \u0026#39;#100f2c\u0026#39; 13 buttonColorLight: \u0026#39;#fff\u0026#39;, // default: \u0026#39;#fff\u0026#39; 14 saveInCookies: true, // default: true, 15 label: \u0026#39;🌓\u0026#39;, // default: \u0026#39;\u0026#39; 16 autoMatchOsTheme: true // default: true 17 } 18 const darkmode = new Darkmode(options); 19 darkmode.showWidget(); 20 } 21 window.addEventListener(\u0026#39;load\u0026#39;, addDarkmodeWidget); 22\u0026lt;/script\u0026gt; p.s. 用此方式設定時，此代碼和在 Next 中插入背景圖片以及透明化會有衝突，但剛好目前都沒有該設定，就直接套用了。\n","date":"2021-10-31T20:50:00+00:00","updated":"2021-10-31T20:50:00+00:00"},{"objectID":"1635700980","permalink":"/post/blog-hexo-next-upgrade/hexo-and-next-upgrade/","title":"[Hexo] 升級至 5.x 與 NexT 主題 8.x","content":"Hexo 升級\rHexo 版本及系統插件可以透過 npm 實現，請按照下列步驟執行：\n1# 全局升級 hexo-cli 2npm install hexo-cli -g 3 4# 檢察已安裝的插件(package.json)是否可升級 5npm install -g npm-check 6npm-check 7 8# 升級系統中的插件 9npm install -g npm-upgrade 10npm-upgrade 11 12# 更新全局插件 13npm update -g 14npm update --save 15 16# 查看 hexo 版本，看是否有升級成功 17hexo v 更新完成後我就直接下 hexo g 了，因為前後版的配置有些許差異，所以報錯不須緊張，只要根據錯誤訊息修改已 deprecated 的配置項。\n1# Deprecated 2external_link: true|false 3# New option 4external_link: 5 enable: true # Open external links in new tab 6 field: site # Apply to the whole site 7 exclude: \u0026#39;\u0026#39; 8 9# Deprecated 10use_date_for_updated: true 11# New option 12updated_option: date NexT升級\r用不同於原先版本的名字，下載新的 v8 倉庫 1git clone https://github.com/next-theme/hexo-theme-next themes/next8 如此，你可以在不修改原有的 NexT 舊版目錄的同時使用 next8 目錄中的新版本主題。\n在 hexo 的主配置文件設置主題： 1theme: next8 如此，新主題將在生成站點時被加載。如果升級途中遇到了任何錯誤、或只是不喜歡這一新版本，可以隨時切換回舊版本。\n比對新舊主題的 _config.yml，針對客製化的樣式進行設定，如字體、icon\u0026hellip; Reference\rhttps://www.cylong.com/blog/2020/08/10/update-hexo-next/\r","date":"2021-10-31T17:23:00+00:00","updated":"2021-10-31T17:23:00+00:00"},{"objectID":"1635515280","permalink":"/post/devops-install-gitlab-by-docker-compose/install-gitlab-server-by-docker-compose/","title":"[Docker] 使用 docker-compose 建立 GitLab (w/ https)","content":"安裝 docker \u0026amp;amp; docker compose\r請參考之前的筆記\nInstall Docker \u0026amp;amp; Docker Compose on CentOS\rInstall Docker \u0026amp;amp; Docker Compose on Ubuntu18.04\r準備自簽憑證\r建立 ssl.conf 設定檔\r1[req] 2prompt = no 3default_md = sha256 4default_bits = 2048 5distinguished_name = dn 6x509_extensions = v3_req 7 8[dn] 9C = TW 10ST = Taiwan 11L = Taipei 12O = ABC Inc. 13OU = IT Department 14emailAddress = ulahsieh@abc.com 15CN = 10.1.5.8 16 17[v3_req] 18subjectAltName = @alt_names 19 20[alt_names] 21DNS.1 = 10.1.5.8 22IP.1 = 10.1.5.8 [dn] 區段 (\rDistinguished Name\r) 為憑證的相關資訊\nC: 國碼，臺灣是 TW ST: 州 L: 地區 O: 組織名稱 OU: 部門名稱 emailAddress: E-Mail CN: 憑證名稱，通常填域名名稱 alt_names 用來設定 SSL 憑證的域名。可以設定很多組，也可以把區網的 IP 填上去。\n產生自簽憑證與私密金鑰\r1openssl req -x509 -new -nodes -sha256 -utf8 -days 3650 -newkey rsa:2048 -keyout gitlab.key -out gitlab.crt -config ssl.conf 產生 stronger DHE parameters 加強 server 安全性\n1openssl dhparam -out dhparam.pem 2048 完成後，總共會產生出兩個檔案，分別是：\nserver.key (私密金鑰) (使用 PEM 格式) (無密碼保護) server.crt (憑證檔案) (使用 PEM 格式) dhparam.pem …","date":"2021-10-29T13:48:00+00:00","updated":"2021-10-29T13:48:00+00:00"},{"objectID":"1635280020","permalink":"/post/kubernets-install-istio/","title":"使用 istio operator 安裝 Istio v1.11","content":"下載 Istio\r下載資源\r用自動化工具下載並提取最新版本（Linux 或 macOS）：\n1$ curl -L https://istio.io/downloadIstio | sh - 或是用指定參數下載指定的、不同處理器體系的版本。例如，下載 x86_64 架構的、1.6.8 版本的 Istio ，運行：\n1$ curl -L https://istio.io/downloadIstio | ISTIO_VERSION=1.6.8 TARGET_ARCH=x86_64 sh - 進入 Istio 包目錄\r1$ cd istio-1.11.4 安裝目錄包含：\nsamples/ 目錄下的示例應用程序 bin/ 目錄下的 [istioctl](https://www.bookstack.cn/read/istio-1.11-zh/af90c7c768b11bf8.md) 客户端二進制文件 設定 istioctl\r將 istioctl 客户端加入執行路径（Linux or macOS）:\n1$ export PATH=$PWD/bin:$PATH 部署 Istio Operator\r1$ istioctl operator init 此命令運行 Operator 在 istio-operator 命名空間中創建以下資源：\nOperator 自定義資源定義（CRD） Operator 控制器的 deployment 對象 一個用來訪問 Operator 指標的服務 Istio Operator 運行必須的 RBAC 規則 查看創建的資源\n1kubectl get all -n istio-operator 安裝 Istio\r可以依據 profile 安裝指定的 istio 套件\n1$ kubectl create ns istio-system 2$ kubectl apply -f - \u0026lt;\u0026lt;EOF 3apiVersion: install.istio.io/v1alpha1 4kind: IstioOperator 5metadata: 6 namespace: istio-system 7 name: istiocontrolplane 8spec: 9 profile: demo 10EOF 各種 profile 的 yaml 檔放置在 ./manifests/profiles 下，可依照需求改內容參數。並使用 $ kubectl apply -f xxx.yaml 部屬。\n安裝 Addons Component\r不同於以往 1.6 以前的版本的 istio yaml 檔可直接指定 addonComponents，在 1.11 版如果要安裝 Kiali、Jaeger 等 addon component，則需要另外部屬。 1# 一次部屬所有 addons 2$ kubectl apply -f samples/addons 3 4# 單獨指定套件部屬 5$ kubectl apply -f samples/addons/kiali.yaml Resource\rhttps://preliminary.istio.io/latest/zh/docs/setup/install/operator/\rhttps://istio.io/latest/docs/setup/getting-started/\r","date":"2021-10-26T20:27:00+00:00","updated":"2021-10-26T20:27:00+00:00"},{"objectID":"1635241440","permalink":"/post/k8s-install-nodered/install-nodered-on-kubernetes/","title":"[Node-RED] Deploy on Kubernetes","content":"原先使用 k8s-at-home 的 helm chart\r部屬，但完成後發現 node 安裝後會 deploy 異常，懷疑是 persistence 設定問題，但又不想花時間深究，所以就直接自己寫 yaml 部屬比較快。\n準備 node-red.yaml 檔 1apiVersion: v1 2kind: PersistentVolumeClaim 3metadata: 4 name: nodered-pvc 5 namespace: node-red 6spec: 7 accessModes: 8 - ReadWriteOnce 9 resources: 10 requests: 11 storage: 10Gi 12 storageClassName: nfs 13 14--- 15 16apiVersion: apps/v1 17kind: Deployment 18metadata: 19 name: node-red 20 namespace: node-red 21spec: 22 replicas: 1 23 selector: 24 matchLabels: 25 app: node-red 26 template: 27 metadata: 28 labels: 29 app: node-red 30 spec: 31 containers: 32 - name: node-red 33 image: nodered/node-red:2.1.2 34 ports: 35 - containerPort: 1880 36 name: node-red 37 protocol: TCP 38 imagePullPolicy: IfNotPresent 39 volumeMounts: 40 - name: nodered-data 41 mountPath: /data 42 volumes: 43 - name: nodered-data 44 persistentVolumeClaim: 45 claimName: nodered-pvc 46 47--- 48 49apiVersion: v1 50kind: Service 51metadata: 52 name: node-red 53 namespace: node-red 54spec: 55 type: NodePort 56 selector: 57 app: node-red 58 ports: 59 - protocol: TCP 60 port: 1880 61 nodePort: 31880 部屬 1kubectl create ns node-red 2kubectl -n node-red apply -f node-red.yaml 完成 1kubectl get all -n node-red 訪問 service 使用 NodePort，則直接使用 control plane 的 IP 以及 NodePort 指定的 port 31880 訪問 Node-RED 即可。 ","date":"2021-10-26T09:44:00+00:00","updated":"2021-10-26T09:44:00+00:00"},{"objectID":"1635241140","permalink":"/post/programming-node-red-mail-node-send-error/nodered-mail-node-error/","title":"[Node-RED] mail node 534-5.7.14 Error","content":"當 google 帳號設置了啟用 允許安全性較低的應用程式，但 node-red 的 mail node 還是出現 534-5.7.14 Please log in via your web browser and then try again. 的錯誤。\n可能是因為 Gmail 自動阻擋了可疑的登入。\n此時到用來發信的 Gmail 信箱，可發現一封 系統已阻止可疑的登入的系統通知信。 或到\r異常活動\r頁面 ，也可看到 應用程式/裝置登入嘗試遭拒 的記錄。 如果通知信、異常活動記錄中的 IP 與時間是正常由我們的程式發出的，則可到 授權存取您的 Google 帳戶 頁面 https://accounts.google.com/DisplayUnlockCaptcha\r，按 繼續。 然到到原本的程式，執行登入/寄信，應該就可以認證了。 其他：\n如果原程式換了 IP，可能會再次被 Gmail 自動阻擋，則須在進行一次以上的的動作。 有時候顯示密碼錯誤，有可能密碼是對的，但被 Google 當成可疑的登入阻擋了。 Source\rhttps://xyz.cinc.biz/2014/09/gmail-password-not-accepted-from-server.html\rhttps://stackoverflow.com/questions/20337040/getting-error-while-sending-email-through-gmail-smtp-please-log-in-via-your-w\r另外補充一下 email node 的使用\nfunction node\n1msg.to = \u0026#39;ulahsieh@domain.com\u0026#39;; 2msg.topic = \u0026#39;信件主旨\u0026#39;; 3msg.payload = \u0026#39;信件內容\u0026#39; 4return msg; 在 function node 寫上寄件者後，mail node 的 To 欄位即可省略。\n","date":"2021-10-26T09:39:00+00:00","updated":"2021-10-26T09:39:00+00:00"},{"objectID":"1635173460","permalink":"/post/os-ubuntu-create-applications-shortcut-on-desktop/ubuntu-app-shortcut/","title":"[Ubuntu] Create Application's Shortcut on Desktop","content":"Follow the steps to create the app\u0026rsquo;s shortcut on desktop in Ubuntu.\nFind the target application\u0026rsquo;s .desktop file The file usually can be found in the following path:\n/usr/share/applications/ /var/lib/snapd/desktop/applications Copy the specific .desktop file to ~/Desktop\nGo the Desktop and right-click the copied .desktop file\nEnable the \u0026ldquo;Allow Launching\u0026rdquo; option\nDone ","date":"2021-10-25T14:51:00+00:00","updated":"2021-10-25T14:51:00+00:00"},{"objectID":"1633713900","permalink":"/post/programming-vscode-ssh-remote-without-password/vscode-remote/","title":"VSCode 免密碼 SSH 到遠端機器","content":"VS Code 安裝插件\r安裝完成後我們就可以看到在左側新增了一個新的Remote欄位\n到 Settings \u0026gt; Extensions \u0026gt;Remote - SSH 中將 Remote.SSH: Show Login Terminal 勾選\nWindows 設定安裝 OpenSSH\r使用 Windows Server 2019 和 Windows 10 裝置上的 Windows 設定，安裝 OpenSSH 元件。\n若要安裝 OpenSSH 元件：\n開啟 設定，選取 [應用程式] \u0026gt; 應用程式 \u0026amp; 功能，然後選取 [選用功能]。 掃描清單，查看是否已安裝 OpenSSH。 如果沒有，請在頁面頂端選取 [ 新增功能]，然後： 尋找 OpenSSH 用戶端，然後按一下 [安裝]。 尋找 OpenSSH 伺服器，然後按一下 [安裝]。 安裝程式完成後，請返回 應用程式 \u0026gt; 應用程式 \u0026amp; 功能 和 選用功能 ，應該會看到已列出 OpenSSH。\nSSH Key 設定\rRemote-SSH extension 提供我們使用 ssh key 的方式進行連接，可以不用輸密碼更方便的連接。方式是將本地端的公鑰（id_rsa.pub）存到遠端的 authorized_keys 檔案中，然後在本地端的 config 檔中設定連線資訊。\n在本地機器上使用 ssh-keygen 產生 key pair 將本地公開金鑰加入到遠端機器上的 authorized_keys 檔案中，若遠端機器無該檔案，則 touch ~/.ssh/authorized_keys，並修改權限 chmod 644 ~/.ssh/authorized_keys，最後將本地端的 id_rsa.pub 內容複製到檔案中。 修改本地端機器的家目錄下的 .ssh 中的 C:\\Users\\%USER%\\.ssh\\config 檔 1Host remoteLinux #填寫別名例如 LabSever 2HostName 10.1.5.130 #主機名稱或是ip位置 3User root #登入的使用者名稱 4IdentityFile ~/.ssh/id_rsa #指定連線的私鑰 測試是否可 ssh 登入，開啟 CMD 執行指令 ssh 登入帳戶@ServerIP，即可免密碼直接登入 Linux Server，如果金鑰名稱不為預設 id_rsa，則使用 -i 參數指定金鑰 ssh 登入帳戶@ServerIP -i C:\\Users\\%USER%\\.ssh\\mykey。 SSH Key 原理\r透過\r公開金鑰加密\r(Public-key cryptography) 或稱「非對稱金鑰加密」的兩把加解密鑰匙 Public Key (公鑰) 和 Private Key (私鑰)，取代 Client 使用 SSH\r(Secure Shell，安全殼協議) 協定連結 Server 登入時必須輸入驗證密碼的動作：\nPublic Key：流通於公開網路上，在各伺服器上公鑰集中保存的檔案為 authorized_keys，必須依據 /etc/ssh/sshd_config 內的 AuthorizedKeysFile 定義來設定。 Private Key：存放於各伺服器的本地端，用來解密公開給不同來源端的 Public Key，不可外流。 權限\r雖然 ssh-keygen 預設的權限就都是正確的了，但還是有必要了解一些規則，因為只要一個設定有誤，就可能會被判定為危險，而造成 Public Key 和 Private Key 無法順利比對：\n目錄或檔案 許可權 ~/.ssh/ 700 (drwx\u0026mdash;\u0026mdash;) ~/.ssh/id_rsa (Private Key) 600 (-rw\u0026mdash;\u0026mdash;-) ~/.ssh/authorized_keys \u0026amp; ~/.ssh/id_rsa.pub (Public Key) 600 (-rw\u0026mdash;\u0026mdash;-)。預設為 600，可以將許可權變更為 640 或 644，使公開金鑰變成可讀取。 Reference\rhttps://hackmd.io/@brick9450/vscode-remote\rhttps://www.footmark.info/linux/centos/windows-ssh-nopassword-linux/\r","date":"2021-10-08T17:25:00+00:00","updated":"2021-10-08T17:25:00+00:00"},{"objectID":"1633620720","permalink":"/post/data-datapipeline-definition/what-is-data-pipeline/","title":"Data Pipeline 意義、要解決什麼、應具備的功能","content":"意義\r數據管道（Data Pipeline）是一種允許數據通過數據分析過程從一個位置高效流向另一個位置的軟體，是處理資料流的系統。這就好比一條傳送帶，它能高效、準確地將數據傳送到流程的每一步。 數據管道是提取、轉換、合併、驗證、進一步分析數據和數據可視化的過程自動化。通過消除錯誤並避免瓶頸和延遲，數據管道可提供端到端效率。 數據管道將所有數據視為流式數據，因此它們考慮了靈活的架構。無論數據來自靜態源還是實時源，數據管道都可以將數據流分割成更小的片段，以便並行處理，從而提升了計算能力。 ETL 是 Data Pipeline 的一部分，一條 Data Pipeline 可以是好幾段 ETL 的組合。 ETL 通常指資料的取、用、存這三個動作而已；而 Data Pipeline 除了 ETL 之外，也可包含執行 ETL 的系統。 要解決什麼\r傳統使用 ETL tools 可能會遇到的痛點：\n問題1 ：\n使用的 RDS 類型多種多樣，有 ORALCE、SQL SERVER、MYSQL，甚至有 MONGODB。現在要進行大數據分析，如何整合這些數據庫的數據，到一個大數據平台進行數據分析？如何解決多個數據源數據獲取不及時造成數據獲取延遲？\n問題2：\n數據設計之初，沒有考慮ETL數據抽取的問題，所以數據沒有時間字段，如何在上百G的數據中抽取增量數據？\n問題3：\n面對業務部門多種需求，要求在業務獲得數據的1個小時內，將更新的業務數據傳遞到數據部門進行處理，並獲得 DATAVIEW。\n問題4：\n數據分析人員有的精通 T-SQL、有的擅長 PL/SQL，還有的只會JAVA ，你如何滿足這樣多種多樣的數據目的地需求？\n問題5：\n目前由於數據庫更新，將 ORACLE 數據庫替代，使用 PostgresQL 來代替 ORACLE。目前需要進行灰度發布(金絲雀發布)，ORACLE 和 POSTGRESQL 數據之間進行實時同步，當程序跑通，上線兩個禮拜後沒有問題，將 ORACLE 清除，如何做到？\n問題6：\n數據流是否能及時且一致的傳導到各種目的地，以進行分佈式的運算，同時數據必須在管道進行加工處理且須保證事件只能被處理一次，另外還要保留 raw data 對計算的數據進行驗證。也就是單點多傳、數據清洗、數據整理的要求，數據獲取不准確、數據提供的格式不對、數據提取對系統的負 …","date":"2021-10-07T15:32:00+00:00","updated":"2021-10-07T15:32:00+00:00"},{"objectID":"1631789520","permalink":"/post/os-kvm-virt-install-centos-vm/kvm-virt-install-centos-vm/","title":"[KVM] virt-install 建立 CentOS 虛擬機","content":"安裝 KVM 套件\r檢查 CPU 是否支援虛擬化\n1lscpu | grep -i virtualization 加入 qemu repo\n1yum install -y centos-release-qemu-ev 安裝套件\n1yum install -y qemu-kvm-ev libvirt libvirt-python libguestfs-tools virt-install 確認 kvm module 有正常載入，如果執行結果有 kvm_xxx(CPU系列）則說明 kvm 服務已經啟動\n1lsmod | grep kvm 如果未啟動，可通過如下命令啟動\n1systemctl start libvirtd 將服務設為開機啟用\n1systemctl enable libvirtd 2systemctl is-enabled libvirtd 配置 KVM 網絡橋接功能\r產生虛擬橋接器，並將實體網卡納入該 bridge 之下，使得將來 VM 可以使用 bridge 模式，具有獨立對外 IP。\n本機網路設定\n1nmcli con add type bridge ifname br0 2nmcli connection show 3 4# 10.1.5.1 為實體網卡個網路位置 5nmcli con modify bridge-br0 ipv4.method manual ipv4.address \u0026amp;#34;10.1.5.1/24\u0026amp;#34; ipv4.gateway \u0026amp;#34;10.1.5.254\u0026amp;#34; ipv4.dns \u0026amp;#34;10.1.1.3\u0026amp;#34; autoconnect yes 6 7# 把原先使用的網卡 ens1f0 加在 bridge 裡 8nmcli con add type bridge-slave ifname ens1f0 master bridge-br0 9 10# 啟用 bridge 11nmcli connection up bridge-br0 12 13# 重啟網路 14systemctl restart network 15nmcli connection show KVM 網路設定\n1# 其中 uuid 的欄位可用 uuidgen 產生 2cat \u0026amp;lt;\u0026amp;lt; EOF \u0026amp;gt; br0.xml …","date":"2021-09-16T10:52:00+00:00","updated":"2021-09-16T10:52:00+00:00"},{"objectID":"1631788200","permalink":"/post/kubernets-metallb/","title":"Change MetalLB IP Range","content":" 1# note the old IPs allocated to the services 2kubectl get svc 3 4# edit config 5kubectl edit cm -n metallb-system config 6 7# delete the metallb pods 8kubectl -n metallb-system delete pod --all 9 10# watch the pods come back up 11kubectl -n metallb-system get pods -w 12 13# inspect new IPs of services 14kubectl get svc or\n1cat \u0026lt;\u0026lt; EOF \u0026gt; new_config.yaml 2apiVersion: v1 3kind: ConfigMap 4metadata: 5 namespace: metallb-system 6 name: config 7data: 8 config: | 9 address-pools: 10 - name: default 11 protocol: layer2 12 addresses: 13 - 10.0.0.10-10.0.0.19 14EOF 15 16# delete the old configmap 17kubectl -n metallb-system delete cm config 18 19# apply the new configmap 20kubectl apply -f new_config.yaml why we need to delete all metallb pod\rMetalLB rejects new configurations if the new configuration would break existing services. Your new configuration does not allow existing services to continue existing, so MetalLB ignored it. There are log entries in the controller and speaker pods about this.\nTo force MetalLB to accept an unsafe configuration, delete all the controller and speaker pods. When they restart, they\u0026rsquo;ll accept the new configuration and change all your services. kubectl delete po -n metallb-system --all\nReference\rhttps://github.com/metallb/metallb/issues/308\rhttps://github.com/metallb/metallb/issues/348\r","date":"2021-09-16T10:30:00+00:00","updated":"2021-09-16T10:30:00+00:00"},{"objectID":"1631624520","permalink":"/post/devops-terraform-upgrade/terraform-upgrage-to-v1/","title":"[Terraform] Upgrade to the Latest Version","content":"目前的環境使用的 terraform 版本是 v0.13.5\n參考以下官網說明，如果從 0.13 版要往最新版 v1 升，需要先升到 0.14。\nCurrent Version Recommendation v0.10 or earlier Refer to the upgrade guides for these historical versions until you have upgraded to the latest v0.11 release, then refer to the following item. v0.11 Use the terraform 0.12checklist command to detect any situations that must be addressed before upgrading to v0.12, resolve them, and then upgrade to the latest v0.12 release and follow the v0.12 Upgrade Guide. v0.12 Upgrade to the latest Terraform v0.13 release and then follow the v0.13 upgrade guide to upgrade your configuration and state for explicit provider requirements. v0.13 Upgrade to the latest Terraform v0.14 release and attempt a normal Terraform run. If you encounter any new errors, refer to the v0.14 upgrade guide for resolution steps. v0.14 Upgrade directly to the latest Terraform v1.0 release and attempt a normal Terraform run. If you encounter any new errors, refer to the v0.15 upgrade guide for resolution steps. v0.15 Upgrade directly to the latest Terraform v1.0 release and attempt a normal Terraform run. Terraform v1.0 is a continuation of the v0.15 series, and so v1.0.0 and later are directly backward-compatible with Terraform v0.15.5. 開始升級\r各版 release 載點\r，請依照不同的作業系統安裝。\n1[root@master1 ~]# wget https://releases.hashicorp.com/terraform/0.14.11/terraform_0.14.11_linux_amd64.zip 2--2021-09-14 13:00:30-- https://releases.hashicorp.com/terraform/0.14.11/terraform_0.14.11_linux_amd64.zip 3Resolving releases.hashicorp.com (releases.hashicorp.com)... 151.101.1.183, 151.101.65.183, 151.101.129.183, ... 4Connecting to releases.hashicorp.com (releases.hashicorp.com)|151.101.1.183|:443... connected. 5HTTP request sent, awaiting response... 200 OK 6Length: 33789439 (32M) [application/zip] 7Saving to: ‘terraform_0.14.11_linux_amd64.zip’ 8 9100%[==============================================================================================\u0026gt;] 33,789,439 8.05MB/s in 4.0s 10 112021-09-14 13:00:34 (8.10 MB/s) - ‘terraform_0.14.11_linux_amd64.zip’ saved [33789439/33789439] 12 13[root@master1 ~]# 14[root@master1 ~]# unzip terraform_0.14.11_linux_amd64.zip 15Archive: terraform_0.14.11_linux_amd64.zip 16 inflating: terraform 17 18[root@master1 ~]# 19[root@master1 ~]# mv terraform /usr/local/bin/ 20mv: overwrite ‘/usr/local/bin/terraform’? y 21[root@master1 ~]# terraform version 22Terraform v0.14.11 23 24Your version of Terraform is out of date! The latest version 25is 1.0.6. You can update by downloading from https://www.terraform.io/downloads.html 26[root@master1 ~]# 27[root@master1 ~]# wget https://releases.hashicorp.com/terraform/1.0.6/terraform_1.0.6_linux_amd64.zip 28[root@master1 ~]# wget https://releases.hashicorp.com/terraform/1.0.6/terraform_1.0.6_linux_amd64.zip 29--2021-09-14 13:31:21-- https://releases.hashicorp.com/terraform/1.0.6/terraform_1.0.6_linux_amd64.zip 30Resolving releases.hashicorp.com (releases.hashicorp.com)... 151.101.129.183, 151.101.1.183, 151.101.65.183, ... 31Connecting to releases.hashicorp.com (releases.hashicorp.com)|151.101.129.183|:443... connected. 32HTTP request sent, awaiting response... 200 OK 33Length: 32677516 (31M) [application/zip] 34Saving to: ‘terraform_1.0.6_linux_amd64.zip’ 35 36100%[=================================================================================================================\u0026gt;] 32,677,516 11.0MB/s in 2.8s 37 382021-09-14 13:31:24 (11.0 MB/s) - ‘terraform_1.0.6_linux_amd64.zip’ saved [32677516/32677516] 39 40[root@master1 ~]# 41[root@master1 ~]# unzip terraform_1.0.6_linux_amd64.zip 42Archive: terraform_1.0.6_linux_amd64.zip 43 inflating: terraform 44 45[root@master1 ~]# 46[root@master1 ~]# mv terraform /usr/local/bin/ 47mv: overwrite ‘/usr/local/bin/terraform’? y 48[root@master1 ~]# terraform version 49Terraform v1.0.6 cannot execute binary file\r如果在安裝完後，使用 terrform cli 時，發現以下錯誤\n1[root@master1 ~]# terraform version 2-bash: /usr/local/bin/terraform: cannot execute binary file 則是因為下載到錯誤的作業系統的安裝包。\nReference\rhttps://www.terraform.io/upgrade-guides/1-0.html\r","date":"2021-09-14T13:02:00+00:00","updated":"2021-09-14T13:02:00+00:00"},{"objectID":"1631052300","permalink":"/post/devops-build-golang-and-oracle-instant-client-img/build-golang-and-oracle-instant-client-docker-image/","title":"[Docker] 建立 golang 中使用到 oracle instant client 的 image","content":"紀錄一下建立的過程中總共遇到兩個問題：\nstandard_init_linux.go:xxx: exec user process caused \u0026amp;ldquo;no such file or directory\u0026amp;rdquo;\r之前在 build golang 的時候沒有用到 C library，所以在編譯的時候 CGO 都是關閉的。但目前要 build 的這隻程式有使用到 oracle 第三方套件 godror\r需要用到 C library，所以在使用原生 alpine image 時，跑 container 起來會遇到 standard_init_linux.go:228: exec user process caused \u0026amp;quot;no such file or directory\u0026amp;quot; 的錯誤。\n概念\r靜態編譯 \u0026amp;amp; 動態編譯\r靜態編譯指的是，在編譯可執行文件的時候，將可執行文件需要調用的對應庫都集成到可執行文件內部，使得可執行文件不需要其他任何依賴就能運行。\n默認情況下，golang 的編譯是動態編譯，通過環境變量 CGO_ENABLED 控制，默認開啟，允許在 Go 代碼中調用 C 代碼。\nAlpine 鏡像\rAlpine 是眾多 Linux 發行版中的一員，和 CentOS、Ubuntu 一樣，只是一個發行版的名字，號稱小巧安全，有自己的包管理工具 apk，開發者可以使用 apk 在基於 alpine 的鏡像中添加需要的包或工具。\n相比於其他 Docker 鏡像，它的容量非常小，僅僅只有 5 MB 左右（對比 Ubuntu 系列鏡像接近 200 MB）\n錯誤原因\r然而動態編譯完成後的二進制文件，放在同是使用動態庫的 Alpine 基礎鏡像中運行會報錯：standard_init_linux.go:211: exec user process caused \u0026amp;quot;no such file or directory\u0026amp;quot;\n但是放在 ubuntu 基礎鏡像中可以運行。原因是因為兩者使用的 C library 不一樣，在製作 Alpine 的時候，是基於musl libc 和 busybox 構建的，與基於標準 C 執行庫 GNU C library (glibc) 上編譯出來的應用程序不兼容，導致動態依賴的二進制文件在運行時找不到依賴 …","date":"2021-09-07T22:05:00+00:00","updated":"2021-09-07T22:05:00+00:00"},{"objectID":"1630880220","permalink":"/post/kubernets-minikube/","title":"Minikube","content":"minikube 是一個由 Google 發布的部署單節點的 Kubernetes Cluster 的工具，可以安裝在本機上，支援 Windows 與 Mac Minikube 只有一個 Node (節點)。對於本地實驗可以避免節點不足的困擾；讓開發者可以在本機上輕易架設一個 Kubernetes Cluster，快速上手 Kubernetes 的指令與環境。\n運作原理就是會在本機上建立一個 virtual machine，並且在這 VM 建立一個 signle-node Kubernetes Cluster。\nminikube 適合用於開發環境測試，不會把它用在實際生產環境中。\n下載與部屬\rMinikube 支援 Windows、MacOS、Linux，在這三種平台的本機端都可以安裝並執行 Minikube 。安裝及執行步驟，請參考\r官網\r。\n整體步驟如下：\n安裝Virtualization Software，如 VirtualBox\r安裝 kubectl\r套件，用以和 K8S 集群交互溝通 從 Github\r下載 Minikube 套件 啟動 minikube 及 K8s 集群 使用 kubectl 操作集群及應用 官網跟其他教學文寫得很詳細，在這裡就不一一列示了。\nReference\rhttps://ithelp.ithome.com.tw/articles/10192490\r","date":"2021-09-05T22:17:00+00:00","updated":"2021-09-05T22:17:00+00:00"},{"objectID":"1630681980","permalink":"/post/data-nifi-consume-kafka-and-put-to-mongodb/nifi-consume-kafka/","title":"[nifi] Consume Kafka Topic and Put to MongoDB","content":"這個範例將示範消費指定的 kafka topic，並寫進指定的 MongoDB。\nkafka 的資料源為 JSON 結構的 string，如下：\n1\u0026#34;{\u0026#34;Time\u0026#34;:1630652601.119,\u0026#34;SMT_B_3_machine_name\u0026#34;:null,\u0026#34;SMT_B_3_SN\u0026#34;:null,\u0026#34;SMT_B_3_program_number\u0026#34;:null,\u0026#34;SMT_B_3_WO\u0026#34;:null,\u0026#34;SMT_B_3_whole_OK\u0026#34;:null,\u0026#34;SMT_B_3_whole_NG\u0026#34;:null,\u0026#34;SMT_B_3_whole_reOK\u0026#34;:null,\u0026#34;SMT_B_3_whole_yieldRate\u0026#34;:null,\u0026#34;SMT_B_3_board_OK\u0026#34;:null,\u0026#34;SMT_B_3_board_NG\u0026#34;:null,\u0026#34;SMT_B_3_board_reOK\u0026#34;:null,\u0026#34;SMT_B_3_board_yieldRate\u0026#34;:null,\u0026#34;SMT_B_3_component_OK\u0026#34;:null,\u0026#34;SMT_B_3_component_NG\u0026#34;:null,\u0026#34;SMT_B_3_component_reOK\u0026#34;:null,\u0026#34;SMT_B_3_component_yieldRate\u0026#34;:null,\u0026#34;SMT_B_3_tin_OK\u0026#34;:null,\u0026#34;SMT_B_3_tin_NG\u0026#34;:null,\u0026#34;SMT_B_3_tin_reOK\u0026#34;:null,\u0026#34;SMT_B_3_tin_yieldRate\u0026#34;:null}\u0026#34; 1. ConsumeKafka_2_6\r2. PutMongo\r3. 建立關係\ra. ConsumeKafka —\u0026gt; PutMongo\rb. 建立完成後可以發現最後一個 PutMongo 的 processor 有報錯\rc. 進入 PutMongo processor 的 SETTINGS 將 Automatically Terminate Relationships 的關係打開\rInfo\nAutomatically Terminate Relationships 指的是數據流路由到這個 Processor 後，特定狀態下會被刪除，一般在 Endpoint Processor 配置，因為數據流不需要再被繼續路由了。\n4. 啟動流程\r都完成後可以看到 flow 的原件都已經 Ready，將 Processor 依次啟動。\n啟動後可以看到數據開始收送，\n另外通過 NiFi Data Provenance 可以看到數據流的狀態\nReference\rhttps://anyisalin.github.io/2019/01/03/nifi-demo/\rhttps://cloud.tencent.com/developer/article/1416651\r","date":"2021-09-03T15:13:00+00:00","updated":"2021-09-03T15:13:00+00:00"},{"objectID":"1630444500","permalink":"/post/kubernets-scheduler-controller-manager-unhealthy/","title":"解決 scheduler and controller-manager unhealthy state","content":"Problem\r在嘗試更新 Kubernetes 時，下了下面的 command 取得目前集群的組件狀態：\n1$ kubectl get cs 發現 controller-mamager 和 scheduler 有 unhealthy 的狀態：\n1NAME STATUS MESSAGE ERROR 2controller-manager Unhealthy Get http://127.0.0.1:10252/healthz: dial tcp 127.0.0.1:10252: connect: connection refused 3scheduler Unhealthy Get http://127.0.0.1:10251/healthz: dial tcp 127.0.0.1:10251: connect: connection refused 4etcd-0 Healthy {\u0026#34;health\u0026#34;:\u0026#34;true\u0026#34; Reason\r這兩個 pod 的非安全端口沒有開啟，健康檢查時報錯，但是由於本身服務是正常的，只是健康檢查的端口沒啟，所以不影響正常使用。\nSolution\r在所有 Master nodes 上修改下面檔案:\n1$ vim /etc/kubernetes/manifests/kube-scheduler.yaml 2$ vim /etc/kubernetes/manifests/kube-controller-manager.yaml 刪掉或註釋掉 - --port=0 (spec-\u0026gt;containers-\u0026gt;command) 這行\n1$ sudo vi /etc/kubernetes/manifests/kube-controller-manager.yaml 重啟 kubelet 服務\n1$ sudo systemctl restart kubelet.service 這時10251，10252端口就開啟了，健康檢查狀態也正常了。\n1[root@master1 ~]# netstat -tulpn | grep \u0026#39;10251\\|10252\u0026#39; 2tcp6 0 0 :::10251 :::* LISTEN 11863/kube-schedule 3tcp6 0 0 :::10252 :::* LISTEN 11902/kube-controll Reference\rhttps://www.cnblogs.com/wuliping/p/13780147.html\r","date":"2021-08-31T21:15:00+00:00","updated":"2021-08-31T21:15:00+00:00"},{"objectID":"1630444140","permalink":"/post/devops-docker-build-network-error/docker-build-network-error/","title":"[Docker] docker build network error","content":"在 build docker image 時發生 network error 的錯誤\nDockerfile\r1FROM golang:1.15.3-alpine3.12 AS builder 2WORKDIR / 3COPY . . 4 5RUN apk update \u0026amp;\u0026amp; apk add --update git 6RUN CGO_ENABLED=0 go build -installsuffix cgo -o /it-preprocess-adapter ./cmd/it-preprocess-adapter/it-preprocess-adapter.go 7 8FROM alpine:3.12 9COPY --from=builder /it-preprocess-adapter /it-preprocess-adapter 10COPY ./configs /configs 11COPY ./settings /settings 12COPY ./build/docker/startup.sh /startup.sh 13 14RUN chmod 2777 -R /settings 15#USER 1001 16 17CMD [\u0026#34;sh\u0026#34;, \u0026#34;/startup.sh\u0026#34;] Docker build error\rSolution\r重啟 docker 就解決了\n1systemctl restart docker Reference\rhttps://github.com/laradock/laradock/issues/2551\r","date":"2021-08-31T21:09:00+00:00","updated":"2021-08-31T21:09:00+00:00"},{"objectID":"1630273620","permalink":"/post/kubernets-nfs-client-provisioner-pending-in-creating-pvc/","title":"k8s v1.20 nfs-client-provisioner 創建 pvc 時停在 Pending","content":"上次將 K8s 集群從 1.7 升級到 1.20 之後，在創建 pvc 時，發現狀態會一直停留在 Pending，詳細資訊如下：\n1[root@master1 telegraf]# kubectl get pvc 2NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE 3test Pending nfs 5s 4[root@master1 telegraf]# kubectl describe pvc test 5Name: test 6Namespace: default 7StorageClass: nfs 8Status: Pending 9Volume: 10Labels: \u0026lt;none\u0026gt; 11Annotations: volume.beta.kubernetes.io/storage-provisioner: cluster.local/nfs-sc-0-nfs-client-provisioner 12Finalizers: [kubernetes.io/pvc-protection] 13Capacity: 14Access Modes: 15VolumeMode: Filesystem 16Used By: \u0026lt;none\u0026gt; 17Events: 18 Type Reason Age From Message 19 ---- ------ ---- ---- ------- 20 Normal ExternalProvisioning 10s (x5 over 50s) persistentvolume-controller waiting for a volume to be created, either by external provisioner \u0026#34;cluster.local/nfs-sc-0-nfs-client-provisioner\u0026#34; or manually created by system administrator 查了原因可能是因為 selfLink 為空，無法建立參考。 Kubernetes v1.20 開始，根據 change log\r說明，默認刪除了 metadata.selfLink 字段，然而部分應用仍然依賴於此功能，例如 nfs-client-provisioner。如果仍然要繼續使用這些應用，需要重新啟用selfLink。\n解決方式\r修改 kube-apiserver 文件\n1vim /etc/kubernetes/manifests/kube-apiserver.yaml 在 command 下的參數加上 --feature-gates=RemoveSelfLink=false\n1spec: 2 containers: 3 - command: 4 - kube-apiserver 5 - --advertise-address=10.1.5.141 6 - --allow-privileged=true 7 - --au... 8 . 9 . 10 . 11 **- --feature-gates=RemoveSelfLink=false** 儲存編輯後，kube-apiserver 便會自動重啟。\n","date":"2021-08-29T21:47:00+00:00","updated":"2021-08-29T21:47:00+00:00"},{"objectID":"1630273200","permalink":"/post/devops-terraform-connection-refused-error/terraform-connection-refused-error/","title":"Terraform Connection to Kuberentes Refused Error","content":"問題\r在 terrform apply 的時候一直卡在 Error: Post \u0026quot;http://localhost/api/v1/namespaces\u0026quot;: dial tcp [::1]:80: connect: connection refused 的錯誤\n原因\r看起來是沒有正確的抓到 Provider 裡面設的 kube config file\n原本的腳本如下\n1// main.tf 2provider \u0026#34;helm\u0026#34; { 3 kubernetes { 4 config_path = \u0026#34;/root/.kube/config\u0026#34; 5 } 6} 1// kubernetes.tf 2provider \u0026#34;kubernetes\u0026#34; {} 解法\r修改腳本如下\n1// main.tf 2provider \u0026#34;kubernetes\u0026#34; { 3 host = \u0026#34;https://10.1.5.140:8443\u0026#34; 4 5 client_certificate = file(\u0026#34;~/.kube/client.pem\u0026#34;) 6 client_key = file(\u0026#34;~/.kube/client-key.pem\u0026#34;) 7 cluster_ca_certificate = file(\u0026#34;~/.kube/ca.pem\u0026#34;) 8} 1// kubernetes.tf 2// 只能有一個 provider，否則會有 Error: Duplicate provider configuration 的錯誤 重新佈署後錯誤碼就莫名其妙變了:\n1[root@master1 terraform]# terraform apply 2helm_release.metrics-server[0]: Refreshing state... [id=metrics-server] 3 4Error: Kubernetes cluster unreachable: invalid configuration: no configuration has been provided, try setting KUBERNETES_MASTER environment variable 照指示加上 環境變數\n1export KUBE_CONFIG_PATH=/root/.kube/config 就莫名其妙成功了 = =\n補充:\rk8s provider 說明 https://registry.terraform.io/providers/hashicorp/kubernetes/latest/docs#credentials-config\rk8s cluster ca https://blog.csdn.net/ll837448792/article/details/103658502\r配置 TLS 連接\r查看 kubectl 配置文件，裡面記錄了三個證書和 API server 的地址： 1[root@testm ~]# cat .kube/config 2apiVersion: v1 3clusters: 4- cluster: 5 certificate-authority-data: LS0tLS1CR...... 6 server: https://10.1.5.145:8443 7 name: kubernetes 8contexts: 9- context: 10 cluster: kubernetes 11 user: kubernetes-admin 12 name: kubernetes-admin@kubernetes 13current-context: kubernetes-admin@kubernetes 14kind: Config 15preferences: {} 16users: 17- name: kubernetes-admin 18 user: 19 client-certificate-data: LS0tLS1CR...... 20 client-key-data: LS0tLS1CRUdJTiBSU0...... 匯出金鑰及證書 1[root@testm .kube]# export clientcert=$(grep client-cert ~/.kube/config | cut -d\u0026#34; \u0026#34; -f 6) 2[root@testm .kube]# echo $clientcert 3LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0=...... 4[root@testm .kube]# export clientkey=$(grep client-key-data ~/.kube/config | cut -d\u0026#34; \u0026#34; -f 6) 5[root@testm .kube]# echo $clientkey 6LS0tLS1CRUdJTiBSU0EgUFJJVkFURSB...... 7[root@testm .kube]# export ca=$(grep certificate-authority-data ~/.kube/config | cut -d\u0026#34; \u0026#34; -f 6) 8[root@testm .kube]# echo $ca 9LS0tLS1CRUdJTiBDRVJUSUZJQ0FU...... 10[root@testm .kube]# echo $clientcert | base64 -d \u0026gt; ./client.pem 11[root@testm .kube]# echo $clientkey | base64 -d \u0026gt; ./client-key.pem 12[root@testm .kube]# echo $ca | base64 -d \u0026gt; ./ca.pem 從配置文件中讀取server 地址： 1[root@testm ~]# kubectl config view | grep server 2 server: https://10.1.5.145:8443 使用 curl 和剛剛加密的密鑰文件來訪問 API server： 1curl --cert ./client.pem --key ./client-key.pem --cacert ./ca.pem https://10.1.5.145:8443/api/v1/pods 2 3{ 4 \u0026#34;kind\u0026#34;: \u0026#34;PodList\u0026#34;, 5 \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, 6 \u0026#34;metadata\u0026#34;: { 7 \u0026#34;resourceVersion\u0026#34;: \u0026#34;387199\u0026#34; 8 }, 9 \u0026#34;items\u0026#34;: [ 10 { 11 \u0026#34;metadata\u0026#34;: { 12 \u0026#34;name\u0026#34;: \u0026#34;ingress-nginx-controller-5c5bf8c854-7pcf7\u0026#34;, 13 \u0026#34;generateName\u0026#34;: \u0026#34;ingress-nginx-controller-5c5bf8c854-\u0026#34;, 14 \u0026#34;namespace\u0026#34;: \u0026#34;ingress-nginx\u0026#34;, 15 \u0026#34;uid\u0026#34;: \u0026#34;f84b09e4-7d7d-40bf-ae66-f2eb72ab7a59\u0026#34;, 16 \u0026#34;resourceVersion\u0026#34;: \u0026#34;104900\u0026#34;, 17 \u0026#34;creationTimestamp\u0026#34;: \u0026#34;2022-02-09T05:42:28Z\u0026#34;, 18. 19. 20. ","date":"2021-08-29T21:40:00+00:00","updated":"2021-08-29T21:40:00+00:00"},{"objectID":"1629667740","permalink":"/post/k8s-upgrade/k8s-upgrade/","title":"Kubernetes 升級紀錄","content":"紀錄在現有 kubernetes 1.17 集群升級到 1.20 的過程。\n環境說明\r集群配置 主機名 系統 IP Address vip Server Load Balancer (SLB) 10.1.5.140 master1 CentOS 7.8 10.1.5.141 Worker1 CentOS 7.8 10.1.5.142 當前運行版本 組件 版本 kubeadm v1.17.13 kubelet v1.17.13 kubectl v1.17.13 Container Runtime - Docker v20.10.7 etcd V3.4.3-0 kube-apiserver v1.17.17 kube-controller-manager v1.17.17 kube-proxy v1.17.17 kube-scheduler v1.17.17 coredns v1.6.5 pause v3.1 查看版本的命令如下\n1kubeadm version 2kubelet --version 3kubectl version 4kubectl get node master1 -o yaml Kubernetes Non-Active Branch History\r目前使用的 1.17 版已在 2021-01-13 EOL，final patch release 在 1.17.17，請參考官方 repo\r說明。\n準備升級\r版本升級的必要\r對於 Kubernetes 集群的使用者： 更新的版本能有更新的功能、更加全面的安全補丁以及諸多的bugfix。\n對於 Kubernetes 集群的運維者： 通過集群升級功能可以拉齊所管理的集群版本，減少集群版本的碎片化，從而減少管理成本和維護成本。\n升級注意事項\r升級僅支持一個小版本號。也就是說，只能從 1.7 升級到 1.8 的最新版本，或是從 1.17.13 升級到 1.17.17，而不能從 1.7 直接升級到 1.9。 一次更新一個節點，確保 Kubernetes 功能不會因為更新而中斷。 升級後所有容器都會被重啟，避免服務中斷，需確保應用程式使用進階的 Kubernetes API 建立，如 Deployment，或是利用副本機制。 kubeadm upgrade 不會影響工作負載，只會涉及Kubernetes 內部的 …","date":"2021-08-22T21:29:00+00:00","updated":"2021-08-22T21:29:00+00:00"},{"objectID":"1610726040","permalink":"/post/programming-python-system-performance-monitor/monitor-system-perf-by-python/","title":"[Python] 系統效能監測","content":"監測的指標有 CPU 使用率\u0026amp;頻率、已使用的記憶體、磁碟讀寫 bytes 數、網路收發 bytes 數。會將每秒讀取到的數值存為 csv 檔。\nPrerequisite\rPython3.5+ psutil code\r1import psutil 2import time 3from collections import namedtuple 4 5################################################################################ 6class SystemMonitor(object): 7 running = True 8 CPU = namedtuple(\u0026#39;cpu\u0026#39;, [\u0026#39;percentage\u0026#39;, \u0026#39;freq\u0026#39;]) 9 NET = namedtuple(\u0026#39;net\u0026#39;, [\u0026#39;rx\u0026#39;, \u0026#39;tx\u0026#39;]) 10 DISK = namedtuple(\u0026#39;disk\u0026#39;, [\u0026#39;read\u0026#39;, \u0026#39;write\u0026#39;]) 11 SysMon = namedtuple(\u0026#39;sysmon\u0026#39;, [\u0026#39;timestamp\u0026#39;, \u0026#39;cpu\u0026#39;, \u0026#39;vm\u0026#39;, \u0026#39;disk\u0026#39;, \u0026#39;net\u0026#39;]) 12 13 def __init__(self, csvfile=\u0026#39;./sysmon.csv\u0026#39;, interval=1.0): 14 self.interval = interval 15 self.csvfile = csvfile 16 17 def run(self): 18 self.info() 19 with open(self.csvfile, \u0026#39;w\u0026#39;) as fp: 20 fp.write(\u0026#39;timestamp,percentage,freq,vm,disk_rd,disk_wr,net_rx,\u0026#39; 21 \u0026#39;net_tx\\n\u0026#39;) 22 23 while self.running: 24 time.sleep(self.interval) 25 info = self.info() 26 #print(info.timestamp) 27 info = [ 28 info.timestamp, 29 info.cpu.percentage, 30 info.cpu.freq, 31 info.vm, 32 info.disk.read, 33 info.disk.write, 34 info.net.rx, 35 info.net.tx, 36 ] 37 info = \u0026#39;,\u0026#39;.join([str(x) for x in info]) + \u0026#39;\\n\u0026#39; 38 with open(self.csvfile, \u0026#39;a+\u0026#39;) as fp: 39 fp.write(info) 40 41 def info(self): 42 ts = int(time.time()) 43 cpu = self.CPU(psutil.cpu_percent(), psutil.cpu_freq().current) 44 vm = psutil.virtual_memory().used 45 diskio = psutil.disk_io_counters() 46 netio = psutil.net_io_counters() 47 48 result = self.SysMon( 49 ts, cpu, vm, 50 self.DISK(diskio.read_bytes, diskio.write_bytes), 51 self.NET(netio.bytes_recv, netio.bytes_sent)) 52 return result 53 54################################################################################ 55if __name__ == \u0026#39;__main__\u0026#39;: 56 m = SystemMonitor() 57 m.run() ","date":"2021-01-15T15:54:00+00:00","updated":"2021-01-15T15:54:00+00:00"},{"objectID":"1610725860","permalink":"/post/programming-python-install-python39-on-centos7/install-python39-on-centos7/","title":"[Python] Install Python3.9 on CentOS7","content":"step1\r下載依賴工具以及安裝包\n1$ yum install zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gcc make libffi-devel wget 2$ wget https://www.python.org/ftp/python/3.9.1/Python-3.9.1.tgz 3$ tar zxvf Python-3.9.1.tgz step2\r編譯 Python\n1$ cd Python-3.9.1 2# 檢測並產生 Makefile，且指定安裝目錄 3$ ./configure prefix=/usr/local/python3 4# 開始編譯 5$ make \u0026amp;\u0026amp; make install step3\r因為想直接用 python command 代表使用 python3，所以新增別名\n1$ vi ~/.bashrc 1alias python=\u0026#39;/usr/local/python3/bin/python3\u0026#39; 2alias pip=\u0026#39;/usr/local/python3/bin/pip3\u0026#39; 1$ source ~/.bashrc step4\r確認安裝成功\n1$ python -V 2$ pip -V 補充 (virtualenv)\r如果想保留系統預設的 python 2.7，則不要新增上面第三步的別名，相反的加上 python3 執行路徑的環境變數。\n1$ vi ~/.bashrc 1export python3=\u0026#39;/usr/local/python3/bin\u0026#39; 1$ source ~/.bashrc 為了區隔兩個 python 的版本，可以使用 virtualenv 隔離不同的開發環境。\n1$ pip3 install virtualenv 2$ python3 -m virtualenv project1 3$ cd project1 4$ source bin/activate 5$ python --version 6Python 3.9.1 Info\n環境變數設定小補充 /etc/profile \u0026ndash;\u0026gt; 對所有用戶永久生效 ~/.bashrc \u0026ndash;\u0026gt; 對單一用戶永久生效，當你\u0026quot;啟動\u0026quot; shell 時執行 ~/.bash_profile \u0026ndash;\u0026gt; 對單一用戶永久生效，當你\u0026quot;登入\u0026quot; shell 時執行 export xxx = xxx \u0026ndash;\u0026gt;直接運行 export 命令定義變量，只對當前 shell 有效，關閉shell 終端後失效。\nRefernce\rhttps://blog.jiebu-lang.com/centos-7-install-python-3-7/\rhttps://liqiang.io/post/install-python3-8-in-centos-973bdb81\rhttps://blog.csdn.net/qq_36758461/article/details/103841798\r","date":"2021-01-15T15:51:00+00:00","updated":"2021-01-15T15:51:00+00:00"},{"objectID":"1610115960","permalink":"/post/kubernets-configmap/","title":"ConfigMap 建立及掛載","content":"ConfigMap\rConfigMap 以 key-vaule 的方式用來描述系統相關設定，所有與應用程式相關的非敏感性未加密的資訊可放在 ConfigMap 內。而如有敏感性資料，則需透過 Secret。\n主要目的\r主要目的是將應用程式與設定解耦，ConfigMap 與 Pod 將個別單獨存在於 k8s 叢集中，當 Pod 需要使用 ConfigMap 時才需要將 ConfigMap 掛載到 Pod 內使用。解耦的好處有：\n便於管理 彈性高，可掛載不同的 ConfigMap 到 Pod 內使用；或是同一個 ConfigMap 掛載到多個 Pod。 用法\rKubernetes 的 ConfigMap 透過 kubectl create 或 kubectl apply 來建立。\n1$ kubectl create configmap [資源名字] [來源參數] 2$ kubectl apply condfigmap.yaml 使用 kubectl create 建立\r使用 kubectl create 可以從檔案路徑、檔案或是 literal value 來建立 configMap。\n\u0026ndash;from-file 1# 建立名為 myConf、資料來源是某路徑下所有檔案的 configMap 2$ kubectl create configmap myConf --from-file=/path/for/config/file/ 3 4# 建立名為 myConf、資料來源是一個檔案的 configMap 5$ kubectl create configmap myConf --from-file=/path/to/app.properties 6 7# 建立名為 myConf、資料來源是多個檔案的 configMap 8$ kubectl create configmap myConf --from-file=/path/of/app1.properties --from-file=/path/of/app2.properties Note\n如果是來源是檔案的話，則 configMap 中的 key 就會是檔名，value 則是檔案內容。\n\u0026ndash;from-literal 1# 建立名為 myConf、包含指定鍵值對的 configMap 2$ kubectl create configmap myConf --from-literal=key1=config1 3 4$ kubectl create configmap myConf --from-literal=key1=config1 --from-literal=key2=config2 兩個共用 1$ kubectl create configmap myConf --from-file=/path/of/config.conf \\ 2 --from-literal=key1=config1 \\ 3 --from-literal=key2=config2 \u0026ndash;from-env-file 使用環境變數表示的檔案。 Warning\n請注意，value 如果有 \u0026quot;\u0026quot; 則會視為是值的一部份。且如果在同個 create 中使用多個 \u0026ndash;from-env-file 則指會應用最後一個。\n使用 kubectl apply yaml 檔案建立\r準備 yaml 檔\n1--- 2apiVersion: v1 3kind: ConfigMap 4metadata: 5 name: myConf 6data: 7 key1: config1 8 key2: config2 9 app.properties: | 10 property.1 = value1 11 property.2 = value2 12 property.3 = value3 佈署\n1$ kubectl apply -f configmap.yaml 查看 ConfigMap\r建立完成後可以使用 kubectl get 或是 kubectl describe 的擷取 configMap 的內容。\n1$ kubectl get configmaps myConf -o yaml 1apiVersion: v1 2kind: ConfigMap 3metadata: 4 creationTimestamp: 2021-01-04T18:52:05Z 5 name: myConf 6 namespace: default 7 resourceVersion: \u0026#34;516\u0026#34; 8 uid: b4952dc3-d670-11e5-8cd0-68f728db1985 9data: 10 key1: config1 11 key2: config2 12 app.properties: | 13 property.1 = value1 14 property.2 = value2 15 property.3 = value3 將 ConfigMap 掛載到 pod 使用\r當成環境變數使用\rpod 的 yaml 檔如下\n1apiVersion: v1 2kind: Pod 3metadata: 4 name: testenv 5spec: 6 containers: 7 - name: test 8 image: tomcat:8 9 imagePullPolicy: IfNotPresent 10 command: [ \u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;echo $(KEY1_ENV)\u0026#34; ] 11 env: 12 - name: KEY1_ENV 13 valueFrom: 14 configMapKeyRef: 15 name: myConf 16 key: key1 將 pod 跑起來後，ConfigMap myConf 中的 key1 的 value 就會做為環境變數 KEY1_ENV 的值。\n掛載成 volume\rpod 的 yaml 檔如下\n1apiVersion: v1 2kind: Pod 3metadata: 4 name: testvolume 5spec: 6 containers: 7 - name: test 8 image: tomcat: 8 9 imagePullPolicy: IfNotPresent 10 command: [ \u0026#34;/bin/sh\u0026#34;,\u0026#34;-c\u0026#34;,\u0026#34;cat /etc/config/keys\u0026#34; ] 11 volumeMounts: 12 - name: config-volume 13 mountPath: /etc/config 14 volumes: 15 - name: config-volume 16 configMap: 17 name: myConf Note\n使用 volume 將 ConfigMap 作為文件或目錄直接掛載，ConfigMap 中每一個 key-value 鍵值對都會生成一個文件，key 為文件名，value 為內容。\n另一種方式，只掛載某個 key，並指定相對路徑。\n1apiVersion: v1 2kind: Pod 3metadata: 4 name: testvolume 5spec: 6 containers: 7 - name: test 8 image: tomcat:8 9 imagePullPolicy: IfNotPresent 10 volumeMounts: 11 - name: config-volume 12 mountPath: /etc/config 13 volumes: 14 - name: config-volume 15 configMap: 16 name: myConf 17 items: 18 - key: key1 19 path: /path/to/key1 # key1 會放在 mountPath /etc/config/path/to 下。 20 - key: app.properties 21 path: app.properties # 如果 path 與 key 相同，則會直接把 app.properties 文件放在 mountPath 下。 Reference\rhttps://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/\rhttps://www.cnblogs.com/pu20065226/p/10690628.html\r","date":"2021-01-08T14:26:00+00:00","updated":"2021-01-08T14:26:00+00:00"},{"objectID":"1610041080","permalink":"/post/kubernets-object/","title":"Kubernetes Object 基本對象介紹","content":"K8s Obejcts\r常用的基本 objects\rPod\nPod 有兩種類型：普通 Pod 和靜態 Pod (static pod)。靜態 Pod 即不通過 K8S 調度和創建，直接在某個具體的 Node 機器上通過具體的文件來啟動。普通 Pod 則是由 K8S 創建、調度，同時數據存放在 etcd 中。 Service\n可以認為是 pod 的反向代理，負責接收客戶端請求，把請求轉給 pod。因為每個 pod 都有自己的內部 ip，但是 deployment 的 pod 的 ip 是有可能變的 (pod 掛掉或複製)，所以需要 service 來做類似中間者的抽象存在。Service 挑選、關聯 Pod 的方式為基於 Label Selector 進行定義。通過 type 在 ServiceSpec中指定可以以不同的方式公開服務： ClusterIP (default)：只有內部 IP，只能從集群內訪問服務。 NodePort：工作於每個節點的主機 IP 之上，可以從集群外部訪問服務。ClusterIP 的超集。 LoadBalancer：在當前集群中創建一個外部負載平衡器，把外部請求負載均衡至多個Node 主機 IP 的 NodePort 之上，為該服務分配一個固定的外部 IP。NodePort 的超集。 ExternalName-externalName：通過在 Cluster 的 DNS Server 添加一筆 CName Record，使用指定名稱（在 yaml 中設定）公開服務，不使用代理 (kube-proxy)，而是透過 kube-dns。此類型在 kubernetes 1.7 版本有提供，但是要 kube-dns version 要在 1.14.9 以上，否則會遇到 Resolve External Name issue。主要是為了讓不同 namespace 中的 Service 可以利用 ExternalName 設定的外部名稱連到其它的 namespace 中的 Service。 Label\n標籤用於區分對象，使用標籤引用對象而不再是 IP。Label 以鍵值對的形式存在，每個對象可以有多個標籤，通過標籤可以關聯對象。 Volume\n共享 Pod 中使用的數據。 Namespace\n可以抽象理解為一群對象的集合。 High-level …","date":"2021-01-07T17:38:00+00:00","updated":"2021-01-07T17:38:00+00:00"},{"objectID":"1609349460","permalink":"/post/data-rabbitmq-installation-on-k8s/rabbitmq-on-k8s/","title":"[RabbitMQ] 在 K8s cluster 上安裝叢集","content":"先決條件\rkubernetes 1.17 版以上 RabbitMQ image 3.8.8+ 安裝 operator\r1$ kubectl apply -f \u0026#34;https://github.com/rabbitmq/cluster-operator/releases/latest/download/cluster-operator.yml\u0026#34; 2namespace/rabbitmq-system created 3customresourcedefinition.apiextensions.k8s.io/rabbitmqclusters.rabbitmq.com created 4serviceaccount/rabbitmq-cluster-operator created 5role.rbac.authorization.k8s.io/rabbitmq-cluster-leader-election-role created 6clusterrole.rbac.authorization.k8s.io/rabbitmq-cluster-operator-role created 7rolebinding.rbac.authorization.k8s.io/rabbitmq-cluster-leader-election-rolebinding created 8clusterrolebinding.rbac.authorization.k8s.io/rabbitmq-cluster-operator-rolebinding created 9deployment.apps/rabbitmq-cluster-operator created 確認 CRD 部屬完成\n1$ kubectl get customresourcedefinitions.apiextensions.k8s.io | grep rabbitmq 2rabbitmqclusters.rabbitmq.com 2020-12-29T06:22:27Z 安裝 RabbitMQ Cluster\rRabbitMQ server 透過 RabbitmqCluster 資源來建立，整個集群資源(如 pod, svc, statefulSet)都會建在同一個指定 namespace 下。\n準備 yaml 檔\r準備一個 yaml 檔來定義 RabbitmqCluster 資源：\n1$ mkdir rbmq \u0026amp;\u0026amp; cd rbmq \u0026amp;\u0026amp; touch rbmq.yaml 2$ vim rbmq.yaml 定義了 cluster name 為 rbmq、指定放在 rabbitmq namespace 下，複本數指定為 3、service 類型為 LoadBalancer (前提是 k8s 集群有建置 LB 套件)、指定 storageClass 以及空間。\n1apiVersion: rabbitmq.com/v1beta1 2kind: RabbitmqCluster 3metadata: 4 name: rbmq 5 namespace: rabbitmq 6spec: 7 replicas: 3 8 service: 9 type: LoadBalancer 10 persistence: 11 storageClassName: standard 12 storage: 20Gi Warning\npersistence 可以不寫，但 k8s 集群必須事先設置 default 的 StorageClass，若無填寫則會使用 default storageClass。否則集群將不會被 scheduled 成功，因為 rabbitMQ 需要有 persistent volume。\np.s. 使用 $ kubectl get sc 取得目前 k8s 集群的 StorageClass 資源\n開始建立\r應用上一步驟定義好的 RabbitmqCluster 資源，如果不使用與 operator 相同的 namespace 的話，需要先自己創一個。\n1$ kubectl create ns rabbitmq 2$ kubectl apply -f rbmq.yaml 驗證資源是否都有建立成功\n1$ kubectl get all -n rabbitmq 加入 TLS\r建立 Secret\r需先準備好一組含有 key 跟 cert 的 pem 格式的金鑰，如果沒有就按照下面指令創建。\n1$ openssl req -x509 -newkey rsa:2048 -sha256 -nodes -keyout key.pem -out cert.pem -days 3650 使用現有的金鑰做成 secret\n1$ kubectl create secret -n rabbitmq tls rbmq-tls-secret --cert=/root/cert.pem --key=/root/key.pem 修改配置\r1$ kubectl edit rabbitmqcluster rbmq 在 spec.tls.secretName 中加上剛剛的 secret。\n1spec: 2 tls: 3 secretName: rbmq-tls-secret 保存後離開就可以看到集群上已經開啟了 tls。\n進入 Rabbit Management\r開啟瀏覽器，輸入 LoadBalancer 配置的 ip:15672 進入 Rabbit Management。\n取得使用者帳密\radmin user 的帳密儲存在 secret 中，資源名字為 cluster name 加上 -default-user，帳密皆以 base44 編碼儲存。使用以下命令取得帳號以及密碼：\n1$ kubectl -n rabbitmq get secret rbmq-default-user -o jsonpath=\u0026#34;{.data.username}\u0026#34; | base64 --decode 2$ kubectl -n rabbitmq get secret rbmq-default-user -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 --decode Reference\rhttps://www.rabbitmq.com/kubernetes/operator\r","date":"2020-12-30T17:31:00+00:00","updated":"2020-12-30T17:31:00+00:00"},{"objectID":"1608494820","permalink":"/post/database-mongodb-cannot-connect-with-replicaset/mongodb-replicaset-connection-failed/","title":"[MongoDB] 無法使用 replicaSet 連線問題","content":"前言\r用 docker 跑三個 mongoDB 的 container，做成 replica set cluster。但卻遇到從外面的 shell (跑在本機或其他 server) 加上 replica set 的名字時無法連線的問題 (mongo --host myReplica/mongo1 失敗)。而不加 replica set 卻都能成功連線 (mongo --host mongo1:30001 成功)。\n原因\r外部連線的 hostname 跟 port 與 mongodb 內部的 replica set configuration 不同\n解決辦法\r參考 這篇 issue\r後發現有兩種做法，一種是在建立 container 的時候為每個角色指定環境變數 (如 MONGODB_REPLICA_SET_MODE=primary)。但這個做法總覺得哪裡怪怪的，不太確定在環境變數指定 primary 或 secondary 後，當 primary 掛掉時環境變數固定會不會造成什麼影響。\n所以我就用第二種方法，把 container 內的 mongod 的 port 與外面 export 的 port 設成一樣的。然後連線前在 /etc/hosts 中設定對應的 hostname。\n1docker run \\ 2-p 30001:30001 \\ 3--name mongo1 \\ 4--network mongo-cluster \\ 5mongo mongod --replSet myReplica --port 30001 這樣子 rs.conf().members 的 ip \u0026amp; port 才和外面會一樣。\n1[root@test ~]# mongo --host myReplica/mongo1:30001,mongo2:30002,mongo3:30003 2myReplica:PRIMARY\u0026gt; rs.conf().members 3[ 4 { 5 \u0026#34;_id\u0026#34; : 0, 6 \u0026#34;host\u0026#34; : \u0026#34;mongo1:30001\u0026#34;, 7 \u0026#34;arbiterOnly\u0026#34; : false, 8 \u0026#34;buildIndexes\u0026#34; : true, 9 \u0026#34;hidden\u0026#34; : false, 10 \u0026#34;priority\u0026#34; : 1, 11 \u0026#34;tags\u0026#34; : { 12 13 }, 14 \u0026#34;slaveDelay\u0026#34; : NumberLong(0), 15 \u0026#34;votes\u0026#34; : 1 16 }, 17 { 18 \u0026#34;_id\u0026#34; : 1, 19 \u0026#34;host\u0026#34; : \u0026#34;mongo2:30002\u0026#34;, 20 \u0026#34;arbiterOnly\u0026#34; : false, 21 \u0026#34;buildIndexes\u0026#34; : true, 22 \u0026#34;hidden\u0026#34; : false, 23 \u0026#34;priority\u0026#34; : 1, 24 \u0026#34;tags\u0026#34; : { 25 26 }, 27 \u0026#34;slaveDelay\u0026#34; : NumberLong(0), 28 \u0026#34;votes\u0026#34; : 1 29 }, 30 { 31 \u0026#34;_id\u0026#34; : 2, 32 \u0026#34;host\u0026#34; : \u0026#34;mongo3:30003\u0026#34;, 33 \u0026#34;arbiterOnly\u0026#34; : false, 34 \u0026#34;buildIndexes\u0026#34; : true, 35 \u0026#34;hidden\u0026#34; : false, 36 \u0026#34;priority\u0026#34; : 1, 37 \u0026#34;tags\u0026#34; : { 38 39 }, 40 \u0026#34;slaveDelay\u0026#34; : NumberLong(0), 41 \u0026#34;votes\u0026#34; : 1 42 } 43] Reference\r文中的 github issue 連結 https://github.com/bitnami/bitnami-docker-mongodb/issues/84\r這篇也解釋得很清楚，可以參考參考 https://serverfault.com/questions/895355/docker-container-unable-to-connect-to-mongodb-replica-set\r","date":"2020-12-20T20:07:00+00:00","updated":"2020-12-20T20:07:00+00:00"},{"objectID":"1608494580","permalink":"/post/database-mongodb-install-shell-only/mongodb-install-shell/","title":"[MongoDB] Install MongoDB Shell","content":"Install Mongo Shell Only on Linux\rmongo shell 已經包含在 MongoDB 服務器中。如果已經安裝了 DB，則 mongo shell 將安裝在與服務器 binary 文件相同的位置。 但如果只想從 MongoDB 服務器外單獨下載 mongo shell，可以按照以下步驟將 shell 作為獨立程序安裝：\n下載\r進入 MongoDB Coummunity Download Center\r，依照作業系統版本選擇 mongo shell，並把連結複製起來。\n1$ wget https://fastdl.mongodb.org/linux/mongodb-shell-linux-x86_64-rhel70-4.4.2.tgz 2$ tar tzxvf mongodb-shell-linux-x86_64-rhel70-4.4.2.tgz 複製 mongo 執行檔到執行目錄\r1$ cd mongodb-shell-linux-x86_64-rhel70-4.4.2/bin 2$ cp mongo /usr/local/bin 3$ mongo 4MongoDB shell version v4.4.2 5connecting to: mongodb://127.0.0.1:27017/?compressors=disabled\u0026amp;gssapiServiceName=mongodb 6Error: couldn\u0026#39;t connect to server 127.0.0.1:27017, connection attempt failed: SocketException: Error connecting to 127.0.0.1:27017 :: caused by :: Connection refused : 7connect@src/mongo/shell/mongo.js:374:17 8@(connect):2:6 9exception: connect failed 10exiting with code 1 就成功了！（p.s. 錯誤是因為本機 127.0.0.1:27017 沒裝 MongoDB）\nReference\rhttps://docs.mongodb.com/manual/mongo/\r","date":"2020-12-20T20:03:00+00:00","updated":"2020-12-20T20:03:00+00:00"},{"objectID":"1608070140","permalink":"/post/database-mongodb-install-monogodb-replicaset-with-docker/mongodb-install-db-replicaset-with-docker/","title":"[MongoDB] Install MongoDB Replica Set with Docker","content":"Replica Set Installation\rPre-requisties\r確認是否安裝 Docker，並確認 docker daemon 是否有跑起來\n1$ docker -v 2$ docker images 下載最新版的官方 mongo image\n1$ docker pull mongo 架構\r開始安裝\r設置網路\r1[root@test ~]# docker network ls 2NETWORK ID NAME DRIVER SCOPE 3cd9c58204dfa bridge bridge local 43954d16a34fc host host local 5146f0c7f58e7 none null local 新增網路\r1$ docker network create mongo-cluster 跑 container\r1$ docker run -d --name mongo1 --network mongo-cluster -p 30001:30001 -v mongo1:/data/db mongo:latest mongod --replSet myReplica --port 30001 --dbpath /data/db 2$ docker run -d --name mongo2 --network mongo-cluster -p 30002:30002 -v mongo2:/data/db mongo:latest mongod --replSet myReplica --port 30002 --dbpath /data/db 3$ docker run -d --name mongo3 --network mongo-cluster -p 30003:30003 -v mongo3:/data/db mongo:latest mongod --replSet myReplica --port 30003 --dbpath /data/db 進入 container 設置 replication\r1$ docker exec -it mongo1 mongo --port 30001 1\u0026gt; rs.initiate({\u0026#34;_id\u0026#34;:\u0026#34;myReplica\u0026#34;,\u0026#34;members\u0026#34;:[{\u0026#34;_id\u0026#34;:0,\u0026#34;host\u0026#34;:\u0026#34;mongo1:30001\u0026#34;},{\u0026#34;_id\u0026#34;:1,\u0026#34;host\u0026#34;:\u0026#34;mongo2:30002\u0026#34;},{\u0026#34;_id\u0026#34;:2,\u0026#34;host\u0026#34;:\u0026#34;mongo3:30003\u0026#34;}]}) 第一個參數的 _id 必須與跑 mongod 的 \u0026ndash;replSet 參數相同，第二個參數 members 需列出所有欲加入 replica set 的 node。而因為所有 container 都加在 mongo-cluster 的 docker network，所以可以直接使用 container name 辨別。\n1{ 2\u0026#34;ok\u0026#34; : 1, 3\u0026#34;$clusterTime\u0026#34; : { 4\u0026#34;clusterTime\u0026#34; : Timestamp(1608026586, 1), 5\u0026#34;signature\u0026#34; : { 6\u0026#34;hash\u0026#34; : BinData(0,\u0026#34;AAAAAAAAAAAAAAAAAAAAAAAAAAA=\u0026#34;), 7\u0026#34;keyId\u0026#34; : NumberLong(0) 8} 9}, 10\u0026#34;operationTime\u0026#34; : Timestamp(1608026586, 1) 11} 12myReplica:SECONDARY\u0026gt; 13myReplica:PRIMARY\u0026gt; 成功初始化群集後，應該可以看到回傳的訊息顯示 ok = 1。並且 shell 最前面會先變成 myReplica:SECONDARY\u0026gt;，再按一下 enter，就能成功變成 myReplica:PRIMARY\u0026gt;。先是 SECONDARY 的原因是因為還沒選出 PRIMARY。\n開始寫資料\r1\u0026gt; use test 2\u0026gt; db.col1.insert({hello:\u0026#39;world\u0026#39;}) 3WriteResult({ \u0026#34;nInserted\u0026#34; : 1 }) 4\u0026gt; db.col1.find() 5{ \u0026#34;_id\u0026#34; : ObjectId(\u0026#34;87362127767493de37ff95ed\u0026#34;), \u0026#34;hello\u0026#34; : \u0026#34;world\u0026#34; } 再試試看連到第二台 MongoDB 找資料室不是已經備份成功了。在使用 Secondary 資料庫搜尋前，需要先下 db2.setSlaveOk () 讓 shell 知道我們有意要用非 primary 資料庫查詢。\n1\u0026gt; db2 = (new Mongo(\u0026#39;mongo2:30002\u0026#39;)).getDB(\u0026#39;test\u0026#39;) 2test 3\u0026gt; db2.setSlaveOk() 4\u0026gt; db2.col1.find() 5{ \u0026#34;_id\u0026#34; : ObjectId(87362127767493de37ff95ed\u0026#34;), \u0026#34;hello\u0026#34; : \u0026#34;world\u0026#34; } Reference\rhttps://www.sohamkamani.com/blog/2016/06/30/docker-mongo-replica-set/\r","date":"2020-12-15T22:09:00+00:00","updated":"2020-12-15T22:09:00+00:00"},{"objectID":"1608069300","permalink":"/post/database-mongodb-intro-and-install-on-baremetal/mongodb-introduction-and-installation/","title":"[MongoDB] Introduction and Installation on Bare Metal","content":"文件導向式(document-oriented)資料庫，儲存文件（document）或物件（object），沒有 row 的概念，取而代之的是 document。\nFeatures\rschema-less：擁有彈性的 schema 易於橫向擴展，document 的數據模型能很容易在多台伺服器之間進行數據分割。 性能：MongoDB 能預分配，以利用額外的空間換取穩定，同時盡可能把多的內存用作 cache，試圖為每次查詢自動選擇正確的索引。查詢(使用索引)、插入(自動分片)等操作的速度很快。 Map/Reduce 的聚合（aggregation）功能：更多可能性，且提供方便的資料分組、處理與二次加工等操作 副本（Replication）\u0026amp;amp; 容錯移轉（failover）：提供資料的高可用性（HA, High Availability） 缺點\r不支援事務操作 : 所以通常不適合應用在銀行或會計這種系統上，因為不包證一致性。 占用比較多空間 : 主要是有兩個原因，首先是它會預分配空間，為了提高效能，而第二個原因是欄位所占用的空間。\n儲存架構\rdocument\rDocument 是 mongodb 的核心，它就是 Key-Value 的對應組合。資料的儲存架構是以類似 JSON 的資料結構 BSON(Binary JSON) 儲存，每筆資料的 key 和 value 都是區分大小寫。\n1{ 2 _id: \u0026amp;#34;948794777\u0026amp;#34;, 3 name: \u0026amp;#34;Robby\u0026amp;#34;, 4 age: 30, 5 email: \u0026amp;#34;Robby\u0026amp;#34;, 6 skill: [ 7 \u0026amp;#39;javascript\u0026amp;#39;, 8 \u0026amp;#39;java\u0026amp;#39; 9 ] 10} _id：雖然稱為 NoSQL，但系統會自動幫你產生 key 區分大小寫，且不能相同 collection\rCollection 是一組 Document，如果把它用來與關聯式資料庫比較，他就是 Table 裡面存放了很多 Row (document)。Collection 是動態的，一個 collection 裡的document 可以是各種類型。如下；\n1{ id : 1, name : \u0026amp;#34;mark\u0026amp;#34; } 2{ age : 100 } 在 Ubuntu 18.04 上 …","date":"2020-12-15T21:55:00+00:00","updated":"2020-12-15T21:55:00+00:00"},{"objectID":"1608069120","permalink":"/post/devops-install-docker-and-docker-compose-on-centos7/install-docker-and-docker-compose-on-centos7/","title":"[Docker] Install Docker \u0026 Docker Compose on CentOS7","content":"移除舊版\r1$ sudo yum remove docker \\ 2 docker-client \\ 3 docker-client-latest \\ 4 docker-common \\ 5 docker-latest \\ 6 docker-latest-logrotate \\ 7 docker-logrotate \\ 8 docker-engine 透過 repository 安裝 Docker\r設定 repo\r1$ sudo yum install -y yum-utils 2$ sudo yum-config-manager \\ 3 --add-repo \\ 4 https://download.docker.com/linux/centos/docker-ce.repo 安裝 Docker engine\r安裝最新版\n1$ sudo yum install docker-ce docker-ce-cli containerd.io 第一次安裝 Docker 的時候，會匯入 GPG 的金鑰，Docker CE 版的金鑰指紋是 060A 61C5 1B55 8A7F 742B 77AA C52F EB6B 621E 9F35，確認無誤就選擇 y 匯入。\n啟動 Docker\r1$ sudo systemctl start docker 查看是否成功跑起來\n1[root@test ~]# docker version 2Client: Docker Engine - Community 3 Version: 20.10.0 4 API version: 1.41 5 Go version: go1.13.15 6 Git commit: 7287ab3 7 Built: Tue Dec 8 18:57:35 2020 8 OS/Arch: linux/amd64 9 Context: default 10 Experimental: true 11 12Server: Docker Engine - Community 13 Engine: 14 Version: 20.10.0 15 API version: 1.41 (minimum version 1.12) 16 Go version: go1.13.15 17 Git commit: eeddea2 18 Built: Tue Dec 8 18:56:55 2020 19 OS/Arch: linux/amd64 20 Experimental: false 21 containerd: 22 Version: 1.4.3 23 GitCommit: 269548fa27e0089a8b8278fc4fc781d7f65a939b 24 runc: 25 Version: 1.0.0-rc92 26 GitCommit: ff819c7e9184c13b7c2607fe6c30ae19403a7aff 27 docker-init: 28 Version: 0.19.0 29 GitCommit: de40ad0 以非 root 使用者執行 docker\r如果想要使用一般使用者帳號來執行 Docker，要先將該帳號加入 docker 群組：\n1$ sudo usermod -aG docker \u0026lt;USERNAME\u0026gt; 安裝 docker compose\r使用 curl\r1$ curl -L \u0026#34;https://github.com/docker/compose/releases/download/1.26.2/docker-compose-$(uname -s)-$(uname -m)\u0026#34; -o /usr/local/bin/docker-compose 2$ chmod +x /usr/local/bin/docker-compose 3$ docker-compose -v 使用 pytion-pip\r1$ yum -y install -y epel-release # 安裝pip需要先安裝epel-release包 2$ yum install -y python-pip # 安裝pip 3$ pip install --upgrade pip # 升級pip 4$ pip install docker-compose # 安裝docker-compose 5$ docker-compose -v # 查看docker-compose的版本 6$ pip install --upgrade backports.ssl_match_hostname # 如果安裝時報錯則下 Reference\rhttps://docs.docker.com/engine/install/centos/\rhttps://www.itread01.com/content/1555398180.html\r","date":"2020-12-15T21:52:00+00:00","updated":"2020-12-15T21:52:00+00:00"},{"objectID":"1607362800","permalink":"/post/os-linux-raid-intro-hw-raid-and-sw-raid/linux-raid/","title":"[Linux] RAID 介紹及實作 (HW RAID \u0026 SW RAID)","content":"RAID（Redundant Array of Independent Disks) 磁碟陣列，多個硬碟組合成為一個邏輯磁區(作業系統只會把它當作一個硬碟)。\nRAID 一般分類成 Software RAID 與 Hardware RAID。\nHardware RAID\rHardware RAID 用專屬 RAID 運算晶片，晶片位於介面卡或直接嵌入在主機板上。作業系統只需要驅動 RAID 晶片即可運作，RAID 任務分工是由 RAID 晶片做，較不影響CPU。\nATA(SATA) RAID / BIOS RAID / Fake RAID\r這類型 RAID 介於 Software RAID 與 Hardware RAID 之間，算是半個 Hardware RAID，通常由主機板晶片來「幫忙」RAID 運算，因此有 Fake RAID（假 RAID）之稱。 要啟動這個 RAID 功能，要先在 BIOS 畫面設定以後，再使作業系統能夠正確辨識裝置即可。因為這款 RAID 要在 BIOS 內開啟，所以亦有人稱之為 BIOS RAID。\nSATA RAID 設定\r若不確定系統是否內建 RAID card，可以使用下面 command 確認：\n1$ lspci -vv | grep -i raid 200:1f.2 RAID bus controller: Intel Corporation 631xESB/632xESB SATA RAID Controller (rev 09) 步驟概覽\r在 BIOS 中啟用 RAID。 進入 RAID configuration utility。 創建 RAID。 進入 BIOS\r進入 BIOS ，進入 Advance \u0026amp;gt; SATA Configuration，將 Configure SATA as 從 AHCI 更改為 RAID。其中要確認系統碟是否存在在 SATA，要確保 SATA/sSATA RAID Boot Select 的選項為系統碟所在的地方，或是直接選 Both。 保存設置，並離開 BIOS。\n進入 RAID Configuration Utility\rRAID configuration utility 將在 BIOS 發布之前出現。畫面應該會先出現 RAID Volumes 以及 HDD 的列表。在該步驟中， …","date":"2020-12-07T17:40:00+00:00","updated":"2020-12-07T17:40:00+00:00"},{"objectID":"1607361960","permalink":"/post/os-linux-lvm-extend-root-remove-pv/linux-lvm-extend-root-and-remove-pv/","title":"[Linux] LVM- 擴大根目錄 \u0026 移除 pv 碟","content":"前言\r結果上次因為 LVM 分的根目錄空間不夠，仗著硬碟多就直接不管空間過剩的 /home，直接找一顆 sdc 掛在相同的根目錄 lv 中。後來系統打算拿一些硬碟做 RAID5，才發現我的系統碟(根目錄)用的是 sda 跟 sdc，跳過了 sdb，強迫症驅使下決定把 sdc 的 pv 移除!!\nxfsdump\rXFS 是 CentOS 7 預裝的 OS 的檔案系統，在 LVM 中 XFS 只能擴大不能縮小，所以需要利用 xfsdump 備份資料。\n目前掛載情況如下，預計把根目錄減掉 5.5T；\n1[root@pc100 /]# df -h 2Filesystem Size Used Avail Use% Mounted on 3devtmpfs 95G 0 95G 0% /dev 4tmpfs 95G 0 95G 0% /dev/shm 5tmpfs 95G 11M 95G 1% /run 6tmpfs 95G 0 95G 0% /sys/fs/cgroup 7/dev/mapper/centos-root 10T 53G 9.9T 1% / 8/dev/sda2 1014M 187M 828M 19% /boot 9/dev/sda1 200M 12M 189M 6% /boot/efi 10tmpfs 19G 0 19G 0% /run/user/0 11/dev/mapper/centos-home 1.0T 33M 1.0T 1% /home 12/dev/sdb1 5.5T 33M 5.5T 1% /data 安裝xfsdump工具\r1$ yum install xfsdump -y 備份根目錄\r將根目錄備份到 /data 下。注意，/data 需與根目錄在不同硬碟，因為根目錄需重建。\n1[root@pc100 /]# xfsdump -f /data/root.dump / 2xfsdump: using file dump (drive_simple) strategy 3xfsdump: version 3.1.7 (dump format 3.0) - type ^C for status and control 4 5 ============================= dump label dialog …","date":"2020-12-07T17:26:00+00:00","updated":"2020-12-07T17:26:00+00:00"},{"objectID":"1606498620","permalink":"/post/os-linux-lvm/linux-lvm/","title":"[Linux] LVM","content":"LVM (Logical Volume Manager) 邏輯磁碟管理器，是 Linux 系統 kernel 所提供的功能，在硬碟的分割區上建立邏輯層，可以彈性調整磁碟、分割區與檔案系統的大小。\nLVM 層級\r將實體分割區變成 Physical Volume (PV)，由 PV 組成虛擬硬碟 Virtual Group (VG)，VG 可以分割成數個虛擬分割區 Logical Volume (LV)，然後在 LV 上建立檔案系統，可以當成一般檔案系統使用。\nLVM 實作\r安裝 1$ yum install -y lvm2 列出磁碟資訊 1$ fdisk -l 分割磁碟 1$ fdisk /dev/sdb 進入對話框後，依提示作相對應的選擇。\nn # 開始分割 按 enter # default is p for primary 按 enter # 預設分一區 按 enter # 預設開頭可行的磁區 按 enter # 預設結尾可行的磁區 t # 更動型態 輸入 LVM 的 Hex Code 8e00 w # 寫入磁區設定並離開 更新 Linux Kernel 分割表資訊\n1$ partprobe 建立 Pysical Volume (PV)\n使用 pvcreate 標記指定磁碟分區為要使用的 LVM PV。 1$ pvcreate /dev/sdb1 2$ pvcreate /dev/sdb1 /dev/sdc1 # 或建立多個 pv 建立 Virtual Group (VG) 1$ vgcreate vgName /dev/sdb1 2 3# 或多個 PV 建成一個 VG 4$ vgcreate vgName /dev/sdb1 /dev/sdc1 建立 Logical Volume (LV) -n (to set the LV name) -L (LV Size in bytes) the VG name for the LV 1$ lvcreate -n lvName -L 100M vgName 查詢已建立虛擬磁區\n1$ lvdisplay Info\n不同的工具會使用不同的 LV 路徑，傳統名稱 /dev/vgName/lvName 或 kernel device mapper 名稱 /dev/mapper/vgname-lvname。\n格式化虛擬磁區\n格式化為 xfs 文件系統 1$ mkfs.xfs /dev/vgName/lvName 掛載\n在目錄中創建新的 LV，再將磁區掛載上去。 1$ mkdir data 2$ mount /dev/vgName/lvName /data 確認一下\n1$ df -h 把 mount point 記錄到 fstab 檔案下\n1$ vi /etc/fstab 1/dev/vgName/lvName /data xfs defaults 0 0 21 把原有的 lvm 目錄增大\r1# 擴大 LV 2$　lvextend -L +1T /dev/vgName/lvName 3# 擴大檔案系統 4$ resize2fs /dev/vgName/lvName Reference\rhttps://zh.wikipedia.org/wiki/%E9%82%8F%E8%BC%AF%E6%8D%B2%E8%BB%B8%E7%AE%A1%E7%90%86%E5%93%A1\rhttps://www.sysonion.de/centos-logical-volume-management-lvm/\rhttps://sc8log.blogspot.com/2017/03/linux-lvm-lvm.html\r","date":"2020-11-27T17:37:00+00:00","updated":"2020-11-27T17:37:00+00:00"},{"objectID":"1604833200","permalink":"/post/okinawa/","title":"四天三夜沖繩遊記✨","content":"長大以後都沒有過的全家出國旅行，在這次終於成行了！從六月決議玩開始，我就擔任導遊的角色，著手規劃了整個旅程。來來回回修改不下十次，甚至連小手冊都做了，雖然最後只有自己在看 XD\n這幾天天氣都極好，第一天抵達的時候多雲、第二天早上飄雨外其餘的都是豔陽高照的超級好天氣，真的很幸運，因為高中朋友早我一個禮拜去剛好遇到每天都在下雨的颱風(QQ)，出發前一個禮拜每天都不時的看四五個不同的氣象預報呢，萬幸萬幸。\nDAY1\r第一天坐早上六點半的樂桃航空，我坐凌晨的客運到機場，本來打算在機場睡一下的，但冷爆。拔昨晚從大陸飛到桃園等，麻跟二姊則從大林開車北上順道載；完全沒睡，真名符其實的紅眼班機丫 😪 最喜歡準備出發的時候，興奮得難以言喻，然後就載換行李箱的時候把手機忘載報到櫃台了哈哈哈超級烏龍。幸好在時間有餘的起飛前發現了，有驚無險地拿回來。\n日本比台灣快一個小時，所以當地時間九點抵達那霸機場，原本以為已經做好萬全準備了，但抵達時還是發現忘了提錢買好景點套票，在機場補買票時，姊他們去買必吃的豬肉蛋飯糰，吃了明太子跟山苦瓜口味，真的就如評價說的好吃，大家都很喜歡，拔還說回程可以再來買 😁 租車\r吃完後去 ORIX 接駁的地點坐去營業處取車（提前在 TABIRAI 日文網預定的，還比較划算呢 👍）租的車是 NISSAN 的五人座休旅車，比家裡的 outlander 小一點，但在沖繩來說可是超級大車啊！放眼望去幾乎七成的車都是正方形屁股的小車。出發後先去隔壁的 OTS 買便宜的第四天的玉泉洞門票後就正式出發了。沒開過右駕的拔首次上路也是手忙腳亂的，切方向燈都會不小心開到雨刷 😂 而且停車的時候前兩天也是很不習慣(但後面愈停愈順)，然後固定副駕駛導遊的我到第四天也都還是心驚膽跳的，怕習慣左偏因此擦撞到路邊道路(雖然還是有擦到路邊的三角錐跟不小心開上路邊的坎，但幸虧都沒有刮傷，可喜可賀可歌可泣 🤓) 古宇利島\r古宇利島在北邊，這四天計畫由北慢慢玩回來。所以去的車程大概有一個半小時，這也是唯一開高速公路的路程，中間有路過到一個大型休息站休息，看得到一點點海景，沖繩真的沒有一處不美的 🥺 因為比較偏僻，所以蚊蟲特別多，但令人意外的是公廁非常乾淨，也終於體會到日本整潔的公共場所。後半程換我開車，不過左右相反真的超難適應的啊啊啊，戰戰兢兢地開的半小時後就到了南詰展望所。\n這邊是小停車場，從這可以看到 …","date":"2020-11-08T19:00:00+08:00","updated":"2020-11-08T19:00:00+08:00"},{"objectID":"1604419200","permalink":"/post/programming-node-red-simulate-product-line/production-line-simulation-by-nodered/","title":"[Node-RED] Production Line Simulation","content":"Production Line (gateway1)\r初始流程\r首先第一個 inject flow (inject once at start) 會去下 http request 去跟事先寫好的 NodeJS express http server (\r參考這篇\r)索取事先放在 MySQL 的訂單資料 (目前僅設計為一筆)，function node 會放所有會使用到的 global 變數。\n1global.set(\u0026amp;#34;wo\u0026amp;#34;,msg.payload[0].wo); 2global.set(\u0026amp;#34;totalPcs\u0026amp;#34;,msg.payload[0].qty); 3node.warn(global.get(\u0026amp;#39;wo\u0026amp;#39;)); 4node.warn(global.get(\u0026amp;#39;totalPcs\u0026amp;#39;)); 5global.set(\u0026amp;#39;startTime\u0026amp;#39;,1603670400); 6 7global.set(\u0026amp;#39;m1Status\u0026amp;#39;,\u0026amp;#39;Running\u0026amp;#39;); 8global.set(\u0026amp;#34;m1DonePcs\u0026amp;#34;,0); 9// node.warn(global.get(\u0026amp;#39;m1DonePcs\u0026amp;#39;)); 10 11global.set(\u0026amp;#39;m2Status\u0026amp;#39;,\u0026amp;#39;Stop\u0026amp;#39;); 12global.set(\u0026amp;#34;m2DonePcs\u0026amp;#34;,0); 13 14global.set(\u0026amp;#39;m3Status\u0026amp;#39;,\u0026amp;#39;Stop\u0026amp;#39;); 15global.set(\u0026amp;#34;m3DonePcs\u0026amp;#34;,0); 16 17global.set(\u0026amp;#34;ng\u0026amp;#34;,0); 18global.set(\u0026amp;#34;ok\u0026amp;#34;,0); 19 20return msg; 送片流程\r接下來的 START!! inject flow 會用 link node 形成一個大迴圈，去循迴產線。進入每一台機器前會有一個 switch node 判斷目前機器運行狀態是 Running 還是 Idle，在 Running 條件下，模擬機器製作一片的時間 Producing... 後，將每一台的完成片數加一，再將機器狀態設為閒置。接著再用 link …","date":"2020-11-03T16:00:00+00:00","updated":"2020-11-03T16:00:00+00:00"},{"objectID":"1603273126","permalink":"/post/programming-overlap-border/css-overlapping-repeat-border/","title":"[CSS] 將重複邊框重疊","content":"當兩個 inline-block 元素或是 具有 float 屬性的元素放在同一行時，左邊元素的右邊邊框與右邊元素的左邊邊框會貼在一起，導致原本想設計成 4px 的邊框變成 8px，如下範例；\n1\t\u0026lt;style\u0026gt; 2\t.outter{ 3\theight: 200px; 4\twidth: 180px; 5\tbackground: white; 6\tmargin: 20px auto; 7\tborder: 1px black solid; 8\t} 9 10\t.inner{ 11\twidth: 60px; 12\theight: 60px; 13\tfloat: left; 14\tlist-style: none; 15\tborder: 4px black solid; 16\t} 17\t\u0026lt;/style\u0026gt; 18\t\u0026lt;body\u0026gt; 19\t\u0026lt;div class=\u0026#34;outter\u0026#34;\u0026gt; 20\t\u0026lt;div class=\u0026#34;inner\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; 21\t\u0026lt;div class=\u0026#34;inner\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; 22\t\u0026lt;div class=\u0026#34;inner\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; 23\t\u0026lt;div class=\u0026#34;inner\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; 24\t\u0026lt;/div\u0026gt; 25\t\u0026lt;/body\u0026gt; 解決方法\r設定 margin 屬性為邊框值的相反數，就可以把重疊的部分邊框隱藏起來（被覆蓋），讓邊框以單線的形式顯示，比如重複的地方是右邊跟下邊，那就把 margin-right \u0026amp; margin-bottom 位移 -4px。。\n1\t\u0026lt;style\u0026gt; 2\t.outter{ 3\theight: 200px; 4\twidth: 180px; 5\tbackground: white; 6\tmargin: 20px auto; 7\tborder: 1px black solid; 8\t} 9 10\t.inner{ 11\twidth: 60px; 12\theight: 60px; 13\tfloat: left; 14\tlist-style: none; 15\tborder: 4px black solid; 16\t/* 邊框處理 */ 17\tmargin: 0px -4px -4px 0px; 18\t} 19\t\u0026lt;/style\u0026gt; 20\t\u0026lt;body\u0026gt; 21\t\u0026lt;div class=\u0026#34;outter\u0026#34;\u0026gt; 22\t\u0026lt;div class=\u0026#34;inner\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; 23\t\u0026lt;div class=\u0026#34;inner\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; 24\t\u0026lt;div class=\u0026#34;inner\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; 25\t\u0026lt;div class=\u0026#34;inner\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; 26\t\u0026lt;/div\u0026gt; 27\t\u0026lt;/body\u0026gt; ","date":"2020-10-21T09:38:46+00:00","updated":"2020-10-21T09:38:46+00:00"},{"objectID":"1603100326","permalink":"/post/programming-css-align-and-middle/css-virtical-and-center-alignment/","title":"[CSS] 垂直置中問題","content":"一般在盒內文字想達成垂直置中的效果我們使用 line-height 將高度設置與父元素一樣即可\nhello 1\u0026lt;div style=\u0026#34;width:50px; height:50px; background: lightpink; line-height: 50px; text-align:center;\u0026#34;\u0026gt;hello\u0026lt;/div\u0026gt; 但如果遇到多行文字、圖片或其他元素，我們就需要使用 padding 或是 margin 來調整；\n1\u0026lt;!-- margin --\u0026gt; 2\u0026lt;div style=\u0026#34;width:100px; height:100px; background: lightpink;overflow:hidden;\u0026#34;\u0026gt; 3 \u0026lt;div style=\u0026#34;width:40px; height:40px; background: lightblue; margin: 30px;\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; 4\u0026lt;/div\u0026gt; 5\u0026lt;!-- padding --\u0026gt; 6\u0026lt;div style=\u0026#34;width:40px; height:40px; background: lightpink;padding:30px;\u0026#34;\u0026gt; 7 \u0026lt;div style=\u0026#34;width:40px; height:40px; background: lightblue;\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; 8\u0026lt;/div\u0026gt; 不過此時會遇到的問題是 \u0026mdash; 當頁面中的父級元素改變高度時，其先前設的子元素垂直置中的效果就必須重設。\n解決方式 - 使用 vertical-align\r首先 vertical-align 的使用概念是針對兩個擁有 inline 或是 inline-block 的元素保持垂直置中，舉例如下:\n1\u0026lt;img src=\u0026#34;image.jpg\u0026#34; style=\u0026#34;height:150px; vertical-align:middle;\u0026#34;\u0026gt;\u0026lt;span style=\u0026#34;vertical-align:middle;\u0026#34;\u0026gt;I\u0026#39;m a fun guy.\u0026lt;/span\u0026gt; 我們會利用這個概念來解決上面問題\n在欲垂直置中的子元素旁我們要建一個沒有寬度、高度與父元素同等的標籤，就能以此為對齊基準在父元素中垂直置中了。\n1\u0026lt;head\u0026gt; 2 \u0026lt;style\u0026gt; 3 div{ 4 width:400px; 5 height:300px; 6 background:lightyellow; 7 text-align:center; 8 } 9 div span{ 10 display:inline-block; 11 vertical-align:middle; 12 height:100%; 13 } 14 div img{ 15 vertical-align:middle; 16 height:100px; 17 } 18\u0026lt;/style\u0026gt; 19\u0026lt;/head\u0026gt; 20\u0026lt;body\u0026gt; 21 \u0026lt;div\u0026gt; 22\t\u0026lt;span\u0026gt;\u0026lt;/span\u0026gt; 23\t\u0026lt;img src=\u0026#34;image.jpg\u0026#34;\u0026gt; 24\u0026lt;/div\u0026gt; 25\u0026lt;/body\u0026gt; 常見範例\r排列式商品頁面中的商品圖片不見得一致，此時這個方法就派上用場了，範例以及代碼如下；\n1\u0026lt;!DOCTYPE HTML\u0026gt; 2\u0026lt;html\u0026gt; 3 \u0026lt;head\u0026gt; 4 \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; 5 \u0026lt;meta name=\u0026#34;description\u0026#34; content=\u0026#34;test\u0026#34;\u0026gt; 6 \u0026lt;title\u0026gt;test\u0026lt;/title\u0026gt; 7 \u0026lt;style\u0026gt; 8 *{ 9 margin: 0px; 10 padding: 0px; 11 } 12 13 .box{ 14 width: 608px; 15 height: 506px; 16 margin: 10px auto; 17 } 18 19 .box dl{ 20 float: left; 21 border: 2px grey dotted; 22 width: 200px; 23 height: 250px; 24 margin: 0px -2px -2px 0; 25 } 26 27 .box dl dt{ 28 height: 180px; 29 text-align: center; 30 } 31 32 .box dl dt span{ 33 height: 100%; 34 display: inline-block; 35 vertical-align: middle; 36 } 37 38 .box dl dt a{ 39 display: inline-block; 40 vertical-align: middle; 41 } 42 .box dl dd{ 43 text-align: center; 44 } 45 46 .item{ 47 font-size: 18px; 48 } 49 50 .price{ 51 font-size: 16px; 52 color: darkred; 53 font-family: Arial, Helvetica, sans-serif; 54 margin-top: 10px; 55 } 56 57 \u0026lt;/style\u0026gt; 58 \u0026lt;/head\u0026gt; 59 \u0026lt;body\u0026gt; 60 \u0026lt;div class=\u0026#34;box\u0026#34;\u0026gt; 61 \u0026lt;dl\u0026gt; 62 \u0026lt;dt\u0026gt; 63 \u0026lt;span\u0026gt;\u0026lt;/span\u0026gt; 64 \u0026lt;a href=\u0026#34;#\u0026#34;\u0026gt;\u0026lt;img src=\u0026#34;bread1.PNG\u0026#34;\u0026gt;\u0026lt;/a\u0026gt; 65 \u0026lt;/dt\u0026gt; 66 \u0026lt;dd\u0026gt; 67 \u0026lt;div class=\u0026#34;item\u0026#34;\u0026gt;溫莎麵包\u0026lt;/div\u0026gt; 68 \u0026lt;div class=\u0026#34;price\u0026#34;\u0026gt;NTD$ 50\u0026lt;/div\u0026gt; 69 \u0026lt;/dd\u0026gt; 70 \u0026lt;/dl\u0026gt; 71 \u0026lt;dl\u0026gt; 72 \u0026lt;dt\u0026gt; 73 \u0026lt;span\u0026gt;\u0026lt;/span\u0026gt; 74 \u0026lt;a href=\u0026#34;#\u0026#34;\u0026gt;\u0026lt;img src=\u0026#34;bread2.PNG\u0026#34;\u0026gt;\u0026lt;/a\u0026gt; 75 \u0026lt;/dt\u0026gt; 76 \u0026lt;dd\u0026gt; 77 \u0026lt;div class=\u0026#34;item\u0026#34;\u0026gt;法國香頌\u0026lt;/div\u0026gt; 78 \u0026lt;div class=\u0026#34;price\u0026#34;\u0026gt;NTD$ 55\u0026lt;/div\u0026gt; 79 \u0026lt;/dd\u0026gt; 80 \u0026lt;/dl\u0026gt; 81 \u0026lt;dl\u0026gt; 82 \u0026lt;dt\u0026gt; 83 \u0026lt;span\u0026gt;\u0026lt;/span\u0026gt; 84 \u0026lt;a href=\u0026#34;#\u0026#34;\u0026gt;\u0026lt;img src=\u0026#34;bread3.PNG\u0026#34;\u0026gt;\u0026lt;/a\u0026gt; 85 \u0026lt;/dt\u0026gt; 86 \u0026lt;dd\u0026gt; 87 \u0026lt;div class=\u0026#34;item\u0026#34;\u0026gt;大菠蘿\u0026lt;/div\u0026gt; 88 \u0026lt;div class=\u0026#34;price\u0026#34;\u0026gt;NTD$ 45\u0026lt;/div\u0026gt; 89 \u0026lt;/dd\u0026gt; 90 \u0026lt;/dl\u0026gt; 91 \u0026lt;dl\u0026gt; 92 \u0026lt;dt\u0026gt; 93 \u0026lt;span\u0026gt;\u0026lt;/span\u0026gt; 94 \u0026lt;a href=\u0026#34;#\u0026#34;\u0026gt;\u0026lt;img src=\u0026#34;bread4.PNG\u0026#34;\u0026gt;\u0026lt;/a\u0026gt; 95 \u0026lt;/dt\u0026gt; 96 \u0026lt;dd\u0026gt; 97 \u0026lt;div class=\u0026#34;item\u0026#34;\u0026gt;巧克粒圈\u0026lt;/div\u0026gt; 98 \u0026lt;div class=\u0026#34;price\u0026#34;\u0026gt;NTD$ 45\u0026lt;/div\u0026gt; 99 \u0026lt;/dd\u0026gt; 100 \u0026lt;/dl\u0026gt; 101 \u0026lt;dl\u0026gt; 102 \u0026lt;dt\u0026gt; 103 \u0026lt;span\u0026gt;\u0026lt;/span\u0026gt; 104 \u0026lt;a href=\u0026#34;#\u0026#34;\u0026gt;\u0026lt;img src=\u0026#34;bread5.PNG\u0026#34;\u0026gt;\u0026lt;/a\u0026gt; 105 \u0026lt;/dt\u0026gt; 106 \u0026lt;dd\u0026gt; 107 \u0026lt;div class=\u0026#34;item\u0026#34;\u0026gt;花生什麼事\u0026lt;/div\u0026gt; 108 \u0026lt;div class=\u0026#34;price\u0026#34;\u0026gt;NTD$ 40\u0026lt;/div\u0026gt; 109 \u0026lt;/dd\u0026gt; 110 \u0026lt;/dl\u0026gt; 111 \u0026lt;dl\u0026gt; 112 \u0026lt;dt\u0026gt; 113 \u0026lt;span\u0026gt;\u0026lt;/span\u0026gt; 114 \u0026lt;a href=\u0026#34;#\u0026#34;\u0026gt;\u0026lt;img src=\u0026#34;bread6.PNG\u0026#34;\u0026gt;\u0026lt;/a\u0026gt; 115 \u0026lt;/dt\u0026gt; 116 \u0026lt;dd\u0026gt; 117 \u0026lt;div class=\u0026#34;item\u0026#34;\u0026gt;桂圓三兄弟\u0026lt;/div\u0026gt; 118 \u0026lt;div class=\u0026#34;price\u0026#34;\u0026gt;NTD$ 55\u0026lt;/div\u0026gt; 119 \u0026lt;/dd\u0026gt; 120 \u0026lt;/dl\u0026gt; 121 \u0026lt;/div\u0026gt; 122 \u0026lt;/body\u0026gt; 123\u0026lt;/html\u0026gt; ","date":"2020-10-19T09:38:46+00:00","updated":"2020-10-19T09:38:46+00:00"},{"objectID":"1602864660","permalink":"/post/programming-css-nav-practice/css-display-font-when-hover/","title":"[CSS] 移入轉換文字內容","content":"移入轉換其實很簡單，不需要算什麼正負位移，只要利用 display: none 屬性就可以輕鬆達成。先來看完成的範例：\nHome首頁 About關於 Services服務 Clients客戶 Contact聯絡 代碼如下：\n1\u0026lt;!DOCTYPE HTML\u0026gt; 2\u0026lt;html\u0026gt; 3 \u0026lt;head\u0026gt; 4 \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;/\u0026gt; 5 \u0026lt;style\u0026gt; 6 .nav2{ 7 width: 916px; 8 height: 20px; 9 margin: 0px auto; 10 border-bottom: 8px red solid; 11 padding-left: 10px; 12 } 13 14 .nav2 a{ 15 height: 20px; 16 width: 80px; 17 float: left; 18 margin-right: 1px; 19 text-decoration: none; 20 text-align: center; 21 background: rgb(223,223,223); 22 23 } 24 .nav2 a .en{ 25 display: block; 26 color: black; 27 font-size: 10px; 28 line-height: 20px; 29 } 30 31 .nav2 a .cn{ 32 display: none; 33 color: black; 34 font-size: 10px; 35 line-height: 20px; 36 } 37 .nav2 a:hover{ 38 background: red; 39 } 40\t\u0026lt;!-- 在游標移入時 .en 的樣式 --\u0026gt; 41 .nav2 a:hover .en{ 42 display: none; 43 color: white; 44 } 45\t\u0026lt;!-- 在游標移入時 .cn 的樣式 --\u0026gt; 46 .nav2 a:hover .cn{ 47 display: block; 48 color:white; 49 } 50 \u0026lt;/style\u0026gt; 51 \u0026lt;/head\u0026gt; 52 \u0026lt;body\u0026gt; 53 \u0026lt;div class=\u0026#34;nav2\u0026#34;\u0026gt; 54 \u0026lt;!-- 利用兩個標籤轉換不同文字 --\u0026gt; 55 \u0026lt;a href=\u0026#34;#\u0026#34;\u0026gt;\u0026lt;span class=\u0026#34;en\u0026#34;\u0026gt;Home\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026#34;cn\u0026#34;\u0026gt;首頁\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt; 56 \u0026lt;a href=\u0026#34;#\u0026#34;\u0026gt;\u0026lt;span class=\u0026#34;en\u0026#34;\u0026gt;About\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026#34;cn\u0026#34;\u0026gt;關於\u0026lt;/a\u0026gt; 57 \u0026lt;a href=\u0026#34;#\u0026#34;\u0026gt;\u0026lt;span class=\u0026#34;en\u0026#34;\u0026gt;Services\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026#34;cn\u0026#34;\u0026gt;服務\u0026lt;/a\u0026gt; 58 \u0026lt;a href=\u0026#34;#\u0026#34;\u0026gt;\u0026lt;span class=\u0026#34;en\u0026#34;\u0026gt;Clients\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026#34;cn\u0026#34;\u0026gt;客戶\u0026lt;/a\u0026gt; 59 \u0026lt;a href=\u0026#34;#\u0026#34;\u0026gt;\u0026lt;span class=\u0026#34;en\u0026#34;\u0026gt;Contact\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026#34;cn\u0026#34;\u0026gt;聯絡\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt; 60 \u0026lt;/div\u0026gt; 61 \u0026lt;/body\u0026gt; 62\u0026lt;html\u0026gt; ","date":"2020-10-16T16:11:00+00:00","updated":"2020-10-16T16:11:00+00:00"},{"objectID":"1602784080","permalink":"/post/programming-css-overflow-usecase/css-overflow/","title":"[CSS] overflow 應用","content":"Margin 重疊\r狀況一\r1\u0026amp;lt;head\u0026amp;gt; 2\t\u0026amp;lt;style\u0026amp;gt; 3\t.box1{ 4\twidth: 100px; 5\theight: 100px; 6\tmargin: 30px; 7\tbackground: pink; 8\t} 9\t.box2{ 10\twidth: 100px; 11\theight: 100px; 12\tmargin: 50px; 13\tbackground: lightblue; 14\t} 15\t\u0026amp;lt;/style\u0026amp;gt; 16\u0026amp;lt;/head\u0026amp;gt; 17\u0026amp;lt;body\u0026amp;gt; 18\t\u0026amp;lt;div class=\u0026amp;#34;box1\u0026amp;#34;\u0026amp;gt;\u0026amp;lt;/div\u0026amp;gt; 19\t\u0026amp;lt;div sclass=\u0026amp;#34;box2\u0026amp;#34;\u0026amp;gt;\u0026amp;lt;/div\u0026amp;gt; 20\u0026amp;lt;/body\u0026amp;gt; 可以發現到兩個 div 雖然都有設 margin，但兩者間的間距僅有 50px，是因為當上下都有 margin 時，會取最大的那個值。\n狀況二\r1 2\u0026amp;lt;head\u0026amp;gt; 3\t\u0026amp;lt;style\u0026amp;gt; 4\t.box1{ 5\twidth: 100px; 6\theight: 100px; 7\tbackground: pink; 8\t} 9\t.box2{ 10\twidth: 50px; 11\theight: 50px; 12\tmargin: 20px; 13\tbackground: lightblue; 14\t} 15\t\u0026amp;lt;/style\u0026amp;gt; 16\u0026amp;lt;/head\u0026amp;gt; 17\u0026amp;lt;body\u0026amp;gt; 18\t\u0026amp;lt;div class=\u0026amp;#34;box1\u0026amp;#34;\u0026amp;gt;\u0026amp;lt;/div\u0026amp;gt; 19\t\u0026amp;lt;div sclass=\u0026amp;#34;box2\u0026amp;#34;\u0026amp;gt;\u0026amp;lt;/div\u0026amp;gt; 20\u0026amp;lt;/div\u0026amp;gt; 21\u0026amp;lt;/body\u0026amp;gt; 可以發現在子標籤中加上 margin，會連帶父標籤受影響。解決方式是在父標籤加上 overflow: hidden; 的屬性。 當要顯示的列表不想超出既定範圍時(新聞、消息列表)\r一般什麼樣式都沒指定的狀況下：\n1\u0026amp;lt;head\u0026amp;gt; 2\t\u0026amp;lt;style\u0026amp;gt; 3\tul{ 4\twidth: 200px; 5\theight: 100px; 6 …","date":"2020-10-15T17:48:00+00:00","updated":"2020-10-15T17:48:00+00:00"},{"objectID":"1602756884","permalink":"/post/os-linux-disk-format/linux-disk-format-and-mount/","title":"[Linux] 分割 \u0026 格式化 \u0026 掛載硬碟","content":"新硬碟放入機器後要做三件事情，分割、格式化、掛載，才可以開始儲存資料；\n查看硬碟狀態\r查看目前電腦中的硬碟\n1$ root@srv1:/# ls /dev/sd* 2/dev/sda /dev/sda1 /dev/sda2 /dev/sdb /dev/sdc 列出目前系統硬碟掛載狀態\n1$ df -h 1root@srv1:/# df -h 2Filesystem Size Used Avail Use% Mounted on 3udev 32G 0 32G 0% /dev 4tmpfs 6.3G 2.2M 6.3G 1% /run 5/dev/sda2 458G 36G 399G 9% / 6tmpfs 32G 0 32G 0% /dev/shm 7tmpfs 5.0M 0 5.0M 0% /run/lock 8tmpfs 32G 0 32G 0% /sys/fs/cgroup 9tmpfs 6.3G 0 6.3G 0% /run/user/1000 硬碟分割類型\r硬碟分割有兩種模式：\n傳統 BIOS/MBR 新型 UEFI/GPT (目前有圖形介面的 BIOS 皆屬之) BIOS/MBR\r主開機紀錄（Master Boot Record，MBR）是電腦開機後存取硬碟時首先會讀取的第一個磁區。 開機過程是 BIOS → MBR → 啟動 OS。\nUEFI/GPT\r統一可延伸韌體介面（Unified Extensible Firmware Interface，UEFI），傳統 BIOS 的改良，GPT（GUID Partition Table）就是取代 MBR 的新的磁碟分割表。 開機過程是 UEFI → GPT → 啟動 OS。\n進入正題\r分割\r小於 2TB 硬碟用 MBR\r使用 fdisk 分割，後面接要分割的硬碟。進入後下 m 列出指令簡介；\n1root@srv1:/# fdisk /dev/sdb 2 3Welcome to fdisk (util-linux 2.34). 4Changes will remain in memory only, until you decide to write them. 5Be careful before using the write command. 6 7Device does not contain a …","date":"2020-10-15T10:14:44+00:00","updated":"2020-10-15T10:14:44+00:00"},{"objectID":"1602642240","permalink":"/post/programming-css-practice/first-css-practice/","title":"切版紀錄","content":"先放一下 87 分像的比較圖，左邊是自己切的，右邊是題目。\n綜和聽講解影片，把一些可改進的小地方加上備註；\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 \u0026amp;lt;!DOCTYPE HTML\u0026amp;gt; \u0026amp;lt;html\u0026amp;gt; \u0026amp;lt;head\u0026amp;gt; \u0026amp;lt;meta charset=\u0026amp;#34;utf-8\u0026amp;#34;\u0026amp;gt; \u0026amp;lt;meta name=\u0026amp;#34;description\u0026amp;#34; content=\u0026amp;#34;gotoway\u0026amp;#34;\u0026amp;gt; \u0026amp;lt;title\u0026amp;gt;Gotoway\u0026amp;lt;/title\u0026amp;gt; \u0026amp;lt;link rel=\u0026amp;#34;stylesheet\u0026amp;#34; href=\u0026amp;#34;./css/style.css\u0026amp;#34;\u0026amp;gt; \u0026amp;lt;/head\u0026amp;gt; \u0026amp;lt;body\u0026amp;gt; \u0026amp;lt;div id=\u0026amp;#34;top\u0026amp;#34;\u0026amp;gt; \u0026amp;lt;img src=\u0026amp;#34;./img/logo.png\u0026amp;#34;\u0026amp;gt; \u0026amp;lt;!-- 通常 logo 要加上超連結 --\u0026amp;gt; \u0026amp;lt;ul\u0026amp;gt; \u0026amp;lt;li\u0026amp;gt;\u0026amp;lt;a href=\u0026amp;#34;#\u0026amp;#34;\u0026amp;gt;案例作品\u0026amp;lt;/a\u0026amp;gt;\u0026amp;lt;/li\u0026amp;gt; \u0026amp;lt;li\u0026amp;gt;\u0026amp;lt;a href=\u0026amp;#34;#\u0026amp;#34;\u0026amp;gt;網站建設\u0026amp;lt;/a\u0026amp;gt;\u0026amp;lt;/li\u0026amp;gt; \u0026amp;lt;li\u0026amp;gt;\u0026amp;lt;a href=\u0026amp;#34;#\u0026amp;#34;\u0026amp;gt;整合營銷\u0026amp;lt;/a\u0026amp;gt;\u0026amp;lt;/li\u0026amp;gt; \u0026amp;lt;li\u0026amp;gt;\u0026amp;lt;a href=\u0026amp;#34;#\u0026amp;#34;\u0026amp;gt;新聞資 …","date":"2020-10-14T10:24:00+08:00","updated":"2020-10-14T10:24:00+08:00"},{"objectID":"1602534960","permalink":"/post/programming-vue-admin-template1/vue-admin-template/","title":"[Vue] 解析 vue-admin-template","content":"專案結構(src)\r1user@pc10:~/vue-admin-template/src$ tree -L 1 2├── api # 所有請求，對後端的請求皆寫在這裡 3├── App.vue # 入口頁面 4├── assets # 圖片等資源 5├── components # 全局公共組件，非公共組件在各自的 view 中維護 6├── icons 7├── layout # 全局 layout 8├── main.js # 程式入口 9├── permission.js # 權限管理 10├── router # 路由，把頁面放進導航欄 11├── settings.js 12├── store # 全局 store 管理，Vuex 倉庫，用於儲存狀態 13├── styles # 全局樣式 14├── utils # 全局公共方法，非公共在各自的 view 中維護 15└── views # 所有頁面，自添加的頁面放這裡 main.js\r1import Vue from \u0026amp;#39;vue\u0026amp;#39; 2 3import \u0026amp;#39;normalize.css/normalize.css\u0026amp;#39; // A modern alternative to CSS resets 4 5import ElementUI from \u0026amp;#39;element-ui\u0026amp;#39; 6import \u0026amp;#39;element-ui/lib/theme-chalk/index.css\u0026amp;#39; 7import locale from \u0026amp;#39;element-ui/lib/locale/lang/en\u0026amp;#39; // lang i18n 8 9import \u0026amp;#39;@/styles/index.scss\u0026amp;#39; // global css 10 11// In Vue Webpack template, Webpack is configured to replace @/ with src path，@ 是webpack中定義的別名，代替 resolve(\u0026amp;#39;src\u0026amp;#39;) 指向的路徑 12 13import App from \u0026amp;#39;./App\u0026amp;#39; 14import store from \u0026amp;#39;./store\u0026amp;#39; 15import router …","date":"2020-10-12T20:36:00+00:00","updated":"2020-10-12T20:36:00+00:00"},{"objectID":"1601997060","permalink":"/post/programmimg-frontend-dom/html-dom/","title":"[Frontend] DOM","content":"DOM (Document Object Model) 定義了標準 API，使 JavaScript 可以控制瀏覽器的行為與網頁的內容。\nDOM 將一份 HTML 文件看作是一個樹狀結構的物件，裡面的標籤皆為節點 (node)，可以改變其結構、樣式 (CSS) 或內容等：\n節點類型\rDOM 物件模型把 HTML 的元素 (element) 當作是 JavaScript 物件 (object) 來操作，常見的節點類型\u0026mdash;\ndocument：DOM tree 的根節點，所以當要存取 HTML 時，都從 document 物件開始。 element event window 參考: https://pydoing.blogspot.com/2011/08/javascript-htmldom-overview.html\rDOM API 的使用範例\r1// 取得頁面上所有的 \u0026lt;p\u0026gt; 元素 2var paragraphs = document.getElementsByTagName(\u0026#39;p\u0026#39;); 3 4// 將所有的 \u0026lt;p\u0026gt; 元素的文字顏色都改成綠色 5for (var i=0; i\u0026lt;paragraphs.length; ++i) { 6 paragraphs[i].style.color = \u0026#39;green\u0026#39;; 7} Reference\rhttps://zh.wikipedia.org/wiki/%E6%96%87%E6%A1%A3%E5%AF%B9%E8%B1%A1%E6%A8%A1%E5%9E%8B\rhttps://www.fooish.com/javascript/dom/\r","date":"2020-10-06T15:11:00+00:00","updated":"2020-10-06T15:11:00+00:00"},{"objectID":"1601996880","permalink":"/post/data-realtime-streamming-batch/realtime-streaming-and-batch/","title":"Real-time \u0026 Streaming \u0026 Batch ","content":"real-time 指的是資料的反應時間 如果一個系統為實時系統，表示該系統能在極短的時間內反應\nstreaming 指的是針對資料採取的動作\nhttps://sqlstream.com/real-time-vs-streaming-a-short-explanation/\rhttp://zicesun.com/2019/03/22/201903272000-Batch-Processing-vs-Stream-Processing/\r**待比較 streaming \u0026amp; batch**\n**待比較 kafka stream \u0026amp; spark streaming**\n","date":"2020-10-06T15:08:00+00:00","updated":"2020-10-06T15:08:00+00:00"},{"objectID":"1601996340","permalink":"/post/programming-vue-intro/vue-introduction/","title":"[Vue] 安裝及基本組成簡介","content":"install nodejs and npm\r安裝最新版 Nodejs 以及 npm\n1$ sudo apt update 2$ sudo apt install -y nodejs 3$ sudo apt install -y npm 4$ sudo npm install npm@latest -g 5$ sudo npm cache clean -f 6$ sudo npm install n -g 7$ sudo n latest stable 8$ sudo npm -v 9$ sudo node -v install vue\r1$ sudo npm install -g @vue/cli 2$ sudo chown -R 1000:1000 \u0026amp;#34;/home/user/.npm\u0026amp;#34; 3# let npm install without sudo 4$ sudo chown -R $USER:$(id -gn $USER) /home/user/.config 5# npm update check without sudo new a vue project\r1$ vue create . # 在當前目錄下建立 2# OR 3$ vue create myproject # 或是指定新的資料夾 選擇要預安裝的套件。可以先選預設，後續還想新增其他的工具，可以再使用 npm 或是 yarn 安裝即可。\n1ue CLI v4.5.6 2? Please pick a preset: (Use arrow keys) 3❯ Default ([Vue 2] babel, eslint) 4 Default (Vue 3 Preview) ([Vue 3] babel, eslint) 5 Manually select features 按下 Enter 之後就會開始專案建立，接著等它完成。\n可以看到最下方提示，進入專案目錄後下 npm run serve 的命令就可以開始開發。\n1🎉 Successfully created project myproject. 2👉 Get started with the following commands: 3 4 $ cd myproject 5 $ npm run serve npm run serve\r …","date":"2020-10-06T14:59:00+00:00","updated":"2020-10-06T14:59:00+00:00"},{"objectID":"1601907300","permalink":"/post/programming-openapi-intro/openapi-introduction/","title":"[API] OpenAPI \u0026 Swagger 簡介","content":"OpenAPI\rOpenAPI 是一套 API 規範（OpenAPI Specification ，OAS），用於定義 RESTful API。API 規範使用 YAML 或 JSON 編寫，對於人類和機器來說皆易於閱讀。\nOpenAPI 簡單來說就是 API，一般是指一開始是封閉的系統，比如最開始的 Twitter、Google 或者 Facebook。突然有一天，他們開放了！公佈了實現某些功能的 API，來獲得他們內部的數據、執行特定操作。這個時候，這樣的 API， 我們就可稱為 \u0026ldquo;Open\u0026rdquo; API。\nSwagger\rSwagger 是一套以 OpenAPI 規範構建的開源工具，可以幫助你設計、構建、記錄和使用 REST API。主要的 Swagger 工具包括：\n- Swagger Editor：基於瀏覽器的編輯器，你可以在其中編寫 OpenAPI 規範。\n- Swagger UI：將 OpenAPI 規範呈現為交互式的 API 文檔，使得用戶可以直接在瀏覽器中調用 API。\n- Swagger Codegen：一個開源的代碼產生器，根據定義好的 RESTful API 文件產生 server stubs 及client SDKs。\n為什麼要使用 Open API\r標準化\r遵循 OAS 規範的 RESTful API 定義，可使 API 互通介面不受程式語言限定。且統一採用 API 進行跨系統呼叫，使得內部系統的介接標準一致，即便未來得換掉某一套系統，API 串接仍保持一致，其他系統不用特別修改寫法就能繼續呼叫。\n節省開發時間及成本\r使用規範好的框架來設計 API。再開發者不需要從頭了解 API 就能開始串接，可以加快開發時間。\nReference\rhttps://kknews.cc/code/nrgp5aq.html\rhttps://www.ithome.com.tw/news/133682\rhttps://www.zhihu.com/question/20225153\r","date":"2020-10-05T14:15:00+00:00","updated":"2020-10-05T14:15:00+00:00"},{"objectID":"1601475960","permalink":"/post/programming-python-paramiko/python-paramiko/","title":"[Python] Paramiko","content":"簡介\rparamiko 是一個使用 SSH2 遠端控制的模組，可以對遠端服務器進行命令或文件操作。有兩個核心組件：\nSSHClient：它的作用類似於 Linux 的 SSH 命令，是對 SSH 會話 (Session)\t* 的一個類的封裝，這個類封裝了傳輸(Transport)\t*、通道(Channel)* 及 SFTPClient 建立的方法 (open_sftp)。 SFTPClient：它的作用類似 Linux 的 SFTP 命令，是對 SFTP 客戶端的一個類的封裝。主要是實現對遠端文件的操作，上傳、下載、修改文件權限等操作。 * Transport：是一種加密的會話 (session)，使用時會同步創建一個加密的 Channel（即為一個 socket）。 * Session：client 和 server 保持連接的對象。 SSHClient()：遠端 SSH 登入\r1import paramiko 2 # new a SSHClient instant 3 client = paramiko.SSHClient() 4 5 # 使用自動添加策略，保存伺服器的 hostname 和金鑰資訊 6 client.set_missing_host_key_policy(paramiko.AutoAddPolicy()) 7 8 # connect the server through ssh using password to login 9 client.connect(hostname=\u0026#39;10.1.5.1\u0026#39;, port=22, username=\u0026#39;root\u0026#39;, password=\u0026#39;0000\u0026#39;) 10 # 或是使用金鑰登入 11 # private = paramiko.RSAKey.from_private_key_file(\u0026#39;/home/root/.ssh/id_rsa\u0026#39;) 12 # client.connect(hostname=\u0026#39;10.1.5.1\u0026#39;,port=22,username=\u0026#39;root\u0026#39;,pkey=private) exec_command() 以及 invoke_shell() 的用法與差別\rexec_command() 函數使用 SSH exec 通道，會在執行命令後關閉通道，不同通道間不保證保留互相狀態(例如工作目錄或是變數)。 invoke_shell() 函數使用 SSH shell 通道，會實現交互式 shell 會話，在 stream 關閉前會在同一個通道中執行命令。 exec_command()\r13 # open a channel and execute the command 14 stdin, stdout, stderr = client.exec_command(\u0026#39;df -h\u0026#39;) 15 # stdout 為正確輸出，stderr 為錯誤輸出，同時只有一個變量有值 16 17 # print out the result 18 print(stdout.read().decode(\u0026#39;utf-8\u0026#39;)) 19 20 # close SSHClient 21 client.close() invoke_shell()\r13 command=\u0026#39;dh -h\u0026#39; 14 chan=ssh.invoke_shell() 15 chan.send(command+\u0026#39;\\n\u0026#39;) 16 # \\n 是執行命令的意思，沒有 \\n 不會執行 17 time.sleep(1) # 等待執行 18 res=chan.recv(1024) # 接受返回訊息 19 chan.close() SSHClient 封装 Transport\r1import paramiko 2 3 # create a channel 4 transport = paramiko.Transport((\u0026#39;hostname\u0026#39;, 22)) 5 transport.connect(username=\u0026#39;root\u0026#39;, password=\u0026#39;0000\u0026#39;) 6 7 ssh = paramiko.SSHClient() 8 ssh._transport = transport 9 10 stdin, stdout, stderr = ssh.exec_command(\u0026#39;df -h\u0026#39;) 11 print(stdout.read().decode(\u0026#39;utf-8\u0026#39;)) 12 13 transport.close() Reference\rhttps://stackoverflow.com/questions/6770206/what-is-the-difference-between-the-shell-channel-and-the-exec-channel-in-jsc\rhttps://www.cnblogs.com/xiao-apple36/p/9144092.html\r","date":"2020-09-30T14:26:00+00:00","updated":"2020-09-30T14:26:00+00:00"},{"objectID":"1600267740","permalink":"/post/programming-css-margin-border-padding/css-margin-border-padding/","title":"[CSS] Margin \u0026 Border \u0026 Padding 比較","content":"比較\rpadding border margin 內邊距 邊距 外邊距 padding\r用於調整元素邊界與內容的間距。\n單獨指定寫法\rpadding-top: 30px; padding-right: 30px; padding-bottom: 30px; padding-left: 30px; 合併寫法\rpadding: 30px; \u0026ndash;\u0026gt; 上下左右皆為 30px padding: 30px 50px \u0026ndash;\u0026gt; 上下 30px, 左右 50px padding: 30px 20px 10px \u0026ndash;\u0026gt; 上 30px, 左右 20px , 下 10px padding: 10px 20px 30px 40px \u0026ndash;\u0026gt; 分別對應為上右下左 margin\r用於調整元素之間的邊界間距。 屬性設定同 padding 有上右下左的單獨屬性值，或是合併簡寫寫法。\nborder\rborder 是元素最外層的邊界。\n單獨指定樣式寫法\rborder-width: 5px 5px 5px 5px; \u0026ndash;\u0026gt; 上右下左 border-style: solid|dashed|double|dotted|hidden; border-color: black; border-top: 5px; border-right: 5px; border-bottom: 5px; border-left: 5px; 合併寫法\rborder: 5px red solid; \u0026ndash;\u0026gt; 邊框粗細 5px, 顏色 red, 樣式 solid; ","date":"2020-09-16T14:49:00+00:00","updated":"2020-09-16T14:49:00+00:00"},{"objectID":"1599843660","permalink":"/post/programming-css-float/css-float/","title":"[CSS] float","content":"HTML Normal Flow / inline \u0026amp;amp; block\r一般 HTML 的 normal flow 為由左至右及由上至下排列，一般取決於元素的 display 屬性是 inline 或是 block\nblock 元素：區塊元素，在文檔中自己佔一行。如 \u0026amp;lt;div\u0026amp;gt;、\u0026amp;lt;p\u0026amp;gt;、\u0026amp;lt;ol\u0026amp;gt;\u0026amp;lt;ul\u0026amp;gt;\u0026amp;lt;li\u0026amp;gt;、\u0026amp;lt;dl\u0026amp;gt;\u0026amp;lt;dt\u0026amp;gt;\u0026amp;lt;dd\u0026amp;gt;、\u0026amp;lt;table\u0026amp;gt;\u0026amp;lt;tr\u0026amp;gt;\u0026amp;lt;td\u0026amp;gt;、\u0026amp;lt;form\u0026amp;gt;。 inline 元素：行內元素或內聯元素，可以在一行顯示。如 \u0026amp;lt;span\u0026amp;gt;、\u0026amp;lt;a\u0026amp;gt;、\u0026amp;lt;i\u0026amp;gt;、\u0026amp;lt;b\u0026amp;gt;。 inline-block 元素；行內塊元素。如 \u0026amp;lt;img\u0026amp;gt;、 \u0026amp;lt;input\u0026amp;gt;。 Float 浮動\r因為 HTML flow 的特性四個元素會由上至下排序\n1\u0026amp;lt;div class=\u0026amp;#34;box\u0026amp;#34;\u0026amp;gt;1\u0026amp;lt;/div\u0026amp;gt; 2\u0026amp;lt;div class=\u0026amp;#34;box\u0026amp;#34;\u0026amp;gt;2\u0026amp;lt;/div\u0026amp;gt; 3\u0026amp;lt;div class=\u0026amp;#34;box\u0026amp;#34;\u0026amp;gt;3\u0026amp;lt;/div\u0026amp;gt; 4\u0026amp;lt;div class=\u0026amp;#34;box\u0026amp;#34;\u0026amp;gt;4\u0026amp;lt;/div\u0026amp;gt; 5\u0026amp;lt;div class=\u0026amp;#34;box\u0026amp;#34;\u0026amp;gt;5\u0026amp;lt;/div\u0026amp;gt; 1.box{ 2 width: 50px; 3 height: 50px; 4 background-color: orange; 5 border: 1px solid; 6 margin: 10px; 7} 1 2 3 4 5 此時如果想要將 div box 由左至右排序，則可以使用 float 屬性。\n1.box{ 2 width: 50px; 3 height: 50px; 4 background-color: orange; 5 border: 1px solid; 6 margin: 10px; 7 float: left 8} 1 2 3 4 5 Float 應用情景\r為了實現文字環繞效果 為了設計 DIV 區塊水平排列 ( …","date":"2020-09-11T17:01:00+00:00","updated":"2020-09-11T17:01:00+00:00"},{"objectID":"1599220800","permalink":"/post/data-kafka-ssl-encryption/kafka-ssl-encryption/","title":"[Kafka] server - client \u0026 client - server SSL 加密","content":"Kafka 目前支持 SSL、SASL/Kerberos、SASL/PLAIN 三種認證機制，這篇文章將紀錄如何在 Kafka broker 與 clinet 端建立 TLS/SSL 連線。\n另外關於 SASL 連線機制，可以參考 官網文件\r或是 中文版文件\r。\n步驟如下：\n生成密鑰/ 證書/ CA\r在各個實例中生成 keystore\r需要在集群中的每台機器上單獨生成，keystore 包含了兩個東西：\n含有公鑰(Public Key)/私鑰(Private Key) 的 Key pair 只包含公鑰的未簽名證書(Unsigned Certificate) 1$ keytool -keystore broker1.keystore.jks \\ 2 -alias broker1 -validity 3650 -genkey -keyalg RSA 產生過程中需輸入 keystore 的密碼以及證書的 Distinguished Name (DName)。其中最重要的是 Common Name(CN)，需為機器的 hostname；如果主機綁定了域名，則輸入完整包含域名的 FQDN。\n在其他 broker 以及 clinet 端重複以上動作。\nInfo\n證書相當於一個 ID，來表明自己是誰（區分不同的服務器）。此時，這個證書還沒有被任何 CA（Certificate Authority，證書頒發機構）所認證（進行簽名）。證書可以通過下面兩種方式簽名：\n先自己生成一個證書，再讓 CA 簽名。(此篇將採用的方法) 直接從 CA 申請一個。 在任一台機器中產生 CA\r通常情況下，證書是要向 CA 申請的，但是如果參與者僅是自家客戶端，而非面向網際網路上的客戶端（例如瀏覽器），那麼可以自己生成一個 CA，然後讓這個 CA 去簽署其他證書。\n1$ openssl req -new -x509 -keyout ca-key -out ca-cert -days 3650 產生過程中需輸入 PEM 密碼，以及證書相關資訊。而後會產生 CA 密鑰 ca-key 以及公鑰 ca-cert。\n生成 truststore\rtruststore 包含了所有可以信賴的 CA。以下面腳本創建 truststore，並將上一步驟生成的 ca-cert 匯入。\n1keytool -keystore …","date":"2020-09-04T12:00:00+00:00","updated":"2020-09-04T12:00:00+00:00"},{"objectID":"1598782200","permalink":"/post/data-zookeeper-encryption3/zookeeper-encryption-3/","title":"[Zookeeper] Encryption - node-node \u0026 client-node","content":"Node-Node Encryption\r節點到節點加密使用 SSL 保護 ZooKeeper server 間的內部連接，加密完全在 ZooKeeper 節點之間完成。默認情況下，Quorom TLS 是禁用的，必須通過編輯所有 server 中的 zoo.cfg 文件來啟用：\n1# zoo.cfg 2sslQuorum=true 3serverCnxnFactory=org.apache.zookeeper.server.NettyServerCnxnFactory 4ssl.quorum.keyStore.location=\u0026lt;path to keystore\u0026gt; 5ssl.quorum.keyStore.password=\u0026lt;password\u0026gt; 6ssl.quorum.trustStore.location=\u0026lt;path to truststore\u0026gt; 7ssl.quorum.trustStore.password=\u0026lt;password\u0026gt; 接著重新啟用 zookeeper 集群，查看 Zookeeper server log 出現以下訊息，便可以使用 ssl 在 server 間通訊。\n1info [main:QuorumPeer@1789] - Using TLS encrypted quorum communication info [main:QuorumPeer@1797] - Port unification disabled ... info [QuorumPeerListener:QuorumCnxManager$Listener@877] - Creating TLS-only quorum server socket Client-Node Encryption\r針對 server 端，編輯 zkServer.sh 檔案，加入以下環境變數 1export SERVER_JVMFLAGS=\u0026#34; 2-Dzookeeper.serverCnxnFactory=org.apache.zookeeper.server.NettyServerCnxnFactory 3-Dzookeeper.ssl.keyStore.location=\u0026lt;path to keystore\u0026gt; 4-Dzookeeper.ssl.keyStore.password=\u0026lt;password\u0026gt; 5-Dzookeeper.ssl.trustStore.location=\u0026lt;path to truststore\u0026gt; 6-Dzookeeper.ssl.trustStore.password=\u0026lt;password\u0026gt;\u0026#34; 編輯 zoo.cfg 檔案，加入 secureClientPort 1... 2secureClientPort=2281 針對 client 端，以 zkCli 為例，編輯 zkCli.sh 加入以下環境變數 1export CLIENT_JVMFLAGS=\u0026#34; 2-Dzookeeper.clientCnxnSocket=org.apache.zookeeper.ClientCnxnSocketNetty 3-Dzookeeper.client.secure=true 4-Dzookeeper.ssl.keyStore.location=\u0026lt;path to keystore\u0026gt; 5-Dzookeeper.ssl.keyStore.password=\u0026lt;password\u0026gt; 6-Dzookeeper.ssl.trustStore.location=\u0026lt;path to truststore\u0026gt; 7-Dzookeeper.ssl.trustStore.password=\u0026lt;password\u0026gt;\u0026#34; 重啟 Zookeeper，並透過 secure client port 使用 zkCli 連進 Zookeeper server。 1$ /opt/zookeeper/bin/zkServer.sh restart /opt/zookeeper/conf/zoo.cfg 2$ /opt/zookeeper/binzkServer -server 10.1.5.31:2281 補充：如何使用 ssl 在 zkCLi 中使用 dynamic reconfig 的功能\r在這篇\r文章\r中有提到 reconfig 存取的 znode 需要有 ACL 讀寫的權限或是 super user 的身分。 透過 ssl 使用 zkCLi 加密連線登入時，需要使用 X509AuthenticationProvider.superUser 參數，值設為 X500 principal name，將 x509 金鑰使用者加入 super user。\n1# zoo.cfg 2-Dzookeeper.X509AuthenticationProvider.superUser=CN=pc31 Reference\rhttps://zookeeper.apache.org/doc/r3.6.1/zookeeperAdmin.html#Quorum+TLS\rhttps://cwiki.apache.org/confluence/display/ZOOKEEPER/ZooKeeper+SSL+User+Guide\r","date":"2020-08-30T10:10:00+00:00","updated":"2020-08-30T10:10:00+00:00"},{"objectID":"1598780820","permalink":"/post/data-zookeeper-encryption2/zookeeper-encryption-2/","title":"[Zookeeper] Encryption - 產生 CA-signed 金鑰","content":"產生 CA 加簽金鑰\r產生 CA 金鑰憑證 在隨意一台機器上產生 CA 憑證，用於為其他金鑰加簽。 1$ openssl req -new -x509 \\ 2 -keyout test.ca.key \\ 3 -out test.ca.crt \\ 4 -days \u0026lt;days\u0026gt; \\ 5 -passout pass:\u0026lt;password\u0026gt; \\ 6 -subj \u0026#34;/C=TW/ST=NewTaipei/L=NewTaipei/O=ORG/OU=test/CN=localhost\u0026#34; 會產生加密的 RSA 密鑰 test.ca.key 以及加密的公開 CA 憑證 test.ca.crt\n產生 keypair 在每一台要加入 SSL 的機器上使用 Java keytool 工具產生屬於自己的 keypair。 1$ keytool -genkeypair \\ 2 -alias $(hostname -f) \\ 3 -keyalg RSA -keysize 2048 \\ 4 -dname \u0026#34;CN=$(hostname -f)\u0026#34; \\ 5 -validity \u0026lt;days\u0026gt; \\ 6 -keypass \u0026lt;password\u0026gt; \\ 7 -keystore keystore.jks \\ 8 -storepass \u0026lt;same password\u0026gt; \\ 9 -storetype JKS 將第一步驟產生的 CA 憑證加到個別機器上的 truststore 1$ keytool -importcert \\ 2 -keystore truststore.jks \\ 3 -file test.ca.crt \\ 4 -alias CARoot \\ 5 -keypass \u0026lt;password\u0026gt; \\ 6 -storepass \u0026lt;password\u0026gt; \\ 7 -noprompt 從第二步驟產生的 keypair keystore.jks 中匯出 public certificate 1$ keytool -certreq \\ 2 -keystore keystore.jks \\ 3 -file $(hostname).crt \\ 4 -alias $(hostname) \\ 5 --keypass \u0026lt;password\u0026gt; \\ 6 --storepass \u0026lt;password\u0026gt; 使用 CA 簽署第四步驟產生的憑證 1$ openssl x509 -req \\ 2 -CA test.ca.crt \\ 3 -CAkey test.ca.key \\ 4 -in $(hostname).crt \\ 5 -out $(hostname).signed.crt \\ 6 -days \u0026lt;days\u0026gt; \\ 7 -CAcreateserial \\ 8 -passin pass:\u0026lt;password\u0026gt; 完成後會看到以下訊息：\n1Signature ok 2subject=CN = \u0026lt;hostname\u0026gt; 3Getting CA Private Key 將第五步產生的 signed certificate 匯入到自己的 keystore 中 1$ keytool -importcert \\ 2 -keystore keystore.jks \\ 3 -file $(hostname).signed.crt \\ 4 -alias $(hostname) \\ 5 -keypass \u0026lt;password\u0026gt; \\ 6 -storepass \u0026lt;password\u0026gt; \\ 7 -noprompt 將第五步產生的 signed certificate 匯入到其他機器的 truststore 中，便可以使用 TLS 與其他機器連接。 1$ keytool -importcert \\ 2 -keystore truststore.jks \\ 3 -file $(hostname).signed.crt \\ 4 -alias CARoot \\ 5 -keypass \u0026lt;password\u0026gt; \\ 6 -storepass \u0026lt;password\u0026gt; \\ 7 -noprompt Reference\rhttps://wenku.baidu.com/view/2f1a69078e9951e79b8927df.html\r","date":"2020-08-30T09:47:00+00:00","updated":"2020-08-30T09:47:00+00:00"},{"objectID":"1598780280","permalink":"/post/data-zookeeper-encryption1/zookeeper-encryption-1/","title":"[Zookeeper] Encryption - 產生 self-signed 金鑰","content":"Netty 是一個基於 NIO 的 client/server 通信框架，它通過直接使用 NIO 簡化了 Java 應用程序的 network level 通信的複雜度。 此外，Netty 框架內置了對加密（SSL）和身份驗證（CA）的支持。 這些是可選功能，可以單獨打開或關閉。\nZookeeper 3.5 版開始，通過將環境變數的設定允許 ZooKeeper server 使用 Netty 代替 NIO（默認項）。\n接下來將用幾篇文章記錄如何加密 Zookeeper 通訊。\n產生金鑰\r每個 server 必須具有包含金鑰證書（private key + public certificate）的金鑰庫 (keystore)。 金鑰證書可以是 self-signed 的，也可以由證書頒發機構（CA）簽名。\n產生 self-signed 金鑰\r進入 Zookeeper server 所在機器 使用 Java keytool 工具產生 keypair 1$ keytool -genkeypair -alias $(hostname -f) \\ 2 -keyalg RSA -keysize 2048 \\ 3 -dname \u0026#34;CN=$(hostname -f)\u0026#34; 4 -validity \u0026lt;days\u0026gt; \\ 5 -keypass \u0026lt;password\u0026gt; \\ 6 -keystore keystore.jks \\ 7 -storepass \u0026lt;same password\u0026gt; \\ 8 -storetype JKS hostname -f 顯示主機的 FQDN (fully qualified domain name) 完全限定域名。 別名（-alias）和專有名稱（-dname）必須與與之關聯的 server hostname 匹配，否則主機名驗證會失敗。 從 keystore 中導出證書 1$ keytool -exportcert -alias $(hostname -f) \\ 2 -keystore keystore.jks \\ 3 -file $(hostname -f).cer -rfc 在 Zookeeper quorum 重複以上動作。 將證書放入 truststore 中\r為了使 server 間彼此信任，它們必須在自己的信任庫(truststore)中具有其他 server 的證書。 Truststore 可用於client-to-node 加密以及 node-to-node 的加密。\n使用 Java keytool 工具將所有 Zookeeper 實例的 certificate 導入至 truststore 中 1$ keytool -importcert -alias [host1..3] 2 -file [host1..3].cer \\ 3 -keystore truststore.jks \\ 4 -storepass \u0026lt;password\u0026gt; alias：別名可以是任何值。可以與 keystore 使用相同別名，也可是諸如 self 之類的描述性標籤。 file：指出愈導入的公開證書 出現提示時，輸入 y 將證書添加到信任庫中： 1Trust this certificate? [no]: y 2Certificate was added to keystore 重複以上動作，為在 quorum 的 Zookeeper 建立 truststore。 Reference\rhttps://zookeeper.apache.org/doc/r3.6.1/zookeeperAdmin.html#Quorum+TLS\r","date":"2020-08-30T09:38:00+00:00","updated":"2020-08-30T09:38:00+00:00"},{"objectID":"1598278200","permalink":"/post/data-kafka-topic-rebalancing/kafka-add-broker-and-balance-topic/","title":"[Kafka] 新增 broker 節點並平衡 topic","content":"在 Kafka 集群中增加 broker 非常方便，只需為其分配唯一的 broker ID，指定集群使用的 zookeeper connect 位址，然後在新服務器上啟動 Kafka。但是，舊有的 Topic 不會將 Partition 均勻分配到新的節點上，進而造成數據不平衡。 因此需要將一些現有數據遷移到這些新機器上進行 rebalancing。\nKafka 提供了 kafka-reassign-partitions.sh 工具來進行手動平均分配：\n1This tool helps to moves topic partitions between replicas. 2Option Description 3------ ----------- 4--bootstrap-server \u0026amp;lt;String: Server(s) the server(s) to use for 5 to use for bootstrapping\u0026amp;gt; bootstrapping. REQUIRED if an 6 absolute path of the log directory 7 is specified for any replica in the 8 reassignment json file 9--broker-list \u0026amp;lt;String: brokerlist\u0026amp;gt; The list of brokers to which the 10 partitions need to be reassigned in 11 the form \u0026amp;#34;0,1,2\u0026amp;#34;. This is required 12 if --topics-to-move-json-file is 13 used to generate reassignment 14 configuration 15--command-config \u0026amp;lt;String: Admin client Property file containing configs to be 16 property file\u0026amp;gt; passed to Admin Client. 17--disable-rack-aware Disable rack aware replica assignment …","date":"2020-08-24T14:10:00+00:00","updated":"2020-08-24T14:10:00+00:00"},{"objectID":"1597762440","permalink":"/post/devops-copy-docker-image-to-different-host/copy-docker-image-to-other-host/","title":"[Docker] 在不同電腦中傳 Docker image","content":"通常 docker image 都會放在公開的 repository DockerHub 或是私有的 docker registry 上供使用者 pull，但如果沒有打算公開到網路上或是無架設 repo 的需求，又要在別台電腦上使用 build 好的 docker image 時，就需要使用備份的方式傳遞 image。\n儲存並壓縮 image\r1$ docker save -o filename.tar dockerImage o: output，輸出檔案 傳到別台電腦上\r1$ pscp -scp filename.tar \u0026lt;username\u0026gt;@\u0026lt;ip\u0026gt;:\u0026lt;path\u0026gt; scp：use scp (secure copy) protocol 在目的地將檔案 load 到 docker 中\r1$ docker load -i filename.tar i: import，輸入檔案 Reference\rhttps://stackoverflow.com/questions/23935141/how-to-copy-docker-images-from-one-host-to-another-without-using-a-repository\r","date":"2020-08-18T14:54:00+00:00","updated":"2020-08-18T14:54:00+00:00"},{"objectID":"1597762080","permalink":"/post/devops-install-docker-and-docker-compose-on-ubuntu/install-docker-and-docker-compose-on-ubuntu18/","title":"[Docker] Install Docker \u0026 Docker Compose on Ubuntu 18.04","content":"官方 Ubuntu repository 中提供的 Docker 可能不是最新版本。 為確保獲得最新版本，本篇將從官方 Docker 存儲庫安裝。 為此，我們將添加一個新的 package source，從 Docker 中添加 GPG 密鑰以確保下載有效，然後安裝該程序包。\n安裝 Docker\r更新 package 並匯入 docker repo\r先更新現有的 packages\n1$ sudo apt update 安裝需要用到的套件\n1$ sudo apt install apt-transport-https ca-certificates curl software-properties-common 將官方 docker repo 的 gpg 密鑰匯入\n1$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - 將 docker 穩定版 repo 加到 apt source\n1$ sudo add-apt-repository \u0026#34;deb [arch=amd64] https://download.docker.com/linux/ubuntu bionic stable\u0026#34; 再次更新 packages\n1$ sudo apt update 安裝最新版或是指定版本\r安裝最新版 1$ sudo apt-get install docker-ce docker-ce-cli containerd.io 安裝指定版本 a. 列出 repo 中所有可用版本 1$ apt-cache madison docker-ce 2docker-ce | 5:19.03.13~1.2.beta2-0~ubuntu-bionic | https://download.docker.com/linux/ubuntu bionic/test amd64 Packages 3 docker-ce | 5:19.03.12~3-0~ubuntu-bionic | https://download.docker.com/linux/ubuntu bionic/test amd64 Packages 4 docker-ce | 5:19.03.11~3-0~ubuntu-bionic | https://download.docker.com/linux/ubuntu bionic/test amd64 Packages 5...... 1$ apt-cache madison docker-ce-cli 2docker-ce-cli | 5:19.03.13~1.2.beta2-0~ubuntu-bionic | https://download.docker.com/linux/ubuntu bionic/test amd64 Packages 3docker-ce-cli | 5:19.03.12~3-0~ubuntu-bionic | https://download.docker.com/linux/ubuntu bionic/test amd64 Packages 4...... b. 選擇指定版本安裝，例如 5:19.03.12~3-0~ubuntu-bionic\n1$ sudo apt-get install docker-ce=\u0026lt;VERSION_STRING\u0026gt; docker-ce-cli=\u0026lt;VERSION_STRING\u0026gt; containerd.io 完成安裝\r安裝成功後 daemon 會自動啟用，並且會加到開機自動啟動的程序中。可以使用下列命令確認安裝以及程序狀態： 確認版本資訊\n1$ docker version 確認狀態\n1$ sudo systemctl status docker 將當下使用者加入 docker group\r默認情況下，只能以 root 或由 docker group 中的用戶執行 docker 命令。否則會得到下列警示：\n1docker: Cannot connect to the Docker daemon. Is the docker daemon running on this host?. 2See \u0026#39;docker run --help\u0026#39;. 如果要避免在每次運行 docker 命令時都加 sudo，請將用戶添加到 docker group：\n1$ sudo usermod -aG docker ${USER} 2$ su - ${USER} 確認用戶目前所屬的 group\n1$ id -nG 上面演示了如何手動安裝 docker，有點麻煩，所以官方提供了\r腳本\r可以一鍵安裝。\n使用官方安装腳本自動安裝\r1curl -fsSL https://get.docker.com | sudo sh 完成！超簡潔。\n安裝 docker-compose\r下載最新版\r1$ sudo curl -L \u0026#34;https://github.com/docker/compose/releases/download/1.26.2/docker-compose-$(uname -s)-$(uname -m)\u0026#34; -o /usr/local/bin/docker-compose 若要安裝其他版本，請將 URL 中的 1.26.2 取代成其他\r版本\r。\n把 docker-compose 加入執行檔\r1$ sudo chmod +x /usr/local/bin/docker-compose 測試安裝\r1$ docker-compose --version 2docker-compose version 1.26.2, build 1110ad01 reference\rhttps://docs.docker.com/engine/install/ubuntu/\rhttps://docs.docker.com/compose/install/\rhttps://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-on-ubuntu-18-04\r","date":"2020-08-18T14:48:00+00:00","updated":"2020-08-18T14:48:00+00:00"},{"objectID":"1597403700","permalink":"/post/devops-truncate-docker-log/docker-truncate-docker-log/","title":"[Docker] 限制 log 大小以避免硬碟爆掉","content":"默認情況下，Docker 會抓所有 container 的標準輸出和標準錯誤 (stdout \u0026amp; stderr)，並將其寫入 /var/lib/docker/containers/[container-id]/[container-id]-json.log 的 json 文件中。\n當 container 運作時間愈長，Log 檔案會隨之變大，進而導致機器硬碟空間被 Log 佔據。因此需限制 Log 的檔案大小，以避免硬碟被塞爆。例如以下這個 log 就佔了 1G 多的磁碟空間：\n手動清除日誌\r可以使用下列幾種方法清除該 log 檔案內容：\nredirection operator\r1$ \u0026gt; \u0026lt;log file\u0026gt; \u0026gt; 重新導向運算子，因檔案已經存在，所以重新導向後會清空原有的內容。 truncate\r1$ truncate --size 0 \u0026lt;log file\u0026gt; truncate : 將文件縮小或放大到指定大小的命令。\n\u0026ndash;size : 指定大小的參數。\n/dev/null\r1$ cat /dev/null \u0026gt; \u0026lt;log file\u0026gt; /dev/null：是在類 Unix 操作系統上稱為空設備的特殊設備，當從中讀取數據時不會有任何數據，在寫入時也不存儲任何數據。 上圖使用第一種清除方法，可以看到 1G 的空間被釋放！\n但每次都手動清除是件很麻煩的事，可以考慮使用 cronjob 設定排程，或是直接設定 docker log 的容量上限。\n設定自動日誌滾動\rdocker 默認的日誌驅動程式設定可以在 /etc/docker/daemon.json 中定義，若無此文件，則需額外新增，設定值可參考下列內容：\n1{ 2 \u0026#34;log-driver\u0026#34;: \u0026#34;json-file\u0026#34;, 3 \u0026#34;log-opts\u0026#34;: { 4 \u0026#34;max-size\u0026#34;: \u0026#34;50m\u0026#34;, 5 \u0026#34;max-file\u0026#34;: \u0026#34;3\u0026#34; 6 } 7} 選項 描述 max-size 滾動前日誌的最大大小。一個正整數加上一個代表測量單位（k，m 或 g）的修飾符。默認為 -1（代表無限制） max-file 可以存在的最大日誌文件數量。如果滾動日誌會創建多余文件，則會刪除最舊的文件。只有在設置了 max-size 時才有效。一個正整數。默認為 1。 執行以下命令來重新加載 docker daemon。新的配置將在重新啟動後適用於所有新建立的容器，現有容器即使重啟 Docker 也不會使用新的日誌記錄配置。\n1$ systemctl daemon-reload 2 3$ systemctl restart docker 個別為單一容器設置日誌滾動\r若不想全局配置，也可以在個別容器啟動時，在 command 中加入參數改動。\ndocker run\r1$ docker run \\ 2 --log-driver json-file \\ 3 --log-opt max-size=10m \\ 4 --log-opt max-file=10 5 ubuntu:18.04 docker-compose\r1version: \u0026#39;3.2\u0026#39; 2services: 3 nginx: 4 image: \u0026#39;nginx:latest\u0026#39; 5 ports: 6 - \u0026#39;80:80\u0026#39; 7 logging: 8 driver: \u0026#34;json-file\u0026#34; 9 options: 10 max-size: \u0026#34;1k\u0026#34; 11 max-file: \u0026#34;3\u0026#34; Reference\rhttps://www.rdaemon.com/truncate-files-using-command-line-tools\rhttps://blog.csdn.net/kikajack/article/details/79575659\rhttps://blog.boatswain.io/zh/post/docker-container-log-rotation/\r","date":"2020-08-14T11:15:00+00:00","updated":"2020-08-14T11:15:00+00:00"},{"objectID":"1597270200","permalink":"/post/data-zookeeper-dynamic-configuration/zookeeper-dynamic-configuration/","title":"[Zookeeper] Dynamic Configuration","content":"Zookeeper 在 3.5.0 版發行後，支持動態新增節點，不需將整個集群重啟。這篇文章主要紀錄 dymamic configuration 如何實作。\n環境準備\r準備三台 VM，\r安裝 Zookeeper\r10.1.5.31 10.1.5.32 10.1.5.33 配置 zoo.cfg\r配置三台 Zookeeper 的配置檔，新增參數說明如下：\n1dataDir=/var/lib/zookeeper 2syncLimit=5 3tickTime=2000 4initLimit=10 5maxClientCnxns=100 6standaloneEnabled=false 7reconfigEnabled=true 84lw.commands.whitelist=* 9dynamicConfigFile=/opt/zookeeper/conf/zoo.cfg.dynamic standaloneEnabled\n在 3.5.0 版之前，可以在獨立模式或分佈式模式下運行 ZooKeeper。並且無法在運行時在它們之間進行切換。 默認情況下 standaloneEnabled 設置為 true。使用默認值的話，則當以單個服務器啟動時，不允許集合增長；而如果是以叢集方式啟動，則不允許減少小於等於兩個參與者。由於運行分佈式模式可以提供更大的靈活性，因此建議將標誌設置為 false。\nreconfigEnabled\n從 3.5.0 版開始到 3.5.3 之前，動態重新配置功能皆無法禁用，涉及安全問題，惡意行為者可以對 ZooKeeper 集合的配置進行任意更改，或是將受感染的服務器添加到集合中。 3.5.3 新增了這個參數，並加入一些安全機制（請見下方說明），讓使用者自行決定是否啟用。reconfigEnabled 設為 false 可以完全禁用重新配置功能，默認情況下，無論是否使用身份驗證通過 reconfig API 重新配置群集的任何嘗試都將失敗，除非 reconfigEnabled 設置為 true 。\ndynamicConfigFile\n從 3.5.0 版開始，zookeeper 區分動態配置參數和靜態配置參數，靜態配置參數在 servie 啟動時從配置文件 zoo.cfg 中讀取，並且在執行期間不會更改。動態配置參數則可以寫入 dynamicConfigFile …","date":"2020-08-12T22:10:00+00:00","updated":"2020-08-12T22:10:00+00:00"},{"objectID":"1597265100","permalink":"/post/data-zookeeper-acl/zookeeper-acl/","title":"[Zookeeper] ACL","content":"ACL(Access Control List)\rZookeeper 對 znode 操作採用 ACL 進行了存取權限控制，類似於UNIX/Linux的文件權限機制。使用 scheme:id:perm 來標識，主要涵蓋 3 個方面：\n權限模式（Scheme）：授權的策略 授權對象（ID）：授權的對象 權限（Permission）：授予的權限\n其特性如下： ZooKeeper 的權限控制是基於每個 znode 節點的，需要對每個節點設置權限 每個 znode 支持設置多種權限控制方案和多個權限 子節點不會繼承父節點的權限，客戶端無權訪問某節點，但可能可以訪問它的子節點 Scheme:id\rZK 支持 pluggable authentication schemes，可以通過擴充套件scheme，來擴充 ACL 的機制。\nBuilt-in ACL Schemes\rZK有以下的内置schemes：\nworld：默認方式，僅對應一個 id anyone，為所有用戶端開發權限。 auth：代表已經認證通過的用戶 (cli中可以通過 addauth digest user:pwd 來添加當前上下文中的授權用戶；或是通過 kerberos 來進行 authencation) digest：即用戶名:密碼這種方式認證。用 username:BASE64(SHA1(password)) 字符串作為 ACL ID。意思是認證是通過明文發送 username:password 來進行的，當用在 ACL 時，表達式為 username:BASE64(SHA1(password))。 ip：使用客戶端的主機 IP 作為 ACL ID 。可以使用一個網段，如 10.15.0.0/16。 super：在這種 scheme 情況下，對應的 id 擁有超級許可權，可以做任何事情 (cdrwa)。 sasl：設置為用戶的 uid，通過 sasl Authentication 用戶的 id，在 3.4.4 版本後 sasl 是通過 Kerberos 實現（即只有通過 Kerberos 認證的用戶才可以訪問權限的 znode），使用 sasl:uid:cdwra 字符串作為節點 ACL 的 id（如：sasl:user:cdwra）。 Permission\r5種權限簡寫為 crwda\nCREATE c 可 …","date":"2020-08-12T20:45:00+00:00","updated":"2020-08-12T20:45:00+00:00"},{"objectID":"1597242720","permalink":"/post/data-hadoop-intro/hadoop-introduction/","title":"[Hadoop] HDFS、MapReduce、Yarn 介紹","content":"Hadoop 是一個能夠儲存並管理大量資料的分散式大數據處理平臺，其包含三大模組：\nHDFS MapReduce Yarn HDFS\rHDFS為 Hadoop Distributed File System 的縮寫，分散式檔案系統。由 NameNode 與 DataNode 組成。\nNameNode \u0026amp;amp; DataNode\rNameNode：儲存檔案的 block 清單，稱之為 metadata。 DataNode：負責儲存實體檔案的 block。\nHDFS Architecture\r儲存檔案到 HDFS 前，檔案會被切成數等分小區塊 (block)，並且會將同一個 block 複製成數等分(replication, 預設值是 3 份) 再分散儲存到各個 DataNode，且相同的 block 不會同時存在於同一個 DataNode 上。 同時由 NameNode 管理記載 block 的儲存位置，當某個 Hadoop client 需要讀取這個檔案時，會先跟 NameNode 發出請求，NameNode 會根據這份清單回覆檔案的 block 位於哪幾台 DataNode，Hadoop client 再根據這份清單將各個 block 讀取出來，還原成一個完整個檔案。\n特性\r擴充性：Hadoop 可以通過增加節點輕易的擴展儲存能力或處理效能。 可靠性：由 block 的複本機制達成，當一個 Data Node 失效、毀損造成資料遺失，可以從其他台 Data Node 可以取得該檔案的複本資料。\nMapReduce\rMapReduce是一種程式模型，用於大數據的並行運算。 HDFS 處理好的檔案資料，搭配 Map \u0026amp;amp; Reduce 函數，將資料片段傳送到計算節點（Mapping），由各個節點計算處理之後再做整合（Reducimg），達到分散式計算。\n模型流程\rfork：將要執行的 MapReduce 程式複製到 Master 與每一個 Worker 機器中。Master 與 Worker 在Hadoop 中即為 JobTracker(NameNode) 與 TaskTracker(DataNode)。 assign map：Master 決定 Map 程式與 Reduce 程式，分別由哪些 Worker 機器執行。 read：將所有的資料區塊， …","date":"2020-08-12T14:32:00+00:00","updated":"2020-08-12T14:32:00+00:00"},{"objectID":"1597152655","permalink":"/post/blog-build-hugo-blog/build-hugo-with-netlify/","title":"架 Hugo 網站並部署到 Netlify","content":"前言\r第一個 blog 是用功能強大的 Hexo + Github Pages 做出來的，Hexo 基於 Node.js，有功能齊全的主題如 NexT、和豐富的外掛，僅需在配置檔修改參數就可以達到高客製化的需求，連部署到網頁上都僅需要幾個包裝好的 command，非常容易上手。\n不過隨著文章增加，能發現在生成靜態網頁的速度變慢了很多！\nHugo 由 Golang 編寫，標榜為世界上最快的靜態網站建構框架。缺點是沒有預設的主題樣式，現成的樣板選擇也沒有 Hexo 那麼多，配置檔幾乎是從頭編寫，傳統單純部署在 github pages 的方式也是不那麼簡易。\nNote\n新手適合 Hexo 配置簡單，而要求速度的人則選擇 Hugo。\n比較下來決定使用 Hugo。原本應該是要把 Hexo 部落格搬移到 Hugo，但發現兩者個 markdown 格式有些差異。因為懶得研究更多用法，這邊索性紀錄一些生活吧。\n環境建置\r安裝 Go\r到 golang\r下載作業系統對應的程式。\n下載 Hugo\r到 這裡\r下載作業系統對應的程式。下載後做以下設定：\n電腦本機新增資料夾 c:/Hugo 在該資料夾內新增 bin, Sites 兩資料夾。 將官網下載的 hugo 解壓縮後放入 c:/Hugo/bin 資料夾內。 設置環境變數，在 PATH 參數加入 C:\\Hugo\\bin 重開機 確認 Hugo 安裝成功，開啟 cmd 輸入 hugo version，若出現下面訊息則代表安裝成功。 建立網站\r開啟 cmd，切換到剛剛建立的目錄 c:\\Hugo\\Sites，建立一個名稱為 [example]的測試網站，輸入：\n1hugo new site example 如果出現 Configurations! 字樣，表示已經生成相關資料夾與檔案於 c:\\Hugo\\sites\\example 下。\n套用主題\r用 git submodule 把喜歡的主題下載下來\n1$ cd example 2$ git submodule add https://github.com/varkai/hugo-theme-zozo　theme/ 因為 theme 本身就為一個 repo，而自己本身也會是一個 repo，此時如果要在 repo 裡面使用到其他 repo 的話就必須使用 git submodule！ …","date":"2020-08-11T21:30:55+08:00","updated":"2020-08-11T21:30:55+08:00"},{"objectID":"1597146660","permalink":"/post/database-mysql-ndb-cluster8-on-ubuntu/mysql-ndb-cluster/","title":"[MySQL] 在 Ubuntu18.04 建立 MySQL NDB Cluster 8.0","content":"MySQL NDB Cluster\rMySQL cluster 將標準的 MySQL server 與內存中叢集式儲存引擎 NDB 集成起來，允許在多個無共享的系統中部署「內存中」資料庫的叢集。\n叢集通常會有多台機器，每台運行不同程序，包括 MySQL Server、NDB 叢集的數據節點、管理伺服器，以及可能專門的數據訪問程式，如下圖：\n架構中的三個角色\r管理節點 Management Node\n用於管理叢集內的其他節點，如提供配置數據、啟動並停止節點、運行備份等。由於這類節點負責管理其他節點的配置，應在啟動其他節點之前首先啟動這類節點。\n數據節點 Data Node\n用於保存叢集的數據，資料寫在 RAM 或 Disk。當節點有 2 個以上時就能實現集群的高可用保證，不過節點增加，集群的處理速度會變慢。\nSQL 節點 (API)\n用來訪問叢集數據，負責 SQL 的 Table schema，提供對外應用服務。增加 API 節點會提高整個集群的並發訪問速度和整體的吞吐量，該節點可以部署在 Web 應用伺服器上，也可以部署在專用的伺服器上。\n環境準備\r搭建 MySQL Cluster 至少要一個管理節點來管理，一個 SQL 節點來實現 MySQL server 功能和兩個資料節點實現 NDB Cluster 的功能。此篇文章將測試使用雙 SQL節點來搭建測試環境：\n1a) Management Node 10.1.5.31 2b) Data Node 10.1.5.32 3c) Data Node 10.1.5.33 4d) SQL Node 10.1.5.32 5e) SQL Node 10.1.5.33 以上機器使用 Ubuntu 18.04 作業系統。\nManagement Node\r下載並安裝\r1$ cd ~ 2$ wget https://dev.mysql.com/get/Downloads/MySQL-Cluster-8.0/mysql-cluster-community-management-server_8.0.21-1ubuntu18.04_amd64.deb 3$ sudo dpkg -i mysql-cluster-community-management-server_8.0.21-1ubuntu18.04_amd64.deb …","date":"2020-08-11T11:51:00+00:00","updated":"2020-08-11T11:51:00+00:00"},{"objectID":"1596649560","permalink":"/post/devops-git-multi-account/git-multi-account-on-same-pc/","title":"[Git] 在同一台電腦中設置多個 Git 帳號","content":"為每個帳號產生 ssh key\r1$ ssh-keygen -t rsa -C \u0026#34;userName@address\u0026#34; 產生完畢後，將公鑰放到對應帳號的 github 中\r將新金鑰新增到 SSH agent 中\r因為作業系統預設只讀取私鑰 id_rsa，為了讓 SSH 識別新的私鑰，需將其新增到 SSH agent中：\n1$ ssh-add ~/.ssh/金鑰名稱 如果出現 Could not open a connection to your authentication agent 的錯誤，就試著用以下命令：\n1$ ssh-agent bash 2$ ssh-add ~/.ssh/金鑰名稱 Warning\n重啟電腦後都需要重新 ssh-add。因為這個命令不會永久性的記住私鑰。使用 ssh-add 會把指定的私鑰新增到 ssh-agent 所管理的一個 session 當中。而 ssh-agent 是一個用於儲存私鑰的臨時性的 session 服務，也就是說當你重啟之後，ssh-agent 服務便會重置，session 會話也會失效。\n解決每次重啟都要 ssh-add 的問題\r在 git 安裝目錄下的 etc/bash.bashrc 文件中末加入\n1eval \u0026#34;$(ssh-agent -s)\u0026#34; 2ssh-add ~/.ssh/金鑰名稱 配置多個 ssh-key\r修改 ~/.ssh/config 文件\n1#default github 2Host github.com 3HostName github.com 4IdentityFile ~/.ssh/id_rsa 5 6Host github_second 7HostName github.com 8IdentityFile ~/.ssh/id_rsa_second 其中 Host 為 HostName 的別名。\n測試連線\r1$ ssh -T git@[Host] 出現\n1Hi xxx! You’ve successfully authenticated, but GitHub does not provide shell access. 代表連線成功\n在專案下設配置使用者\r先取消 global\n1$ git config --global --unset user.name 2$ git config --global --unset user.email 設置 repo 自己的 user \u0026amp; email\n1$ git config user.email \u0026#34;xxxx@xx.com\u0026#34; 2$ git config user.name \u0026#34;xxxx\u0026#34; clone 遠端資料\n1$ git clone git@[Host]:UserName/repositoryName git push\n1$ git remote add origin git@[Host]:UserName/repositoryName.git 2$ git add -am \u0026#34;commit msg\u0026#34; 3$ git push origin master Reference\r[1] https://segmentfault.com/q/1010000000835302\r[2] https://blog.csdn.net/EsonJohn/article/details/79134665\r\\\n[3] http://blog.lessfun.com/blog/2014/06/11/two-github-account-in-one-client/\r","date":"2020-08-05T17:46:00+00:00","updated":"2020-08-05T17:46:00+00:00"},{"objectID":"1596207000","permalink":"/post/data-zookeeper-endofstreamexception-error/zookeeper-endofstreamexception/","title":"[Zookeeper] fsync \u0026 EndOfStreamException 導致 zookeeper shutdown","content":"問題\rZookeeper 及 Kafka 在參數保持預設的狀況下，做壓力測試時 Zookeeper 會發生以下錯誤訊息：\nfsync-ing the write ahead log in SyncThread 12020-07-24 08:24:15,665 [myid:1] - WARN [SyncThred:2:FileTxnLog@408] - fsync-ing the write ahead log in SyncThread:3 took 8154ms which will adversely effect operation latency.File size is 67108880 bytes. See the ZooKeeper troubleshooting guide EndOfStreamException: Unable to read additional data from client, it probably closed the socket 12020-07-24 08:26:38,929 [myid:1] - WARN [NIOWorkerThread-2:NIOServerCnxn@364] - Unexpected exception 2EndOfStreamException: Unable to read additional data from client, it probably closed the socket: address = /10.1.5.33:51856, session = 0x200150baa0a0004 3org.apache.zookeeper.server.NIOServerCnxn.handleFailedRead(NIOServerCnxn.java:163) 4org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:326) 5org.apache.zookeeper.server.NIOServerCnxnFactory$IOWorkRequest.doWork(NIOServerCnxnFactory.java:522) 6org.apache.zookeeper.server.WorkerService$ScheduledWorkRequest.run(WorkerService.java:154) 7java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) 8java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) 9 at java.lang.Thread.run(Thread.java:748) 102020-07-24 08:26:48,440 [myid:1] - info [NIOWorkerThread-3:Learner@137] - Revalidating client: 0x200150baa0a0004 出現後通常會伴隨 Zookeeper 集群 shutdown，雖然過一段時間後即群會自動回復，但是整體性能會大幅下降。\n配置\r先說明一下機器環境及壓力測試條件：\n環境\rCPU 24 cores RAM 256 GB 網卡 10 GB 共三台 server 各跑一個 Zookeeper 以及一個 kafka container。\n測試\r模擬 10 個 client，每個 clinet 丟 5000 萬筆訊息，每筆訊息大小為 1K，Kafka topic 的分區數及副本數都設為 3。\n解決方法\rsession timeout 問題的原因在於，配置的超時時間太短，Zookeeper 沒有讀完 Consumer (這裡指 Kafka) 的數據，連接就被 Consumer斷開了。 所以在 Kafka 的 server.properties 文件中將針對 Zookeeper 的超時連接屬性的值調大一點，例如 : 1# server.properties 2zookeeper.session.timeout.ms=500000 fysnc 發生原因是因為 ZooKeeper 將數據 write ahead log(WAL) 寫入磁碟的速度過慢，導致 ZooKeeper 節點間 heartbeat (follower 需與同步 leader) 超時。 根本解決的方式是掛載新的硬碟到 Zookeeper 節點機器上，並建議將應用如 Kafka 與 Zookeeper 的機器分開，提高磁碟 IO 性能。 折衷處理方式則是提高 Zookeeper/config/zoo.cfg 中tickTime 以及 fsync.warningthresholdms 參數。 1# zoo.cfg 2tickTime=4000 3fsync.warningthresholdms=10000 補充\rZookeeper 儲存機制 ZooKeeper 主要是在記憶體中維護數據，但每個改變都會被寫入一個在存儲介質上的持久 WAL（Write Ahead Log）。當一個服務故障時，它能夠通過回放 WAL 恢復之前的狀態。為了防止 WAL 無限制的增長，ZooKeeper 服務會定期的將記憶體狀態快照保存到存儲介質。這些快照能夠直接加載到內存中，所有在這個快照之前的 WAL 條目都可以被安全的丟棄。 fsync.warningthresholdms 參數 用於配置 Zookeeper 進行 WAL fsync 操作時消耗時間的報警閾值。一旦 fsync 操作消耗的時間大於該參數指定的值，就在日誌中打印出報警日誌。 Reference\r[1] https://zookeeper.apache.org/doc/r3.4.10/zookeeperAdmin.html\r[2] https://blog.csdn.net/levy_cui/article/details/52242715?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-3.channel_param\u0026depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-3.channel_param\r[3] https://v1-16.docs.kubernetes.io/zh/docs/tutorials/stateful-application/zookeeper/\r","date":"2020-07-31T14:50:00+00:00","updated":"2020-07-31T14:50:00+00:00"},{"objectID":"1595585465","permalink":"/post/programming-css-transition/css-transition-and-hover/","title":"[CSS] transition + :hover 做一個有質感的 button","content":"transition\r加上漸變效果，可以放慢某一個元素轉換效果的時間，增加質感。語法如下： transition: [property] [duration] [timing-function] [delay];\nproperty：指定 css 屬性的名稱，all 代表全部，並非所有的 css 屬性都可以使用，可動畫屬性\r清單\r。\nduration：transition 效果的持續時間。\ntiming-function：指定效果的轉速曲線，可使用數值如下：\nease linear ease-in ease-out ease-in-out step-start step-end steps() cubic-bezier() 可以透過開發人員工具查看並拖曳曲線 delay：定義多久之後開始發生 transition。\n以下是沒有加上 transition 的畫面\n1#box1 { 2 background: lightgrey; 3 width: 200px; 4 height: 100px; 5} 6#box1:hover { 7 background: darkcyan; 8 color: white 9} box1 接著來看加上 transition 的變化： transition: all 1s; 第一個屬性值 all 代表對所有屬性作用，第二個值代表欲轉換的秒數。\n1#box1 { 2 background: lightgrey; 3 width: 200px; 4 height: 100px; 5 transition: all 1s; 6} 7#box1:hover { 8 background: darkcyan; 9 color: white 10} box1 做一個有質感的 button\r1\u0026lt;head\u0026gt; 2 \u0026lt;style\u0026gt; 3 #box1 { 4 width: 200px; 5 height: 100px; 6 text-align: center; 7 line-height: 100px; 8 border-radius: 30px; 9 transition: all 0.5s; 10 color: black; 11 border: 3px solid darkcyan; 12 } 13 #box1:hover { 14 background: darkcyan; 15 color: white; 16 cursor: point; 17 } 18 \u0026lt;/style\u0026gt; 19\u0026lt;/head\u0026gt; 20\u0026lt;body\u0026gt; 21 \u0026lt;div id=\u0026#34;box1\u0026#34;\u0026gt; 22 box1 23 \u0026lt;/div\u0026gt; 24\u0026lt;/body\u0026gt; box1 Reference\r[1] https://lidemy.com/courses/390445\r[2] https://eyesofkids.gitbooks.io/css3/content/contents/transitions.html\r[3] https://developer.mozilla.org/en-US/docs/Web/CSS/transition-timing-function\r","date":"2020-07-24T10:11:05+00:00","updated":"2020-07-24T10:11:05+00:00"},{"objectID":"1595366400","permalink":"/post/network-http-content-type/http-contenttype/","title":"[HTTP] content-type","content":"定義\r網際網路媒體類型（Internet media type，也稱為 MIME 類型（MIME type）或內容類型（content type））是給網際網路上傳輸的內容賦予的分類類型。HTTP 透過 content-type 的header 來表示 request 或 response message 的 body 是用何種方式編碼，伺服器會根據編碼類型使用特定的解析方式，獲取數據流中的數據。\n其格式為\n1類型名/子類型名; 可選参数 比如\n1text/html; charset = UTF-8 常見類型\rapplication/x-www-form-urlencoded\r最常見的POST提交數據方式，數據以鍵值 key1=val1\u0026amp;key2=val2 的方式編碼（urlencoded），編碼主要用來轉換易混淆的字串如 \u0026amp;、=。 瀏覽器原生表單 \u0026lt;form\u0026gt; 默認的提交方式（沒有特別設 enctype 屬性的話）。 jquery 默認 post 請求提交的方式。 multipart/form-data\r不會對字元編碼，在使用包含檔案上傳控制元件的表單時，則必須使用該值\u0026lt;form enctype=\u0026quot;multipart/form-data\u0026quot;\u0026gt;。\n會生成一個複雜的 boundary 字串來分割不同的字段。\n1POST http://www.example.com HTTP/1.1 2Content-Type:multipart/form-data; boundary=----WebKitFormBoundaryrGKCBY7qhFd3TrwA 3------WebKitFormBoundaryrGKCBY7qhFd3TrwA 4Content-Disposition: form-data; name=\u0026#34;text\u0026#34; 5title 6------WebKitFormBoundaryrGKCBY7qhFd3TrwA 7Content-Disposition: form-data; name=\u0026#34;file\u0026#34;; filename=\u0026#34;chrome.png\u0026#34; 8Content-Type: image/png 9PNG ... content of chrome.png ... 10------WebKitFormBoundaryrGKCBY7qhFd3TrwA-- Body 中按照字段個數可分為多個相同結構的部分，每部分都是以 --boundary 開始，接著內容描述信息，換行，最後是字段具體內容（文本或二進制）。Body 最後以 --boundary-- 標示結束。\ntext/plain\r數據以純文本形式(text/json/xml/html)進行編碼，其中不含任何控制項或格式字符。\nWarning\n以上三種 content type，是 HTML5 規範中 form 的 enctype 的可能值。\napplication/json\r用 json 來傳遞參數資料，可以方便的提交複雜的結構化數據，特別適合 RESTful 的接口。\nWarning\napplication/json 已經被 W3C 遺棄，建議不要在\u0026lt;form enctype=\u0026quot;...\u0026quot;\u0026gt;中使用，即使用了如果瀏覽器不支持，也會替換成 application/x-www-form-urlencoded。 同理，其余的 MIME 類型，也不支持，均會替換成默認編碼 application/x-www-form-urlencoded。\nReference\r[1]https://imququ.com/post/four-ways-to-post-data-in-http.html [2] https://zh.wikipedia.org/wiki/%E4%BA%92%E8%81%94%E7%BD%91%E5%AA%92%E4%BD%93%E7%B1%BB%E5%9E%8B\r[3] https://www.itread01.com/content/1516643918.html\r","date":"2020-07-21T21:20:00+00:00","updated":"2020-07-21T21:20:00+00:00"},{"objectID":"1595285220","permalink":"/post/programming-css-font-style/css-font/","title":"[CSS] 文字樣式 \u0026 文字斷行","content":"文字相關的 CSS 屬性\rcolor\r文字顏色，值可以是顏色敘述如 red、\r色碼表\r#FF0000，或是 rgb(255,0 , 0)。另外 rgba(255, 0, 0, 0.5) 的 a 表示透明度，值從零到一。\nfont-size\r文字大小。\nfont-weight\r字的粗細，值有 normal, bold, lighter, bolder 或是 100 ~ 900 的整百數值。常見的字重數值大致對應的字重描述詞語：\n1100 - Thin 2200 - Extra Light (Ultra Light) 3300 - Light 4400 - Regular (Normal、Book、Roman) 5500 - Medium 6600 - Semi Bold (Demi Bold) 7700 - Bold 8800 - Extra Bold (Ultra Bold) 9900 - Black (Heavy) font-family\r字體。可以接多個字體，如果字體名稱中有空白，就必須用 \u0026#39;\u0026#39; 或 \u0026amp;quot;\u0026amp;quot; 括起來。 有兩種類型的字體系列名稱：\nfamily-name：指定字體，如 times、courier、arial。 generic-family：通用字體，如 serif、sans-serif、cursive、fantasy、monospace。\n當瀏覽器載入網頁樣式時，會從 font-family 的第一個字體開始判斷，如果沒有對應的字體，就會採用下一種字體，如果到最後都沒有可用的字體，就採用電腦預設字體，一般來說定義font-family時，會將泛用字放在最後方，此時就可以透過 generic-family 來指定哪種預設字體。 letter-spacing\r字的間距。值可以是 normal(按照當前字體的正常間距確定)、3px、0.3em。\nline-height\r行距，兩行文字中間的高度。值可以是 normal(按照當前字體的正常行距確定)、3px、0.3em。\ntext-align\r水平對齊，常見屬性有：\ntext-align:left; 向左對齊 text-align:right; 向右對齊 text-align:center; 置中 text-align:justify; 使左右對齊本文， …","date":"2020-07-20T22:47:00+00:00","updated":"2020-07-20T22:47:00+00:00"},{"objectID":"1595264820","permalink":"/post/programming-css-padding-margin/css-padding-margin/","title":"[CSS] padding \u0026 margin","content":"Padding\r設定定義元素邊框與元素內容間的距離。\n1/* style.css */ 2#box1 { 3 background: palegreen; 4 width: 100px; 5 height: 100px; 6 padding: 30px;　/* 或填百分比 */ 7} 可以發現 box1 元素的字距離邊框有 30px 的邊距\n其他寫法\r1padding: 30px; padding 四個邊都是 30px。 1padding: 25px 50px 75px 100px; 上填充為25px 右填充為50px 下填充為75px 左填充為100px 1padding:25px 50px 75px; 上填充為25px 左右填充為50px 下填充為75px 1padding:25px 50px; 上下填充為25px 左右填充為50px 另外有針對四個邊的獨立寫法：\npadding-top：上方的內距 padding-right：右方的內距 padding-bottom：下方的內距 padding-left：左方的內距 四個屬性可以單獨使用也可以混搭使用。 Margin\r設定元素邊框與外面的距離。\n1/* style.css */ 2#box2 { 3 background: cadetblue; 4 width: 100px; 5 height: 100px; 6 margin: 20px;　/* 或填 auto 自動調整 或 百分比 */ 7} 其他寫法\rmargin 的其他寫法與 padding 完全相同。\nmargin-top：設置元素的上外邊距。 margin-right：設置元素的右外邊距。 margin-left：設置元素的左外邊距。 margin-bottom：設置元素的下外邊距。 針對單一的 margin 屬性也可以設 一個到四個 的值。 瀏覽器自動建的 margin\r瀏覽器會自動幫我們內建 body 的 margin，如下圖所標示\n此時只要在 css 中加入 body 的 selector，並把 margin 設成零就可以把邊框消除了。\n1/* style.css */ 2body { 3 margin: 0px; 4} ","date":"2020-07-20T17:07:00+00:00","updated":"2020-07-20T17:07:00+00:00"},{"objectID":"1595264700","permalink":"/post/programming-css-border/css-border-outline/","title":"[CSS] border \u0026 outline","content":"border\r1\u0026lt;div id=\u0026#34;box1\u0026#34;\u0026gt;box1\u0026lt;/div\u0026gt; 1#box1 { 2 background: yellow; 3 border: 20px solid green; 4 /* 框限大小 樣式 顏色*/ 5 height: 30px 6} 邊框是往外面長的，例如上面的例子 能發現到整個選取的地方高度總共是 70px\nbroder-style\rborder style 可以為外框限加上不同樣式，以下示範使用開發人員工具上面改樣式。\nborder-radius\r可以為外框的角加上弧度。\n用 border 畫圓形\r可以在沒有設 border 參數下直接使用 border-radius，將值設成 50px 或是 50%，即可畫一個圓形。\n用 border 畫三角形\r1/* style.css */ 2#box1 { 3 background: yellow; 4 width: 100px; 5 height: 100px; 6 border-top: 30px solid salmon; 7 border-bottom: 30px solid turquoise; 8 border-right: 30px solid peachpuff; 9 border-left: 30px solid pink; 10} 可以單獨為四個邊設定 border，每個邊會變成梯形；\n當中間的元素長寬都設 0px 時，四個邊會變成三角形；\n把除了要留住的三角形以外的邊以及元素本身顏色都設為 transparent 透明；\n調整各邊的大小可以得道不同長邊的三角形。\nOutline\rOutline 跟 Border 都是邊框，不一樣的是 Outline 是外框，不會改變元件的位置，而 border 會。\nInfo\n不過實際上比較常用 border!\n","date":"2020-07-20T17:05:00+00:00","updated":"2020-07-20T17:05:00+00:00"},{"objectID":"1595264400","permalink":"/post/programming-css-background/css-background/","title":"[CSS] background","content":"background\r指定顏色 1\u0026lt;!--html--\u0026gt; 2\u0026lt;div id=\u0026#34;box1\u0026#34;\u0026gt;box1\u0026lt;/div\u0026gt; 3\u0026lt;div id=\u0026#34;box2\u0026#34;\u0026gt;box2\u0026lt;/div\u0026gt; 4\u0026lt;div id=\u0026#34;box3\u0026#34;\u0026gt;box3\u0026lt;/div\u0026gt; 5\u0026lt;div id=\u0026#34;box4\u0026#34;\u0026gt;box4\u0026lt;/div\u0026gt; 1/* style.css */ 2#box1 { 3 background: yellow; 4} 5#box2 { 6 background: #2ab4b4; 7} 8#box3 { 9 background: rgb(117, 107, 255); 10} 11 12#box4 { 13 background: rgba(117, 107, 255, 0.3); /* a 指透明度，值從 0 到 1*/ 14} 指定背景圖片 1/* style.css */ 2#box1 { 3 background: url(\u0026#34;./lemon.jpg\u0026#34;) 4} background-repeat\r背景平舖方式，默認為 repeat，可以接受的屬性值有no-repeat/ repeat-x/ repeat-y，其中 repeat-x 為水平方向重複，repeat-y 則為垂直方向重複。\nbackground-position\r可能的屬性值有 center center / top center / bottom center，第一個值為水平方向、第二個值為垂直方向。\ncenter center 指定圖片置中。 1/* style.css */ 2#box1 { 3 background: url(\u0026#34;./lemon.jpg\u0026#34;); 4 background-repeat: no-repeat; 5 background-position: center center; 6 height: 500px; 7} top center 指定圖片靠上置中對齊 1/* style.css */ 2#box1 { 3 background: url(\u0026#34;./lemon.jpg\u0026#34;); 4 background-repeat: no-repeat; 5 background-position: top center; 6 height: 500px; 7} bottom center 指定圖片靠下置中對齊 1/* style.css */ 2#box1 { 3 background: url(\u0026#34;./lemon.jpg\u0026#34;); 4 background-repeat: no-repeat; 5 background-position: bottom center; 6 height: 500px; 7} 簡寫\rbackgound-repeat 以及 backgound-position 可以直接表示在 background 屬性中，例如：\n1background: url(\u0026#34;./lemon.jpg\u0026#34;) no-repeat center center; 這三個屬性值沒有特性順序，也可以將 no-repeat 寫在 url(\u0026quot;./lemon.jpg\u0026quot;) 前。\nbackground-size\r用 pixel 表示 1/* style.css */ 2#box1 { 3 background: url(\u0026#34;./lemon.jpg\u0026#34;) no-repeat center center; 4 background-size: 300px 100px 5 height: 500px; 6} 7#box2 { 8 background: #2ab4b4; 9} 用百分比表示 1/* style.css */ 2#box1 { 3 background: url(\u0026#34;./lemon.jpg\u0026#34;) no-repeat center center; 4 background-size: 50% 80% 5 height: 500px; 6} 7#box2 { 8 background: #2ab4b4; 9} contain\n縮放圖片以完全裝入背景，背景區可能會留白。 1/* style.css */ 2#box1 { 3 background: url(\u0026#34;./lemon.jpg\u0026#34;) no-repeat center center; 4 background-size: contain; 5 height: 200px; 6} 7#box2 { 8 background: #2ab4b4; 9} cover\n縮放圖片以完全覆蓋背景區，圖片可能部分會被裁切。 ","date":"2020-07-20T17:00:00+00:00","updated":"2020-07-20T17:00:00+00:00"},{"objectID":"1595263680","permalink":"/post/programming-pfx-convert/ssl-pfx-to-pem/","title":"[SSL] PFX 轉 PEM, CRT, KEY \u0026 s_client, s_server 測試","content":"憑證發行機構(CA)發出的 PFX 憑證文件，通常將根憑證/中繼憑證、網站憑證(certificate)和網站金鑰(網站私鑰)集合在一起。\npfx 轉 pem (CA)\r1$ openssl pkcs12 -in keyname.pfx -out keyname.pem -nodes pfx 轉 crt (cert)\r1$ openssl pkcs12 -in keyname.pfx -nokeys -clcerts -nodes -out keyname.crt pfx 轉 key\r1$ openssl pkcs12 -in keyname.pfx -nocerts -nodes -out keyname.key 以上三個步驟皆須再輸入當時匯出 pfx 設定的密碼。\n-in filename：指定私鑰和證書讀取的文件，默認為標準輸入。 -out filename：指定輸出的文件，默認為標準輸出。 -clcerts：僅僅輸出客戶端證書，不輸出CA證書。 -cacerts：僅僅輸出CA證書，不輸出客戶端證書。 -nocerts：不輸出任何證書。 -nokeys：不輸出任何私鑰信息值。 -nodes：對私鑰不加密。 查看憑證 / 金鑰內容\r查看憑證 pem\r1$ openssl x509 -in keyname.pem -text -noout 查看證書 cert\r1$ openssl x509 -noout -text -in keyname.crt 查看金鑰 key\r1$ openssl rsa -noout -text -in keyname.key 測試工具\rOpenssl提供了簡單的 client 和 server 工具，可以用来模擬 SSL 連接，以測試診斷。\ns_client\r以 ssl 協議連接到遠程伺服器，\n1$ openssl s_client -connect [host]:443 -connect host:port：指定遠程服務器的地址和端口，默認值為 localhost:443 -CAfile filename：指定用於驗證服務器證書的根證書 -cert filename：若服務器端需要驗證客戶端的身份，通過 -cert 指定客戶端的證書文件 -key filename：指定私鑰文件 -state：打印出 SSL 會話的狀態 s_server\r模擬 HTTPS 服務，可以返回 Openssl 相關訊息\n1$ openssl s_server -accept 443 -cert myserver.crt -key myserver.key -www -accept：用来指定監聽的 port -cert：用来指定提供服務的證書 -key：用来指定提供服務的 key Source\ropenssl 查看证书\rpfx分解成pem\r","date":"2020-07-20T16:48:00+00:00","updated":"2020-07-20T16:48:00+00:00"},{"objectID":"1594853100","permalink":"/post/programming-css-selector/css-introduction-selector/","title":"[CSS] 簡介- Selector \u0026 Selector Specificity","content":"CSS\rCascading Style Sheets 階層式樣式表，是一種用來為結構化文件（如HTML文件或XML應用）添加樣式（字型、間距和顏色等）的語言。\n幾種加入 CSS 的方式\rinline style 寫在 HTML 標籤中 (inline style) 例如： 但比較不建議，因為會跟 HTML 混在一起，不便修改及維護。\n\u0026amp;lt;style\u0026amp;gt; 在 \u0026amp;lt;head\u0026amp;gt; 標籤區塊中加上 \u0026amp;lt;style\u0026amp;gt; 標籤 1\u0026amp;lt;head\u0026amp;gt; 2 \u0026amp;lt;style\u0026amp;gt; 3 div { 4 background : red; 5 } 6 \u0026amp;lt;/style\u0026amp;gt; 7\u0026amp;lt;/head\u0026amp;gt; 其中 div 是 selector，background: red 則是 property 及 value。\n但仍然不建議使用此方法，因為當 html 標籤一多，style 區塊中就要有許多 selector，也是不便於修改及維護。\n\u0026amp;lt;link\u0026amp;gt; 將 CSS 獨立於單獨的檔案中，再從 HTML 中使用 \u0026amp;lt;link\u0026amp;gt; 指定 css 檔進來。 1\u0026amp;lt;head\u0026amp;gt; 2 \u0026amp;lt;link rel=\u0026amp;#34;stylesheet\u0026amp;#34; href=\u0026amp;#34;./style.css\u0026amp;#34; /\u0026amp;gt; 3\u0026amp;lt;/head\u0026amp;gt; 1/* style.css */ 2div { 3 background : grey; 4} @import @import 語法有兩種： 1\u0026amp;lt;style\u0026amp;gt; 2 @import \u0026amp;#34;style.css\u0026amp;#34;; 3 @import url(\u0026amp;#34;style.css\u0026amp;#34;); 4\u0026amp;lt;/style\u0026amp;gt; @import 是 CSS 中的一個 @ 規則，必須先於所有其他類型的規則 (@charset 規則除外)，須寫在 \u0026amp;lt;style\u0026amp;gt; 標籤裡。\n非模塊化開發時盡量不要用 @import 使用 @import 會先導入 html 後才會加載 CSS 文件。\n非模組化開發\n正常開發時，所有的 CSS 文件都需要引入。如果在某個 CSS 文件中使用了 @import，瀏覽器要先下載使用了 @import 的 CSS，解析完後發現有另外的 CSS 文件需要下載， …","date":"2020-07-15T22:45:00+00:00","updated":"2020-07-15T22:45:00+00:00"},{"objectID":"1594594260","permalink":"/post/programming-html-seo/html-seo/","title":"[HTML] 設置 html 標籤達成 SEO","content":"SEO\rSearch Engine Optimization 搜尋引擎優化，是一種透過了解搜尋引擎的運作規則來調整網站，以及提高目的網站在有關搜尋引擎內排名的方式。\nSEO 優化的是指不須付費的原生流量。\n以下提供幾種簡單的方式來達成\n\u0026amp;lt;meta\u0026amp;gt; tag\rHTML meta 標籤可以用來提供網頁內容的資訊給瀏覽器或是搜尋引擎。\n\u0026amp;lt;title\u0026amp;gt;頁面標題\u0026amp;lt;/title\u0026amp;gt;：頁面標題是爬蟲第一個檢索到的要素。 \u0026amp;lt;meta name=\u0026amp;quot;description\u0026amp;quot; content=\u0026amp;quot;網頁說明\u0026amp;quot; /\u0026amp;gt;：是在搜尋結果頁中呈現的網頁說明。 針對不同社群網站的 meta tag，\nfacebook\n例如下面這四行是寫給 facebook 看的，分享到 facebook 上會呈現指定畫面： 1\u0026amp;lt;meta property=\u0026amp;#34;og:title\u0026amp;#34; content=\u0026amp;#34;網站名稱或標題\u0026amp;#34; \u0026amp;gt; 2\u0026amp;lt;meta property=\u0026amp;#34;og:url\u0026amp;#34; content=\u0026amp;#34;網址\u0026amp;#34;\u0026amp;gt; 3\u0026amp;lt;meta property=\u0026amp;#34;og:image\u0026amp;#34; content=\u0026amp;#34;要顯示的縮圖網址\u0026amp;#34;\u0026amp;gt; 4\u0026amp;lt;meta property=\u0026amp;#34;og:description\u0026amp;#34; content=\u0026amp;#34;網頁描述\u0026amp;#34; \u0026amp;gt; app link\n在手機瀏覽網頁時，會自動偵測是否有安裝 app，以下以 iOS 舉例：　1\u0026amp;lt;!-- 當使用者未安裝 App，會跳出的建議下載文字 --\u0026amp;gt; 2\u0026amp;lt;meta property=\u0026amp;#34;al:ios:app_name\u0026amp;#34; content=\u0026amp;#34;TripAdvisor\u0026amp;#34;\u0026amp;gt; 3\u0026amp;lt;!-- 這可以讓使用者在未安裝 App 時，連結至安裝位置 --\u0026amp;gt; 4\u0026amp;lt;meta property=\u0026amp;#34;al:ios:app_store_id\u0026amp;#34; content=\u0026amp;#34;284876795\u0026amp;#34;\u0026amp;gt; JSON-ld\rJavaScript Object Notation for Linked Data，JSON 格式的結 …","date":"2020-07-12T22:51:00+00:00","updated":"2020-07-12T22:51:00+00:00"},{"objectID":"1594589160","permalink":"/post/programmimg-html-basic/html-introduction/","title":"[HTML] 介紹 \u0026 基礎標籤","content":"網頁是什麼\r網頁背後就是一個有結構的純文字檔 html，靠瀏覽器渲染成我們現在看到的畫面。\nHTML\r全名是 HyperText Markup Language 超文本標記語言，是一種用於建立網頁的標準標記語言，是由一堆成對的標籤組合而成的。\n基本架構\r1\u0026amp;lt;!DOCTYPE HTML\u0026amp;gt; 2\u0026amp;lt;html\u0026amp;gt; 3 \u0026amp;lt;head\u0026amp;gt; 4 \u0026amp;lt;/head\u0026amp;gt; 5 \u0026amp;lt;body\u0026amp;gt; 6 \u0026amp;lt;/body\u0026amp;gt; 7\u0026amp;lt;/html\u0026amp;gt; 撰寫規則\rHTML文件裡的標籤需成對，以\u0026amp;lt;標籤名稱\u0026amp;gt;為開頭；\u0026amp;lt;/標籤名稱\u0026amp;gt;為結尾。 如果標籤沒有值要包裹的話，則可以直接使用 \u0026amp;lt;標籤名稱 /\u0026amp;gt; 視為一組標籤。 標籤內可擁屬性，\u0026amp;lt;標籤名稱 屬性名稱=\u0026amp;quot;屬性值\u0026amp;quot;\u0026amp;gt;標籤內容\u0026amp;lt;?標籤名稱\u0026amp;gt;，屬性可有零個或多個，以空格分開。 註解文字使用 \u0026amp;lt;!--註解--\u0026amp;gt;。 HTML標籤不分大小寫。 \u0026amp;lt;!DOCTYPE HTML\u0026amp;gt;\r告知瀏覽器要使用最新版的 HTML 5 語法解讀文件，規定必須放在第一行，否則瀏覽器會認不得。\n\u0026amp;lt;html\u0026amp;gt;\u0026amp;lt;/html\u0026amp;gt;\r用來包裹整個 html 內含的程式碼，代表網頁的開始和結束。\n\u0026amp;lt;head\u0026amp;gt;\u0026amp;lt;/head\u0026amp;gt;\r用來放網頁的描述。主要用來告訴搜尋引擎這個網頁有什麼樣的內容、控制網頁與外部程式碼的連結 (script)、定義網頁使用的樣式等等 (css)。常用的標籤有：\n\u0026amp;lt;title\u0026amp;gt;: 網頁標題 \u0026amp;lt;meta\u0026amp;gt; : 提供搜尋引擎關於網頁內容的簡介 \u0026amp;lt;link\u0026amp;gt; : 網頁內與外部資源的關連 \u0026amp;lt;base\u0026amp;gt; : 設定網頁內 URL 的預設目標 \u0026amp;lt;style\u0026amp;gt; : 導入網頁樣式，如 CSS \u0026amp;lt;script\u0026amp;gt; : 導入 javascript 程式碼 1\u0026amp;lt;head\u0026amp;gt; 2 \u0026amp;lt;meta charset=\u0026amp;#34;utf-8\u0026amp;#34;\u0026amp;gt; 3 \u0026amp;lt;meta name=\u0026amp;#34;keywords\u0026amp;#34; content=\u0026amp;#34;網頁關鍵字\u0026amp;#34;\u0026amp;gt; 4 \u0026amp;lt;meta …","date":"2020-07-12T21:26:00+00:00","updated":"2020-07-12T21:26:00+00:00"},{"objectID":"1593730800","permalink":"/post/programming-api-curl/api-curl/","title":"[API] curl 使用指南","content":"curl\rcurl 可以用來請求 Web 服務器、支持文件的上傳和下載。它的名字就是 client + URL 的意思。\n基本指令格式\r1$ curl [options] [URL] HTTP request\r1-X/--request [GET|POST|PUT|DELETE|…]\t使用指定的 http method 發出 http request 2 3-H/--header\t設定request 裡的 header 4 5-i/--include\t顯示response 的 header 6 7-d/--data\t設定 http parameters 8 9-v/--verbose\t輸出比較多的訊息 10 11-u/--user\t使用者帳號、密碼 12 13-b/--cookie\tcookie GET\r1$ curl https://github.com/ulahsieh 不帶有任何參數時，curl 就是發出 GET 請求，可以方便你測試 http server 是否運作正常。\nPOST\rHttp 參數可以直接加在 url 的 query string，也可以用 -d 帶入參數間用 \u0026amp; 串接，或使用多個 -d\n1curl -X POST -d \u0026#34;param1=value1\u0026amp;param2=value2\u0026#34; 檔案上傳 1$ curl -X POST -F \u0026#39;file=@./upload.txt\u0026#39; http://www.example.com/upload.php -F 是使用http query parameter的方式，指定檔案位置的參數要加上 @\n認證\r許多服務，需先進行登入或認證後，才能存取其 API 服務。可以透過 cookie、session 或在 header 加入 session key、認證的 token 來驗證。\nSession\n如果是用 session 記錄使用者登入資訊，server 會傳一個 session id 給前端，前端需要在每次跟後端的 requests header 中置入此session id，後端便會以此 session id 識別前端。 1curl --request GET \u0026#39;http://www.example.com/api/users\u0026#39; --header \u0026#39;sessionid:xxxxxxx\u0026#39; Cookie\n在認證後，後端會回傳一個 cookie，把該 cookie 存成檔案，當要存取需要任務的 url 時，再用 -b cookie_file 的方式在 request 中植入 cookie 即可正常使用。 1# 將cookie存檔 2curl -i -X POST -d username=ula -d password=1234 -c ~/cookie.txt http://www.example.com/auth 3# 載入cookie到request中\t4curl -i --header \u0026#34;Accept:application/json\u0026#34; -X GET -b ~/cookie.txt http://www.example.com/users/1 其他常用到的參數\r1--user-agent \u0026lt;string\u0026gt;\t設置用戶代理發送給服務器 2 3--header \u0026lt;line\u0026gt;\t自定義頭信息傳遞給服務器 4 5-I\t只顯示 response header 6 7-u/--user \u0026lt;user[:password]\u0026gt;\t設置服務器的用戶和密碼 8 9-x \u0026lt;IP\u0026gt;\t表示使用這個代理IP去請求其他的網頁 10 11-s\t靜默模式，不顯示返回的一大堆頁面內容 12 13-o \u0026lt;檔案名稱\u0026gt;\t取得網頁內容，輸出至檔案 14 15-L\t表示如果在response header中如果有 location 的話就直接轉向到 location的地址 (redirect地址) 憑證錯誤的問題\r如果有遇到以下錯誤，代表 curl 不認得 CA 憑證： 要避免這個情況的話，需要在 curl 指令後面加上 -k 或 –insecure 參數，這樣 curl 便不會檢查 SSL 的有效性，例如：\n$ curl -k https://github.com/ulahsieh $ curl --insecure https://github.com/ulahsieh\nReference\rtesting-rest-with-curl-command\r","date":"2020-07-02T23:00:00+00:00","updated":"2020-07-02T23:00:00+00:00"},{"objectID":"1593724200","permalink":"/post/programming-api/api-data-transform/","title":"[API] 簡介及實作 \u0026 資料交換格式 - XML, JSON, SOAP","content":"API\rApplication Programming Interface 應用程式介面，其中『介面』就是溝通的管道，比如說 USB 隨身碟的介面就是 USB．所有電腦以及 USB 隨身硬碟的廠商只要按照 USB 的規則製造就能使用此介面去溝通。\n提供 API \u0026amp;amp; 使用 API\r比如電腦使用者說想知道網路狀況，作業系統底層變提供了 API 讓我們使用。又比如說想要更改某個網站的會員資料，那這個網站就提供了 API 讓我們能在網站上修改資料庫的會員資料。或是要在網頁上加入 google map 的功能，就需使用 google map 提供的 API。\n所以 API 可以想成是某種功能，用以溝通兩個不同的東西用的。\rWebAPI\r指 HTTP API，透過 HTTP 協定的 API。通常透過 HTTP method 去呼叫 API 進而交換資料，基本上就是丟 request，然後拿到 response 的資料。\nSDK\rSoftware Development Kit 軟體開發套件，是用來開發特定應用程式的工具組，通常是廠商針對某一平臺、系統、或硬體所釋出，用以開發應用程式的工具組，在這個工具包裡面，可能包含了各式各樣的開發工具，模擬器或 API 等。\napi 串接實作\r將使用 reqres\r網站當作伺服器，下 API 去取得該網站的資料。另外使用 request node module 實作。\n發 request 到網址，並印出 body 1const request = require(\u0026amp;#39;request\u0026amp;#39;); 2request( 3 \u0026amp;#39;https://reqres.in/api/users?page=2\u0026amp;#39;, 4 function(error, response, body) { 5 if (error) { 6 return console.log(\u0026amp;#39;request failed\u0026amp;#39;, err); 7 } 8 console.log(body) 9 } 10); 印出所有 user list\n發 request 到網址，並印出單一使用者資料 1const request = require(\u0026amp;#39;request\u0026amp;#39;); 2request( 3 …","date":"2020-07-02T21:10:00+00:00","updated":"2020-07-02T21:10:00+00:00"},{"objectID":"1593625260","permalink":"/post/programming-install-java11-on-win10/install-java10-on-win11/","title":"[Java] Install Java on Win11","content":"選擇 Java 版本\rJava 更新速度之快，截至今天已經出到 Java 14 了！本來是想隨波逐流的裝 Java 8 但參考了這篇\r文章\r後，還是決定裝 11。\nJDK 可以從 AdoptOpenJDK\r下載，AdoptOpenJDK 是 Java 使用者社群建立的，致力於倡導 OpenJDK，上面支援的作業系統以及對應的 JDK 版本 (8~11) 最齊全。\n安裝\r在網站上下載 .msi 檔後執行，在客製安裝頁面一併把第三個選項 Set JAVA_HOME variable 設起來。\n設定 PATH 系統變數\r設定 path 環境變數的目的是為了要上作業系統找到 Java 在哪。 編輯系統變數欄位中的 Path，並加入 %JAVA_HOME% 的選項，然後把它移到最前面。當電腦中安裝兩個以上的不同版本的 Java，則環境變數放在前面的會先被執行。\n確認安裝\r開啟 CMD 並下下方兩個指令，確認安裝是否成功。\n1$ java -version 2$ javac -version Hello World!\r1//Hello.java 2public class Hello { 3 public static void main(String []args) { 4 System.out.println(\u0026#34;Hello World!\u0026#34;) 5 } 6} Note\n關於程式內部詳細的解說，之後的文章會記錄。 (不過本來因為數據中台的專案要用 Map Reduce 所以學的，但因為工作又有小變動，所以不知道何年何月才可以再繼續看 Java，先暫緩了 😝 )\n編譯檔案\r使用 javac 公用程式來編譯檔案。\n1$ javac Hello.java 完成後在目錄下就會出現一個 Hello.class 的 java 位元碼 (bytecode)。\n執行\r1$ java Hello 使用 java 工具程式執行，不須帶附檔名，Java 會根據類別名稱自動載入 .class 檔案。 ","date":"2020-07-01T17:41:00+00:00","updated":"2020-07-01T17:41:00+00:00"},{"objectID":"1593621660","permalink":"/post/programming-java-jvm/java-jvm-structure/","title":"[Java] JVM 架構","content":"前一篇介紹到 Java 程式經過編譯後會產生 .class 檔 (bytecode)，只能運行在 JVM 上，而 JVM 在運算時，如同電腦需要記憶體儲存運算所需的各種資料及指令，這篇文章將紀錄 JVM 的架構。\nJVM Structure\r[1]\n上圖為 JVM 執行一個 Java 程式的過程：\nClass Loader\r用於將編譯好的 .class 文件 － Java Bytecode 加載到 Runtime Data Areas。\nExecution Engine\r用於執行 Java Bytecode 或是 native method\r.\nJava Bytecode 是以人類仍然可以理解的語言而不是機器語言編寫的。因此，Execution Engine 透過以下兩種方式將字節碼轉為 JVM 中的機器語言。\nInterpreter JIT (Just-In-Time) Compiler 什麼時候用 Interpreter 什麼時候用 Compiler?\nThe application code is initially interpreted, but the JVM monitors which part of bytecode are frequently executed and translates them to machine code for direct execution on the hardware. For bytecode which is executed only a few times, interpret it will saves the compilation time and reduces the initial latency; For frequently executed bytecode, JIT compilation will be used after an initial phase of slow interpretation. By interpreting the initial code, execution statistics can be collected before compilation, which helps to perform better optimization.[3] …","date":"2020-07-01T16:41:00+00:00","updated":"2020-07-01T16:41:00+00:00"},{"objectID":"1593621480","permalink":"/post/programming-java-jdk-jre/java-jdk-jre-jvm-intro/","title":"[Java] What are JDK, JRE, JDM ?","content":"比較\rJDK（Java Development Kit，Java開發工具包）\n是用來編譯、調試 Java 程序的開發工具包。組成包含 Java工具（javac/java/jdb等）和 Java 基礎的類庫。\nJRE（Java Runtime Environment， Java運行環境）\n所有的 java 程序都要在 JRE 下才能夠運行。組成包含 JVM 和 Java 核心類庫和支持文件。\nJVM（Java Virtual Machine， Java虛擬機）\n是JRE的一部分，虛擬機代表通過在實體計算機上模擬計算機功能，JVM 有自己的硬體架構以及指令系統，它的工作就是解釋（編譯）自己的指令集（即字節碼）並映射到本地的CPU指令集和OS的系統調用，不同的操作系統會有不同的JVM映射規則，實現 Java 的跨平台特性。 組成包含字節碼指令集、寄存器、棧、垃圾回收堆和存儲方法域\u0026hellip;等。\n[1]\n小結\r使用 JDK 開發 JAVA 程序，再通過 JDK 中的編譯程序（javac）將 Java 程序編譯成 Java Byte Code（字節碼），在 JRE 上運行這些字節碼，JVM 會解析並映射到真實操作系統的 CPU 指令集和 OS 的系統調用。 [2]\n[3]\nJava SE、Java ME、Java EE\r隨著 Java 的應用領域擴展，而發展出的不同的應用開發平台。 - Java SE：Java Platform, Standard Edition，Java 各應用平臺的基礎，可以分作四個主要的部份：JVM、JRE、JDK與Java語言。 - Java EE：Java Platform，Enterprise Edition，是在 Java SE 的基礎上構建的，提供 Web 服務、元件模型、管理和通訊 API， - Java ME：Java Platform，Micro Edition，是 Java 平臺版本中最小的一個，目的是作為小型數位設備上開發及部署應用程式的平臺，像是消費性電子產品或嵌入式系統等。\n小結\rJava SE 是做電腦上執行的軟體。 Java EE 是用來做網站的-（我們常見的JSP技術） Java ME 是做手機軟體的。\nsource\r[1] difference-between-jdk-jre-and-jvm\r[2] https://codertw.com/%E7%A8%8B%E5%BC%8F%E8%AA%9E%E8%A8%80/654140/ [3]\rc-and-java-virtual-machine-code-execution\r[4] https://www.ithome.com.tw/article/124269 ","date":"2020-07-01T16:38:00+00:00","updated":"2020-07-01T16:38:00+00:00"},{"objectID":"1593570240","permalink":"/post/netwrok-http/tcpip-http/","title":"[TCP/IP] 應用層 - HTTP","content":"HTTP\rHyperText Transfer Protocol 超文本傳輸協定，是網頁的基礎通訊協定，基於 client-server 架構來 request-reponse，用來在 client 以及 server 間傳遞超媒體文件(如 HTML) 。\n運作模型\rclient 傳 http request 到 server server 回傳 http response (html) [1]\nHTTP Client 實現程式有瀏覽器、命令列工具 \u0026hellip; 等 HTTP Server 實現程式有 httpd、nginx、apache \u0026hellip; 等 https 的 s 代表 secure，是一個比 http 更安全的連線方式 Http Method\rMETHOD 安全性 動作 語意 GET O 讀取 請求所需要的資源。 POST O 新增 在請求中攜帶 request body，並執行特定資源的處理。 HEAD X 讀取 server 僅回應狀態以及 header。 PUT x 完整更新 請求更新一筆資源的所有內容，必須是存在的資源，資源傳遞必須完整，否則為空。 PATCH X 部分更新 請求更新一筆資源的部分內容，必須是存在的資源。 DELETE x 刪除 請求移除資源。 [2] Http 狀態碼\r狀態碼通常都以開頭的數字做區分，例如 2 開頭的狀態碼都是代表成功。\n1xx informationa - 參考資訊 2xx Successful - 成功 3xx Redirection - 重新導向 4xx Client Error - 用戶端錯誤 5xx Server Error - 伺服器錯誤 詳細代碼可參考 wiki\r。 自己的 request 自己發 - 實作 http client\r前面有提到實現 http client 有瀏覽器還有其他諸如 CLI、爬蟲 \u0026hellip; 的程式。這邊我們來實作一個使用 request\rnode module 的 http client。\n$ npm install request 在 index.js 檔案中貼上 $ node index.js，就可以收到 server 的回覆 架一個簡易 http server\r使用 Node.js 的內建模組 http，\n1var http = require(\u0026#39;http\u0026#39;) 2var server = http.createServer(handelRequest) 3function handelRequest(req, res) { 4 if(req.url === \u0026#39;/\u0026#39;){ //根據 request 的資源 5 res.write(\u0026#39;welcome!\u0026#39;) //給 相對應的 response 6 res.end() 7 return 8 } 9 if(req.url === \u0026#39;/hello\u0026#39;) { 10 res.write(\u0026#39;hello\u0026#39;) 11 res.end() 12 return 13 } 14 if(req.url === \u0026#39;/redirect\u0026#39;) { 15 res.writeHead(302, { //如果 url 是 /redirect 的話則轉址到 /hello 16 \u0026#39;Location\u0026#39; : \u0026#39;/hello\u0026#39; 17 }) 18 res.end() 19 return 20 } 21 res.writeHead(404) 22 res.end() 23} 24server.listen(5000) //port $ node index.js 執行後，開啟瀏覽器輸入 127.0.0.1:5000，便可以看到 response 的結果。\n第 14 行，如果網址結尾是 /redirect 的話，轉到其他位址。\n第 21 行，代表如果帶錯誤的 url 的話，回復 404 status code。\nReference\r[1]https://techthatmatter.com/best-fastest-bsnl-dns-servers/\n[2]https://hackmd.io/@monkenWu/Sk9Q5VoV4/https%3A%2F%2Fhackmd.io%2F%40gen6UjQISdy0QDN62cYPYQ%2FH1yxwXyNN?type=book\n","date":"2020-07-01T10:24:00+08:00","updated":"2020-07-01T10:24:00+08:00"},{"objectID":"1593559800","permalink":"/post/network-tcp-udp/tcp-upd/","title":"[TCP/IP] 傳輸層 - TCP , UDP","content":"傳輸層 Transport Layer\rPort\r連接埠/端口。有了網際網路層的 IP 位址之後，就可以把檔案傳輸到目標電腦上，但是一台電腦上可能有許多不同的應用程式，沒辦法知道這個檔案要由電腦上的哪一個應用程式處理，因此出現了 port。使用 port 就可以區別這些服務，如果有服務的程式在監聽這個埠號的話，就會收到 request。\n常用的 port 有： http 80, https 443, ftp 21。\nTCP\rTransmission Control Protocol 傳輸控制協定，TCP 為可靠的傳輸協定，通訊前須先建立「連線」，連線完成後進行「通訊」，通訊完畢後「中斷連線」，應用如 http, ftp。\n建立連線 - 三次交握\rClient 傳送建立連線訊息給 Server，裡面有個包個數字 x (例如 1000)。 Server 收到後，將這個 x 記錄下來，然後回發個訊息 y (例如 8000) 以及ack(x+1)， x + 1 的目的是因為這樣才能證明是 Server是收到 Client 的訊息) Client 收到後，也回應個 ack(y+1)（同理這樣才能證明是 Server 寫的） 經過以上三次交握，連線建立。 傳送訊息\rClient 發送資料長度為 20 byte 的資料給 Server Server 收到後回 ack 為 x + 20 的訊息給 Client Client 再次發送資料長度為 20 byte 的資料給 Server 過一段時間 timeout 後，Client 再發送一次 Server 收到後回 ack 為 x + 20 的訊息給 Client 結束連線 - 四次揮手\rClient 發送一個斷線訊息Finish \u0026amp; x 給 Server Server 收到後回發 x + 1 訊息給 Client 接下來 Server 接續發送一個斷線訊息 Finish \u0026amp; y 給 A Client 收到後再回發一個 y + 1 訊息給 Server (Server 就正式斷線) Client 正式斷線，資源釋放 UDP\rUser Datagram Protocol，使用者資料元協定，UDP 為不可靠的傳輸協定，使用在要求速度的應用上，不在乎是否送達，實例如視訊，掉了一兩個畫面不影響功能。\nReference\rhttps://ithelp.ithome.com.tw/articles/10205476\r","date":"2020-06-30T23:30:00+00:00","updated":"2020-06-30T23:30:00+00:00"},{"objectID":"1593559200","permalink":"/post/network-ip/tcpip-ip/","title":"[TCP/IP] 網路層 - IP","content":"網路層 Network Layer\rIP\rInternet Protocol，網絡上每一個節點都有一個獨立的 IP Address，當裝置連接網路時會被分配 IP，用以標識。通過 IP 位址，裝置間可以互相知道對方並通訊。\n有 IPv4 及 IPv6 兩個版本，IPv6 是為了要解決 IPv4 不足的問題。\n固定 IP　v.s. 浮動 IP\r固定 IP 指使用的 IP 都是同一個，不會因為斷線再重新連線而改變，適合給線上服務的伺服器使用．便於提供一些固定連線的服務(Web Server, Domain Name Server, Mail Server\u0026hellip;)；\n浮動 IP 指每次連接時 IP 都會不同，通常給對 IP 位址不要求的一般用戶使用。且因為 IP 每次變動，比較不容易被駭客鎖定，相較於固定 IP 安全。\n實體 IP v.s. 虛擬 IP\r實體 IP 又稱 Public IP，指的是可以在網路上溝通的 IP。在網路的世界裡，用實體IP 辨識每一部電腦的位置。 虛擬 IP 又稱 Private IP，被設計用來解決實體 IP 不足的問題。透過 IP 分享器，讓多台電腦各自擁有虛擬 IP，並對外共用一個實體 IP。 預留的三個虛擬 IP 網段：\nA Class：10.0.0.0 - 10.255.255.255 B Class：172.16.0.0 - 172.31.255.255 C Class：192.168.0.0 - 192.168.255.255 [all]\nNAT\r擁有私有 IP 的內部使用者到要到網際網路需經過網路位址轉換(Network address translation, NAT)的機制，這個機制會把網路封包內容的虛擬IP變成真實IP。\n[1]\nDNS\rDomain Name Server，處理 Domain Name 與實際 IP 的轉換。\n[2]\nReference\r[1]\rconfiguring-a-nat-device-when-external-users-want-to-access-an-internal-server\r[2]\rbest-fastest-bsnl-dns-servers/\r[all] 記此篇為觀看 Lidemy NET101 的筆記，圖片來源以及部分內容取自上課影片\n","date":"2020-06-30T23:20:00+00:00","updated":"2020-06-30T23:20:00+00:00"},{"objectID":"1593558240","permalink":"/post/network-tcpip/tcpip-4layer/","title":"[TCP/IP] 網路基礎概念- 傳紙條、TCP/IP 四層概述","content":"網路\r網路的目的就是為了溝通，可以把整個網路運作想像成傳紙條。\n一開始單純的傳紙條\r只需：\n寫明來源 (client IP) 寫明目的地 (server IP) 經過三次前置作業 (發送端發訊息、接收端接訊息並回復、發送端收到訊息)，確保雙方都能收發。(TCP 三次交握) 把動作標準化\r標準化內容格式，分為 header \u0026amp; body header：擴充性強，放額外資訊。 body：放主要的訊息。 用狀態碼標準化 server 回應結果 (200, 301, 404, 502\u0026hellip;) 用動詞標準化 client 要求的動作 (GET, POST, PUT, DELETE\u0026hellip;) 進入規模化\r愈來愈多的服務下，使用 port 將不同服務標示開來\n一個 port 負責一個服務 (http:80, ftp:21\u0026hellip;) 不同服務有不同訊息格式 (比如 GET 不須帶 body，POST 要帶 request body) 有些不需要使用者回傳訊息的服務，意即不需要經過三次確認 (UDP) protocol\r協定就是一個標準，為了要讓彼此能夠溝通而建立的一個規範，有了標準就可以規模化讓它變成在不同作業系統、環境下的一個共同的準則。\nTCP/IP\r網路的層級，理論上有標準的 OSI 七層。但實際上網路的實作，通常都以 TCP/IP 四層模型為主流，每一層都負責不同的事情、有不同的通訊協定。\n[1]\n上層的協定都是建立在下層之上的，舉例 http 建立在 TCP 之上，TCP 又建立在 IP 之上\u0026hellip;以此類推。\nTCP/ IP - 鏈結層 Link Layer\r位於整個網路架構的最底層，負責制定資料傳輸的「實體」規格。所謂「實體」，就是我們眼睛看的到，手可以摸到的部分。以乙太網路來說，凡是接頭規格、傳輸線材種類、MAC 實體位址，都是鏈結層負責的內容。\nInfo\n剩下的三層，網路層、傳輸層、以及應用層將分為後面三篇文章紀錄。\nsource\r[1]http://linux.vbird.org/linux_server/0110network_basic.php\n[all] 記此篇為觀看 Lidemy NET101 的筆記，圖片來源以及部分內容取自上課影片\n","date":"2020-06-30T23:04:00+00:00","updated":"2020-06-30T23:04:00+00:00"},{"objectID":"1593265140","permalink":"/post/programming-js-es6-and-babel/js-es6-and-babel/","title":"[Javascript] ES6 新語法 \u0026 babel 轉譯器簡介","content":"ECMAScript\rECMAScript\r是一個開發標準(規範)，而 Javascript 就是根據此標準來實作。 ES6 指的是 ECMAScript 第六版，於 2015 年發布，所以又稱 ES2015。ES6 包含了許多新的語法，將於下面用 Javascript 介紹。\n新的變數宣告方式\rconst：用於宣告不會變的常數。 但如果今天宣告的常數是物件的話，因為物件的底層存的是位址，所以即使宣告的方式為 const 但物件內的屬性值是可以改變的。 let：使用 let 宣告的變數的生存範圍 (scope) 僅限於變數所在的 block 中。 1function test() { 2 if (10 \u0026amp;gt; 5) { 3 var a = 10 4 let b = 20 5 console.log(b) 6 } 7 consle.log(a) 8} b 的生存範圍僅限於 if 區塊中；而 a 的生存範圍則是整個 function。\nInfo\n作用域愈小愈好，才不會干擾到其他人，所以比較建議使用 let。\n更方便的串接字串 - Template Literals\r字串串接使用 ` ` 取代原先使用的單引號 \u0026#39; \u0026#39; 或是雙引號 \u0026amp;quot; \u0026amp;quot;。\n1//以前 2var name = \u0026amp;#39;jack\u0026amp;#39; 3var str = \u0026amp;#39;hello\u0026amp;#39; + name + \u0026amp;#39;, it is\u0026amp;#39; + new Date() + \u0026amp;#39;now!\u0026amp;#39; 4var str1 = \u0026amp;#39;hello\\nworld\\nhello\\nJavascript\u0026amp;#39; 5 6//ES6 7var newStr = `hello ${name.toUpperCase()}, it is ${new Date()} now!` 8var newStr1 = `hello 9world, 10hello 11Javascript!` 以前在使用單雙引號串接字串的時候，如果遇到換行或是要跟字串相接的狀況時需要使用加號串，很麻煩！\n現在 ES6 提供反引號來串接，當遇到變數的時候只需要在反引號內使用 ${變數} 或 ${javascript code} 包起來，就可以不必再打一大堆加號跟引號了。\n解構 Destructuring\r陣列或物件的元素可以使用 …","date":"2020-06-27T13:39:00+00:00","updated":"2020-06-27T13:39:00+00:00"},{"objectID":"1593123060","permalink":"/post/programming-node-jest-ut/node-unit-test-jest/","title":"[Node.js] 單元測試 \u0026 Jest 簡介","content":"單元測試\r單元測試（Unit Testing），是針對專案中每一個單一功能做測試。一般專案裡的最小單位是一個 function，而通常我們在寫完程式後都是用 console.log 來確認呼叫的結果：\n1//index.js 2function repeat(str, n){ 3 var result = \u0026#39;\u0026#39; 4 for(var i = 0; i \u0026lt; n; i++){ 5 result += str 6 } 7 return result 8} 9 10console.log(repeat(\u0026#39;a\u0026#39;,5) === \u0026#39;aaaaa\u0026#39;) //輸出 true Jest\rJest 是一套現成測試的框架\n$ npm install jest --save-dev 下載 將要測試的程式 export 成 module 1//index.js 2//上半部不變，刪掉原先的測試用的 console.log 這行，加上： 3exports.repeat = repeat 新增 jest 測試檔案， 使用\u0026lt;NAME\u0026gt;.test.js 當作測試檔檔名 1// index.test.js 2var repeat = require(\u0026#39;./index.js\u0026#39;) 3test(\u0026#39;repeat(\u0026#39;a\u0026#39;,5)應該要等於\u0026#39;aaaaa\u0026#39;\u0026#39;, () =\u0026gt; { 4 expect(repeat(\u0026#39;a\u0026#39;, 5).toBe(\u0026#39;aaaaa\u0026#39;)); 5}); 其中 () =\u0026gt;{} 是 function(){} 的縮寫 4. 在 package.json 的 script 中加上 jest\n1\u0026#34;script\u0026#34;:{ 2 \u0026#34;test\u0026#34; : \u0026#34;jest\u0026#34; 3} 下 $ npm run test 就會自動去找附檔名為 .test.js 的檔案並跑測試\n或是只是想測試單一的檔案，則在 script 中指明檔案即可\n1\u0026#34;script\u0026#34;:{ 2 \u0026#34;test\u0026#34; : \u0026#34;jest repeat.test.js\u0026#34; 3} 加上 describe 歸納相同性質的測試\r補充\r如果直接在 terminal 下 jest index.test.js 指令的話，會出現 command not found 的錯誤訊息。因為 jest 只安裝在專案底下，而下在 terminal 是去你的系統找，所以會有錯誤。 使用 package.json 的 script 來下指令外還可以使用 npx 來達成直接在 terminal 下 jest 的功能，$ npx jest repeat.test.js。\nTDD, 測試驅動開發\rTest-driven Development，一種開發方法，會先寫出測試程式，在依預期的測試結果去開發程式。\nSource\r[All] 此篇為觀看 Lidemy JS102 的筆記，圖片來源以及部分內容取自上課影片\n","date":"2020-06-25T22:11:00+00:00","updated":"2020-06-25T22:11:00+00:00"},{"objectID":"1593122580","permalink":"/post/programming-node-basic/node-module-require-and-exports/","title":"[Node.js] module、require \u0026 exports","content":"Module/Package/Liberary\r當程式裡面有很多個功能，可以把它拆開成一個一個的模組，之後才方便維護，因為彼此的相依性被獨立開來了，而且同一個功能不用一直重寫，只要把 module 匯入就好。\n另外還可以在程式中使用別人寫好的 module，以減少開發時間。\n在程式裡使用 node module - require\rrequire 中直接寫要引入的 module 名稱，然後取一個變數給它。\n1var os = require(\u0026#39;os\u0026#39;) 2var http = require(\u0026#39;http\u0026#39;) 如果是自己寫的非官方 module，在 require 時沒有加路徑的話會先從本身檔案的同路徑下找 (./)，沒有找到的話會去 node-module 資料夾裡找。\n把 module 借給別人用 - exports\r1// app.js 2function double(n){ 3 return n*2 4} 5 6module.exports = double 在其他程式呼叫時，就是 module.exports 輸出的東西，意即 module.exports 丟什麼，require 時的變數就會是什麼東西。\n1// index.js 2var double = require(\u0026#39;./app.js\u0026#39;) 3console.log(double(3)) //輸出 6 其中：\nrequire 的檔案要指名路徑 可省略副檔名(一般都是不打居多) 但在大部分的場合下我們不會只輸出一個東西(function , variable\u0026hellip;)，所以用以下兩種方式\nmodule.exports={物件}\r1module.exports={ 2 double: double 3 triple: function(n){ 4 return n*3 5 } 6} 在 require 端就可以：\n1var app = require(\u0026#39;./app\u0026#39;) 2console.log(app.double(2)) 3console.log(app.triple(3)) exports.\u0026lt;輸出的東西\u0026gt;\r1exports.double = double 2exports.triple= function(n) { 3 return n*3 4} 在 require 端就可以：\n1var app = require(\u0026#39;./app\u0026#39;) 2console.log(app.double(2)) 3console.log(app.triple(3)) Info\n使用 export. 輸出的東西，都會是物件！ 而使用 module.export 輸出可以是數值、陣列、函式、或物件。\nSource\r[All] 此篇為觀看 Lidemy JS102 的筆記，圖片來源取自上課影片\n","date":"2020-06-25T22:03:00+00:00","updated":"2020-06-25T22:03:00+00:00"},{"objectID":"1593103440","permalink":"/post/programming-node-npm/node-npm-intro/","title":"[Node.js] npm 簡介","content":"npm\rNode Package Manager，package 就是別人寫好的套件，npm 就是幫你管理 Node.js 套件的管理系統。裝完 Node.js 就會自動把 npm 裝好，下 npm -v 可以查看版本。\nnpm init\r第一次使用 npm 套件的話下 npm init 初始化，填寫專案相關資訊後，就會自動產生 package.json 的檔案。\npackage.json\r如果在程式中使用很多套件，專案就會變得非常的大，在搬遷或是想 git push 分享的話會變得很麻煩。此時只需要一個用來紀錄專案中使用的套件列表以其版本，就能解決。 當別人拿到你的專案的時候，只要下 $ npm install 就會自動安裝完所有所需套件了！\ndependencies：紀錄專案使用了哪些套件。 devDependencies：紀錄開發中用的套件。 script： 可以在下各種 npm　指令時依照 script 做一些事情。例如說一個程式的進入點除了可以在專案描述的時候特別指出 main．另外就是在 script 寫自動執行入口程式的腳本。 之後只要下 $ npm run start 就能跑起程式了，npm run start 算是約定俗成的入口程式指令。\nnpm install\r接著只要在專案目錄下，下 npm install \u0026lt;moduleName\u0026gt; 安裝套件。裝完後目錄下會多了一個 package-lock.json \u0026amp; node_modules/。\npackage-lock.json 裡記錄了安裝的套件中各自又依賴了哪些套件。 node_modules/ 就是存放各套件程式碼的資料夾，通常會很大、不會被推到 git server，要 git pull 下來使用的人只需用 package.json 透過 npm install 來安裝指定套件。 p.s. 如果是在子資料夾下 npm install，則會去記錄在上一層專案目錄下的 package.json 中，如果堅持要把 package 裝在子目錄下的話，就在子目錄下下 npm init 即可。 \u0026ndash;save 自動幫你加進 package.json\r開發的途中只要隨時有新的套件想安裝的話，不需要手動去更改 package.json 的檔案，只需要在 npm install 的時候加上 --save 的參數即可。\n1$ npm install http-server --save 2$ npm install left-pad --save-dev //自動記錄在 devDependencies 區域．僅供開發時用到的 module Warning\n現在新版的 npm 不用加 \u0026ndash;save 參數也會自動 save 到 package.json 了。\nyarn：除了 npm 的另一種選擇\ryarn 比 npm 還快，是一個新出來、由 facebook 開發的開源套件管理。\n裝完 yarn 後，下 $ yarn -v 可以查看版本。 $ yarn init 初始化專案 使用 $ yarn add \u0026lt;module name\u0026gt; 安裝套件，且會自動在 package.json 檔案中更新 直接下 $ yarn 就能根據 package.json 裝完所需套件 Source\r[All] 此篇為觀看 Lidemy JS102 的筆記，圖片來源取自上課影片\n","date":"2020-06-25T16:44:00+00:00","updated":"2020-06-25T16:44:00+00:00"},{"objectID":"1592693760","permalink":"/post/programmimg-node-eslint-husky/eslint-husky/","title":"[Node.js] eslint \u0026 husky 簡介","content":"eslint (npm module)\reslint 等於 es + lint，用來檢查 Javascript coding style 的工具。\nes = ECMAScript, Javascript 的標準 lint = 一個語法檢查的工具 husky (npm module)\rhusky 讓我們能在 package.json 配置 git hook 腳本。\n使用 npm install 完成套件安裝 eslint + husky後，就會在每次 commit 前自動檢查程式碼是否符合規範，符合才能提交成功。通常用在團隊開發時，可以統一程式碼風格。\n不符合規範時會在 console 印出錯誤訊息，\n前面的數字代表第幾行：後面的數字代表該行的幾個字發生錯誤。 通常都從後面開始修，因為有些錯誤是要加空行，從前面改的話會導致後面的行號會亂掉。如果真的亂掉的話，那只要在重新下 git commit 一次，讓 eslint 重新檢查。\n如果想讓 eslint 忽略某些檢查的話，例如要忽略可以宣告沒有被使用的變數，只要在最前面加上註解： // eslint-disable no-unused-vars\nSource\r[All] 此篇為觀看 Lidemy MTR04 的筆記，圖片來源取自上課影片\n","date":"2020-06-20T22:56:00+00:00","updated":"2020-06-20T22:56:00+00:00"},{"objectID":"1592602920","permalink":"/post/programming-lidimy-alg101-note/lidimy-alg101-note/","title":"新手刷題前應該知道的事","content":"這篇文章是 Lidemy Mentor Program 第二周觀看 ALG101 先別急著寫 Leetcode 的筆記。\n新手寫題目怎麼開始\r先想解法 在把解法轉成程式碼 而不是著急的直接邊寫程式碼邊解，要先有一個架構之後再去想怎麼轉換成 code。 舉例：印出 1~100 的奇數\n想解法 印出一到一百 印的時候判斷是不是奇數 把解法換成程式碼 利用虛擬碼 psedu code來表示程式碼\nWarning\n縮排的重要性！縮牌等於一個區塊，例如 if-end if 代表一個區塊，大大的增加程式可讀性。\n需要練習當看到程式碼時，腦中要能知道程式的流程。 可以想想看如果把 max 的初始值設成 arr[0] 的話，arr 在 [7, 5] 的狀況下答案會是對的嗎?\n善用 log \u0026amp;amp; debugger\r用 Debugger 可以讓程式一行一行的跑 code，方便新手了解程式執行流程並找出哪一行有錯。 關於 Chrome Devtool Debbuger 的使用之前就有特別記錄成一篇\r文章\r。\n範圍很重要\r因為不同輸入範圍代表不同限制，同一個題目可能輸入範圍不一樣，解法就會不同。解題時可能會碰到的三種限制：\n空間限制 1一般程式語言中的 int： 4 bytes；double：8 bytes 2JS 中的 Number：8 bytes 當今天程式要使用到一百萬個數字，則大約需要 7.6 MB；十億個數字，則大約需要 7.4 GB 的空間，若要排序一百萬個數字可能還可以宣告陣列來存，但如果換成十億筆的話就需要使用其他方式，比如說先放在檔案，再取出比較。 2. 時間限制 3. 型態限制 不同型態可以儲存的範圍不同，例如 Number.MAX_SAFE_INTEGER 是 Javascript 可以儲存的最大安全數值，如果超過此數，就沒辦法保證正確性，如下範例：\n另外還有浮點數的精準度問題\n遇到困難時\r請利用函式填空法以及簡化法，先避開題目的細節，把架構先做出來。當你把架構寫出來後，會發現剩下的東西其實沒那麼困難。如下圖範例，多了函式，整體架構會看起來更清楚。\n一些上課學到的小技巧\r1function total(){ 2\tif(total%10===0){ 3\treturn true 4\t}else{ 5 return false 6\t} 7} 可以轉換成 return …","date":"2020-06-19T21:42:00+00:00","updated":"2020-06-19T21:42:00+00:00"},{"objectID":"1592433300","permalink":"/post/programming-js-basic-3/js-basic-3/","title":"[Javascript] 函式、內建函式","content":"函式 functiton\r基本函式結構\n1function 函式名稱(參數){ 2 return 回傳值 3} 傳入參數可以一個或多個，可以是值、變數、陣列、物件或是函式 回傳值可以零個或一個，可以是一個值、變數、陣列、物件或是函式 呼叫函式直接打 函式名稱() ；如果有帶參數，將引數置於括號中 函式名稱(引數) 參數 Parameter v.s. 引數 Argument\r1funciton add(a,b){ 2 console.log(arguments) //{\u0026amp;#39;0\u0026amp;#39;:2, \u0026amp;#39;1\u0026amp;#39;:5} 3 console.log(arguments[0]) //2 4 return a+b 5} 6add(2,5) 其中 a, b 為 function add 的參數；2, 5 為引數。 Javascript 提供了一個引述的變數 arguments，可以在 function 內使用。\n小補充\rarguments 是一個物件，但你可能會疑惑為什麼可以用類似陣列的方式 arguments[0] 存取，因為上一篇\r文章\r物件有提到，存取物件屬性值可以用物件.屬性名或是物件[\u0026#39;屬性名\u0026#39;]，而 argument[\u0026amp;lsquo;0\u0026amp;rsquo;] 中的字串\u0026#39;0\u0026#39;，可以直接用數字 0 表示，所以就會看起來想存取陣列的表示方式了。\n不同的函式宣告方式\r直接宣告一個具名的 function 1function hello(){ 2 console.log(\u0026amp;#39;hello\u0026amp;#39;) 3} 宣告變數等於一個匿名 function 1var hello = function(){ 2 console.log(\u0026amp;#39;hello\u0026amp;#39;) 3} function 當參數傳入另一 function 1function print(anything){ 2 anything() 3} 4function hello(){ 5 console.log(\u0026amp;#39;hello\u0026amp;#39;) 6} 7print(hello) 1function trans(x, passInFunc){ 2 return passInFunc(x) 3} 4function double(x){ 5 return x*2 6} …","date":"2020-06-17T22:35:00+00:00","updated":"2020-06-17T22:35:00+00:00"},{"objectID":"1592432580","permalink":"/post/programming-js-basic-2/js-basic-2/","title":"[Javascript] 條件句、迴圈","content":"if/else statement\r使用 if/else 來判斷狀況，用法如下：\nif\u0026amp;hellip;\r可加入運算於判斷條件句內\n1var num = 13 2if(num % 5 === 0){ 3 console.log(\u0026amp;#39;num is multiple of five\u0026amp;#39;) 4} if\u0026amp;hellip;else\r判斷如果 if 相符做什麼事，不相符則則做 else 區塊。\n1var score = 70 2if( score\u0026amp;gt;=60 \u0026amp;amp;\u0026amp;amp; score\u0026amp;lt;100){ 3 console.log(\u0026amp;#39;pass\u0026amp;#39;) 4}else{ 5 console.log(\u0026amp;#39;failed\u0026amp;#39;) 6} if\u0026amp;hellip;else if\u0026amp;hellip;else\r多條件判斷，先判斷 if、再判斷 else if(可以多個 else if 判斷)，最後都不相符的話執行 else 區塊。\n1if( score\u0026amp;gt;=60 \u0026amp;amp;\u0026amp;amp; score\u0026amp;lt;100){ 2 console.log(\u0026amp;#39;pass\u0026amp;#39;) 3}else if(score===100){ 4 console.log(\u0026amp;#39;excellent\u0026amp;#39;) 5}else{ 6 console.log(\u0026amp;#39;failed\u0026amp;#39;) 7} 補充\r小條件通常都用 () 括起來；區塊通常都用 {} 括起來 num % 5 === 0　這個判斷 num 是否為 5 的倍數的判斷句，可以改寫成 !(num % 5)，但 num % 5 === 0 會比較直觀，如果無關乎效能的話．建議使用比較好懂得語句去寫 code。 switch case\r當判斷的條件非常多的時候，除了使用 else if 外，還可以使用 switch case 取代。\n1var g = 1 2switch(month){ 3 case 1: 4 console.log(\u0026amp;#39;Jan\u0026amp;#39;) 5 break 6 case 2: 7 console.log(\u0026amp;#39;Feb\u0026amp;#39;) 8 break 9 case 3: 10 case 4: 11 console.log(\u0026amp;#39;Mar\u0026amp;#39;) 12 break 13 default: 14 …","date":"2020-06-17T22:23:00+00:00","updated":"2020-06-17T22:23:00+00:00"},{"objectID":"1592334180","permalink":"/post/programming-js-basic-1/js-basic-1/","title":"[Javascript] runtime、運算子、變數","content":"Javascript Runtime\rJavascript 需要有執行環境（Runtime）才能執行，最早期只能跑在瀏覽器上面，我們會透過 Javasrcipt 去操控瀏覽器畫面上的東西，例如表單驗證… 等等。\n但 Node.js 誕生後，就可以當成是 JS 除了瀏覽器外的 runtime，下載 Node.js 後便可以直接在 CMD 內下指令\n$ node \u0026amp;lt; fileName.js \u0026amp;gt;\n不過兩個執行環境可以操控的東西不是完全一樣的，例如瀏覽器提供了 JS 操控文件的 API document，Node.js 提供 JS 操控檔案的 API fs module，這兩個 API 對於另外一個 runtime 都沒有。 但還是有共同都有的，例如 console，但呈現方式不同，瀏覽器的 console 如下圖，Node.js 的 console 就是 cmd 本身。\n運算子的種類\r算數運算\r運算 運算子 加 + 減 - 乘 * 除 / 餘數 % 遞增 ++ 遞減 \u0026amp;ndash; i++ 跟 ++i 的差別\r把 ++ 放後面代表 ++ 為後運算，如\n1var i=0; 2console.log(i++ \u0026amp;amp;\u0026amp;amp; 30); 會先執行 console.log(i \u0026amp;amp;\u0026amp;amp; 30)，再執行 i + 1。\n指派\r運算 運算子 示例 賦予值 = i = i + 1，將 i+1 指派回 i 加法指派 += i += 1 意即 i = i + 1 以此類推。\n邏輯運算\r運算 運算子 AND \u0026amp;amp;\u0026amp;amp; OR || NOT ! 邏輯的短路性質\r3 || 10 回傳 3；0 || 10 回傳 10。 第一個運算元是 true 的話，就回傳第一個參數（短路），否則傳第二個。\n3 \u0026amp;amp;\u0026amp;amp; 10 回傳 10；0 \u0026amp;amp;\u0026amp;amp; 10 回傳 0。 第一個運算元是 true 的話，由第二個決定，第一個運算元是 false 的話，直接回傳第一個 false（短路）。\np.s. 在 Javascript 中，0、false 都代表 false。\n位元運算\r將運算元變成二進位後，對每個位元(bit)做操作。\n運算 運算子 示例 結果 補充 左移 \u0026amp;laquo; 8 \u0026amp;laquo; 1 16 左移就是乘以二 右移 \u0026amp;raquo; 1024 …","date":"2020-06-16T19:03:00+00:00","updated":"2020-06-16T19:03:00+00:00"},{"objectID":"1592173680","permalink":"/post/programming-lidimy-cs101-note/lidimy-cs101-note/","title":"程式初學者基礎知識 - CLI, 計概, 網路, session \u0026 cookie, 演算法","content":"這篇文章是紀錄程式實驗導師第一周觀看課程 CS101 的筆記，主要闡述了程式初學者該具備的基礎知識，包含 Coding 簡介、前後端介紹，以及一些計算機概論。\n到底寫程式是什麼\rCoding 的目的就是要跟電腦溝通，要對電腦下指令，讓電腦照著指示做。\n那為什麼需要程式碼呢？ 因為電腦只看得懂以 0 和 1 組成的機器語言，且它針對各種不同的事都只按照同一個標準去做事（標準化），例如不同廠牌的 USB 須按照 USB 標準去生產，而電腦只須懂這個標準就好。 所以寫程式的標準就是程式碼，而市面上的各種程式語言最終都會轉換成電腦懂得機器語言。\n程式碼不是重點，解決問題才是\r把問題拆解，並試著把解法寫成條列式，一行就是一個動作，例如：找單字裡面有沒有包含字母 P，有的話位置是第幾個？ 在經過將無限重複的步驟有限化並加上結束判斷、將提到的變數命名一個代號、將答案回傳等都納入後會得到下面的步驟：\n什麼是 Command Line\r與之對應的是圖形化介面 GUI (Graphic User Interface)，GUI 指的是用看得到的圖形介面去操作電腦。而 Command Line Interface 指的是只能透過文字跟電腦做溝通的介面。 問題是，GUI 這麼直覺好用，為什麼還要有 CLI 呢？因為\u0026amp;mdash;\n有些功能只能用 CLI 有時候用文字去操作指令比較快 這邊羅列幾個常用到的 Command：\npwd：print working directory，列出當前所在位置 ls：list Segment 印出檔案清單，加上 -al 參數可以列出隱藏檔案 (-a) 並把清單詳細條列 (-l)。 cd：change directory 切換資料夾，$ cd .. 代表回到上一層，$ cd ~ 代表回到 home 目錄 (使用者的專屬文件目錄)，$ cd / 代表回到根目錄 (存放電腦底層檔案)。 man：manual 可以叫出指令的使用手冊，例如 $ man ls 列出 ls 的使用方法。 touch：$ touch \u0026amp;lt;fileName\u0026amp;gt;，如果檔案不存在，則會建立一個新檔案；如果檔案存在，則會修改檔案時間。 rm：$ rm \u0026amp;lt;fileName\u0026amp;gt; 刪除檔案，$ rm -r \u0026amp;lt;fileName\u0026amp;gt; 刪除資料夾。 mkdir：$ mkdir …","date":"2020-06-14T22:28:00+00:00","updated":"2020-06-14T22:28:00+00:00"},{"objectID":"1592045100","permalink":"/post/devops-git-basic-3/git-basic-3/","title":"[Git] Basic Git (3) 一些狀況劇","content":"想改最後一次的 commit message\r在本地端 git commit 後發現 commit message 打錯了，只要下 $ git commit --amend即可進入 vim 編輯器做修改。\n請注意，如果已經 commit 而且又 push to remote 了，那就乖乖認命吧，這種情形下你在 local 端改的話可能會造成其他人的困擾。最好的方法還是 push 之前先檢查一下，避免錯的東西被放到遠端。 commit 後但後悔了\r使用 $ git reset HEAD^ (--soft / --hard / --mixed)\n名詞 解釋 head 所在位置 ^ 上一個 index 變更狀態紀錄 (git status) working directory 工作目錄 mode head index working directory 說明 soft changed unchanged unchanged 僅移除 commit 變成新版未 commit，內容仍是新版的。 mixed(default) changed changed unchanged index 移除 staged 標記，變成 Modifiedor Untracked，內容是新版的。 hard changed changed changed 回到上一版版本，完全移除，內容及狀態皆是上一版。 [1]\n改了檔案還沒 commit 但想復原\r用 $ git restore \u0026lt;file\u0026gt; 回復，或是 $ git restore .回復所有檔案 或是舊 command $ git checkout -- \u0026lt;file\u0026gt;\n想改 branch 名稱\r$ git branch -m \u0026lt;新名稱\u0026gt;\n想把遠端的 branch 抓下來\r設目前本地端無任何 branch，可以直接下 $ git checkout \u0026lt;remote-branch-name\u0026gt;。\n想在 commit 前做一些判斷 (git hooks)\r在 .git 資料夾下有 hooks 資料夾，裡面存放了一些 shell script，可以讓使用在在針對某些狀況下設置一些判斷，然後 git 做一些反應，比如說在 commit 或是 push 前檢查是否有放帳號密碼等資訊，然後停止或允許動作。\nSource\r[1] https://ithelp.ithome.com.tw/articles/10187303\r[All] 大部分的內容皆來自 Lidemy\r[GIT101] 的課堂筆記\n","date":"2020-06-13T10:45:00+00:00","updated":"2020-06-13T10:45:00+00:00"},{"objectID":"1592039880","permalink":"/post/devops-git-basic-2/git-basic-2/","title":"[Git] Basic Git (2) - Branch \u0026 GitHub","content":"branch 概念\r一般在線性開發時會是以下這樣：\n當在開發新功能時，發現當下開發的版本有舊有的 bug，此時如果一邊開發一邊改 bug 可能會導致產出的東西有衝突。\n而如果引入分支 branch，讓開發新功能以及 debug 兩邊各自獨立完成，而後再進行合併，就可以把工作乾淨地切割開來。目的是為了保持主枝幹的穩定，因為在開發新功能的時候不可能隨時保持穩定狀態，所以在確定穩定前都不會影響到主幹。\ngit branch -v\r可以查看目前有哪些 branch，顯示方式為 \u0026amp;lt;branch name\u0026amp;gt; \u0026amp;lt;latest commit version\u0026amp;gt; \u0026amp;lt;commit message\u0026amp;gt;，亮綠燈的代表目前工作目錄所在的分區。\ngit branch\r$ git branch \u0026amp;lt;branchname\u0026amp;gt; 可依目前 branch 為基準複製出一個新的分支。\ngit checkout\r$ git checkout \u0026amp;lt;branchname\u0026amp;gt; 可以將目前工作目錄切換到指定 branch 下，可以把 checkout 想像成移到該資料夾下的感覺。\ngit merge\r$ git merge \u0026amp;lt;branchname\u0026amp;gt; 可以把指定的 branch 合併到當下的工作目錄來。如下圖，在 new-feature branch 下修改了 hello.js 的檔案，回到 master branch，先 cat 確認 hello.js 內容是舊的，再下 merge command 將 new-feature branch 合併到 master 後，就能發現 master 的檔案也更新了！\ngit branch -d\r$ git branch -d \u0026amp;lt;branchname\u0026amp;gt; ，-d 為 \u0026amp;ndash;delete，可刪除該 branch。通常在開發完該 branch 並合併進主支線後，就可以刪除分支以保持乾淨的專案內容。\n處理 merge 後的 conflict\r如果 branch 在合併的時候有檔案有衝突，意即同個檔案的檔案內容的同一行不一樣，git 不曉得該以哪一個為準，則會在 merge 時出現下面的訊息：\n須先查看發生衝突的檔案\nGit 把有衝突的段落標記出來了，上半部是 HEAD，也就是目前所在的 master 分支， …","date":"2020-06-13T09:18:00+00:00","updated":"2020-06-13T09:18:00+00:00"},{"objectID":"1592032500","permalink":"/post/devops-git-basic-1/git-basic-1/","title":"[Git] Basic Git (1) - Intro \u0026 Init, add, commit, status... ","content":"前言\r在沒有版本控制系統前，如果有一個常常在修改的檔案，又想保留每個版本狀態時，我們常會在編輯檔案前複製一個備份，時間久了就會變得非常不便且難以維護，有可能造成命名混亂，也很難比較各版本間的差異，尤其是在多人協作的狀況下時還可能會發生衝突。　版本控制概念－用一般資料夾闡述\r如果是以資料夾去做版控的概念會是如下\n1. 需要新版本：開一個新的資料夾 2. 不想加入版控：不要加入資料夾 3. 避免版本號衝突：用看似亂數的東西當作資料夾名稱 4. 知道資料夾順序：用一個檔案來紀錄 5. 知道最新版本：用一個檔案來存 GIT 是什麼\rgit 就是個幫你做版控的程式，不用像上面那樣做很多動作！\n開始使用\rgit init\r進入要做版控的資料夾/專案，並下 $ git init 做初始化。初始化完成後可以看到目錄下多了一個 .git 的隱藏子資料夾，其中包含 Git 所有必需的倉儲檔案，也就是 Git 倉儲的骨架。\ngit status\r用於查看目前工作目錄 (working directory) 的檔案狀態，以下是檔案可能會有的四種狀態：\n未追蹤 (Untracked)：沒有被 GIT 所追蹤控管的檔案，如新增的檔案。 已更改 (Modified)：已提交版本後，又再次修改的檔案。 等待提交 (Staged)：在工作目錄 (WD) 的檔案執行 git add 後，會放在暫存區 (Stage) 等待提交。 已提交 (Committed)：在暫存區的檔案執行 git commit 後，檔案便置於儲存區 (Repo)，這些放在儲存區的檔案即是已提交的狀態。 第一次下 $ git status 可以發現目前目錄下的檔案狀態皆為 Untracked files，表示這些是全新的檔案，沒有被加入版控。\ngit add\r輸入 $ git add \u0026amp;lt;filename\u0026amp;gt; 可將檔案指定加入版控 / 暫存區 (Stage)；\n輸入　$ git add . 可一次加入 所有 檔案 再查看狀態可以看到檔案狀態變為 Changes to be committed。\ngit commit\r輸入$ git commit 將建立新的版控，將放在暫存區的檔案放入 repository 如果單獨輸入$ git commit，則會先進入 vim 編輯器，要求在編輯器輸入 commit 訊息。\n或是下$ …","date":"2020-06-13T07:15:00+00:00","updated":"2020-06-13T07:15:00+00:00"},{"objectID":"1591822320","permalink":"/post/data-kafka-cluster/kafka-cluster/","title":"[Kafka] 建立 Kafka Cluster","content":"1$ cd /opt 2$ wget \u0026#34;http://ftp.tc.edu.tw/pub/Apache/kafka/2.4.1/kafka_2.12-2.4.1.tgz\u0026#34; 3$ sudo tar zxvf kafka_2.12-2.4.1.tgz 4$ sudo mv kafka_2.12-2.4.1 kafka 建立儲存 log 的資料夾，並指定權限\n1$ sudo mkdir /var/lib/kafka 2$ sudo chown -R $(USER id -u):$(USER id -g) /opt/kafka 3$ sudo chown -R $(USER id -u):$(USER id -g) /var/lib/kafka 編輯server.properties設定檔\n1$ sudo vi /opt/kafka/config/server.properties 修改以下參數\n1#指定 cluster 中的 broker 的唯一 id 2broker.id=0 3 4#指定監聽 ip \u0026amp; 阜號 5listeners=INSIDE://[private ip]:9092,OUTSIDE://[private ip]:9094 6advertised.listeners= INSIDE://[private ip],OUTSIDE://[public ip]:9094 7listener.security.protocol.map= INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT 8inter.broker.listener.name= INSIDE 9 10#指定存放log的路徑 11log.dirs=/var/lib/kafka 12 13#指定預設的分區數 14num.partitions=3 15 16#允許使用者刪除 topic 17delete.topic.enable = true 18 19#加入zookeeper 的位置 20zookeeper.connect=10.0.0.4:2181,10.0.0.5:2181,10.0.0.6:2181 將KAFKA設為服務\n1$ sudo vi /etc/systemd/system/kafka.service 加入以下文字\n1[Unit] 2Requires=zookeeper.service 3After=zookeeper.service 4 5[Service] 6Type=simple 7User=root 8ExecStart=/bin/sh -c \u0026#34;/opt/kafka/bin/kafka-server-start.sh /opt/kafka/config/server.properties \u0026gt; /opt/kafka/kafka.log 2\u0026gt;\u0026amp;1\u0026#34; 9ExecStop=/opt/kafka/bin/kafka-server-stop.sh 10Restart=on-abnormal 11 12[Install] 13WantedBy=multi-user.target 啟動 kafka\n1$ sudo systemctl start kafka 2$ sudo systemctl enable kafka ","date":"2020-06-10T20:52:00+00:00","updated":"2020-06-10T20:52:00+00:00"},{"objectID":"1591699380","permalink":"/post/data-kafka-intro/kafka-at-glance/","title":"[Kafka] Kafka at a Glance","content":"Kafka 是由 LinkedIn 開發而後釋出給 Apache 的一個開源 streaming message queue (消息佇列)，由Scala和Java編寫，目前是 Hadoop ecosystem 中的一員。\n特色\r以下簡單列出 Kafka 幾點特色：\n基於發布與訂閱 (Publish \u0026amp; Subscribe) 架構中有三個角色，broker, producer 以及 consumer Broker 為訊息系統本身，負責接收並儲存資料 Producer 負責發布消息到 broker Consumer 負責從 borker 訂閱消息 Producer 以及 Consumer 於 broker 來說是 client 多個 brokers 可以組成一個 cluser，叢集具有負載平衡以及方便擴展而不會影響運作的特性 Kafka 在儲存、發布以及訂閱訊息時是透過 Topic 進行歸類 每一個 Topic 可包含一個或多個 Partition，partition 是資料實際儲存的地方，增加 partition 可以提高吞吐量 可以透過設定副本 replication-factor 達到高可靠性 資料儲存在硬碟，可指定存放的時間或是大小，達到持久性 page cache\rLinux內核會將系統中所有的空閒記憶體全部當做 page cache 來用，而page cache 中的所有 page 數據將一直保存在 page cache 中直到 CPU 根據特定的算法替換掉它們中的某些 page。 當 produce 資料到 Kafka 時，會先將數據寫入 Page Cache 中，並將該 page 上有 dirty 標誌。當 consumer 向 Kafka 讀取資料時，會先在 Page Cache 中查找內容，如果有就直接返回，沒有的話就會從 disk 讀取文件再寫回 Page Cache。 因此只要生產者與消費者的速度相差不大，消費者會直接讀取之前生產者寫入 Page Cache 的數據，大家在記憶體內裡完成接力，沒有磁碟訪問。 比起在記憶體中維護一份消息數據的傳統做法，這既不會重複浪費一倍的記憶體，Page Cache 又不需要 GC，而且即使 Kafka 重啟了，Page Cache 還依然在。\n","date":"2020-06-09T10:43:00+00:00","updated":"2020-06-09T10:43:00+00:00"},{"objectID":"1591644600","permalink":"/post/coldplay-everglow/","title":"[Coldplay] Everglow","content":"Oh, they say people come\nSay people go\n他們說人來人去\nThis particular diamond was extra special\n這特別的鑽石格外特別\nAnd though you might be gone\nAnd the world may not know\n儘管你可能消失，且世界也可能不會知道\nStill I see you, celestial\n我在天國仍然看得見\nLike a lion you ran\nA goddess you rolled\n你像獅子一樣跑著、像女神一般的展現\nLike an eagle you circle\nIn perfect purple\n你像老鷹一樣盤旋著、高貴的完美無缺\nSo how come things move on?\n所以事情會怎麼發展?\nHow come cars don\u0026rsquo;t slow?\n為什麼車子不減速?\nWhen it feels like the end of my world\n什麼時候像我的世界末日?\nWhen I should, but I can\u0026rsquo;t, let you go?\n什麼時候是我應該放手但我不能放手時?\nBut when I\u0026rsquo;m cold, cold\nOh, when I\u0026rsquo;m cold, cold\n但是當我冷的時候\nThere\u0026rsquo;s a light that you give me\n是你給我那盞燈\nWhen I\u0026rsquo;m in shadow\n當我在暗處的時候\nThere\u0026rsquo;s a feeling you give me, an everglow\n是你給我的這個感覺，一道永恆之光\nLike brothers in blood\nSisters who ride\n像親兄弟姊妹般並肩而行\nAnd we swore on that night\nWe\u0026rsquo;d be friends \u0026rsquo;til we die\n在那晚我們發誓我們將成為朋友且至死不渝\nBut the changing of winds\nAnd the way waters flow\n儘管風在變、水在流\nLife is short as the falling of snow\n生命短得如落雪般\nAnd now I\u0026rsquo;m gonna miss you, I know\n此刻我知道我將會思念你\nBut when I\u0026rsquo;m cold, cold\n當我冷的時候\nIn water rolled, salt\n在水中翻湧、且鹹澀無比\nI know that you\u0026rsquo;re always with me\nAnd the way you will show\n我知道你永遠會在我身邊，且用你將會展現的方式\nAnd you\u0026rsquo;re with me wherever I go\n無論我去哪裡你將會伴隨\nAnd you give me this feeling\nThis everglow\n你給我的這個感覺、是永恆之光\nOh, what I would give for just a moment to hold\n我能給的只是短暫一瞬\nYeah, I live for this feeling, it\u0026rsquo;s everglow\n是的，我為這種感覺而活，就是這個永恆之光\nSo if you love someone\nYou should let them know\n所以如果你愛某個人，你應該讓他們知道\nOh, the light that you left me will everglow\n那你留下的這道光便是永恆之光\nNote\n為什麼有這麼美的歌! 😠\n","date":"2020-06-08T19:30:00+00:00","updated":"2020-06-08T19:30:00+00:00"},{"objectID":"1588280100","permalink":"/post/data-build-zookeeper-cluster/zookeeper-build-cluster/","title":"[Zookeeper] 建立 Zookeeper Cluster","content":"Zookeeper 提供分散式應用程式中的協調服務，是 Hadoop 生態系產品中不可或缺的角色。 Zookeeper 叢集適合搭建在奇數臺機器上，目的是為了提高可用性以及維持選舉制度的運行。[1]\nStep 1: Install Java JDK\r1$ sudo apt-get update 2$ sudo apt-get install openjdk-8-jdk 3$ java -version 4$ sudo vi /etc/environment 1JAVA_HOME=\u0026#34;/usr/lib/jvm/java-8-openjdk-amd64\u0026#34; 2JRE_HOME=\u0026#34;/usr/lib/jvm/java-8-openjdk-amd64/jre\u0026#34; 3PATH=\u0026#34;${PATH}:${JAVA_HOME}/bin:${JRE_HOME}/bin\u0026#34; 使環境變數生效\n5$ source /etc/environment 6$ env Step 2: Install zookeeper\r1$ cd /opt 2$ wget \u0026#34;https://archive.apache.org/dist/zookeeper/zookeeper-3.6.1/apache-zookeeper-3.6.1-bin.tar.gz\u0026#34; 3$ sudo tar zxvf apache-zookeeper-3.6.1-bin.tar.gz 4$ sudo mv apache-zookeeper-3.6.1-bin zookeeper 5$ sudo adduser nexcom root 6$ chown -R $USER:root /opt/zookeeper 建立存放 zk config 及其資料的 directory\n7$ mkdir /var/lib/zookeeper 8$ chown -R $USER:root /var/lib/zookeeper 設定 zoo.cfg 配置檔\n9$ sudo cp /opt/zookeeper/conf/zoo_sample.cfg ./zoo.cfg 10$ vi /opt/zookeeper/conf/zoo.cfg 在配置檔中修改以下參數\n1dataDir=/var/lib/zookeeper 2clientPort=2181 3 4#加入其他 zookeeper 節點的 ip, 第一個 port 用於 follower 連接 leader, 第二個 port 用於節點選舉 5server.1=10.0.0.4:2888:3888 6server.2=10.0.0.5:2888:3888 7server.3=10.0.0.6:2888:3888 建立 deamon，將 zookeeper 設為一服務\n11$ sudo vi /etc/systemd/system/zookeeper.service 在 zookeeper.service 中加入以下文字\n1[Unit] 2Description=Zookeeper Daemon 3Documentation=http://zookeeper.apache.org 4Requires=network.target 5After=network.target 6 7[Service] 8Type=forking 9WorkingDirectory=/opt/zookeeper 10User=root 11ExecStart=/opt/zookeeper/bin/zkServer.sh start /opt/zookeeper/conf/zoo.cfg 12ExecStop=/opt/zookeeper/bin/zkServer.sh stop /opt/zookeeper/conf/zoo.cfg 13ExecReload=/opt/zookeeper/bin/zkServer.sh restart /opt/zookeeper/conf/zoo.cfg 14TimeoutSec=30 15Restart=on-failure 16 17[Install] 18WantedBy=default.target 修改 zkServer.sh 以避免在 telnet 時發生 stat is not executed because it is not in the whitelist. Connection closed 的錯誤\n12$ sudo vi /opt/zookeeper/bin/zkServer.sh 在 zkServer.sh 中加入以下這一行\n1ZOOMAIN=\u0026#34;-Dzookeeper.4lw.commands.whitelist=* ${ZOOMAIN}\u0026#34; 在每台節點上設定 Zookeeper 識別用唯一 id\n13$ echo \u0026#34;1\u0026#34; \u0026gt; /var/lib/zookeeper/myid 啟動 zookeeper, 並設為開機自動啟動\n14$ sudo systemctl start zookeeper 15$ sudo systemctl enable zookeeper 下一篇將介紹 Kafka 叢集的安裝。\nsource\r[1] https://medium.com/@bikas.katwal10/why-zookeeper-needs-an-odd-number-of-nodes-bb8d6020e9e9\r","date":"2020-04-30T20:55:00+00:00","updated":"2020-04-30T20:55:00+00:00"},{"objectID":"1586725980","permalink":"/post/programming-js-debugging-in-chrome/js-debugging-in-chrome/","title":"[Javascript] Debugging in Chrome","content":"Debugging 是指在一個腳本中找出錯誤。市面上大宗瀏覽器都支持開發人員工具，能幫助開發者一步步地追蹤代碼，查看當前實際運行情況。 此篇文章以常用的 Google Chrome 做說明－\n進入開發人員工具\r按 F12 或是右上角選單中的 更多工具 \\ 開發人員工具 開啟面板。\n通常 debugging 最常使用到的是 source 以及 console 面板。\nConsole\r控制台，可以輸入命令執行，以及用來輸出資料(如 console.log 將顯示於此)。\nSource\rSources 頁面包含三個部分：\n文件導航 (File Navigator)：列出了所有依附於此頁面的文件。 代碼編輯 (Code Editor)：展示程式碼。 JavaScript Debugging： debugging 操作\r按鈕 操作 描述 Resume 繼續執行直到下一個斷點。如果沒有遇到斷點，則繼續正常執行。 Long Resume 繼續執行，將斷點停用 500 毫秒。便於暫時跳過斷點，否則會持續暫停執行代碼，例如，循環內的斷點。點擊並按住 Resume，直到展開以顯示操作。 Step Over 運行下一條指令，但不會進入到函數中，會在無形中執行函數調用，跳過函數內部。執行會在該函數執行後立即暫停。如果我們對該函數的內部執行不感興趣，這命令會很有用。 Step Into 如果下一行包含一個函數調用，Step Into 將跳轉並在其第一行暫停該函數。 Step Out 函數調用後，執行當前函數剩餘部分，然後在下一個語句暫停。 Step 運行下一條語句。一次接一次地點擊，整個腳本的語句會被逐個執行。 Deactivate breakpoints 暫時停用所有斷點。用於繼續完整執行，不會真正移除斷點。再次點擊以重新啟用斷點 Activate breakpoints 啟用斷點 debugging 狀態\rWatch\n顯示任意表達式的當前值。可以點擊加號 + 然後輸入一個表達式。便會隨時顯示它的值，並在執行過程中自動重新計算該表達式。(同在 scope 中看到的變數) Call Stack\n檢視目前堆疊上的函式或程序呼叫。 Scope\n顯示當前的變量。 Local 顯示當前函數中的變量；Global 顯示全局變量（不在任何函數中）。 斷點\n在程式碼中設定進入中斷點有許多好處， …","date":"2020-04-12T21:13:00+00:00","updated":"2020-04-12T21:13:00+00:00"},{"objectID":"1584215280","permalink":"/post/devops-git-server/build-git-server-on-ubuntu/","title":"[Git] Build a Git Server on Ubuntu","content":"Step 1: 電腦/VM 環境設置\r作業系統選擇 Ubuntu 或其他 Linux 版本 開啟 SSH port 新增使用者帳號 git 1$ sudo adduser --home /home/git --disabled-password git 2//--disabled-password 表示取消用密碼登入 Note\nsmall tips: 若開發團隊僅有兩三人，那不建立 git 帳號而是建立使用者個別的帳號也行。但考慮到日後可能擴編或是人員交替等等的因素，可能會增加管理的負擔。\nStep 2: 安裝所需套件\r安裝 git 1$ sudo apt-get install git 安裝 Gitosis 1$ sudo apt-get install python-setuptools 2$ cd /tmp 3$ git clone https://github.com/tv42/gitosis.git 4$ cd gittosis 5$ sudo python setup.py install 1承第一步結尾，為了使用及管理上的方便，Gitosis 便應運而生，Gitosis 是一套用來管理授權金鑰文件和實現簡單連接限制的腳本。可以輕鬆的管理每個使用者，而不需實際在主機上新增/移除帳號，這樣 user 皆可用 git@server:ServerIP.git 這樣的格式來取得或上傳版本庫到Server上。 在使用者家目錄 /home/git/ 下新增資料夾 repositories 當作版本庫位置 Gitosis 預設會把 /home/git 這個目錄裡面的 repositories 資料夾當成所有 repository 的根目錄，以後所有的專案就都放在裡面。\nStep 3: git client 端的動作\r每個要使用 git server 的使用者，將會擁護一把自己的金鑰，若同一使用者使用多台電腦，則需將同一把的私鑰複製到同路徑下便可連線。\n開啟 git bash 產生一對金鑰 1$ ssh-keygen -t rsa -C \u0026amp;#34;xxx@email.com\u0026amp;#34; 2// -C 參數用於指定這個金鑰的識別碼，預設是「帳號@主機名稱」，但為了好管理，所以建議用使用者的唯一識別值如 email 輸入金鑰存放位置 若沒輸入則存在預設位置 輸入密碼 這裡不需 …","date":"2020-03-14T19:48:00+00:00","updated":"2020-03-14T19:48:00+00:00"},{"objectID":"1581629520","permalink":"/post/programmimg-nginx-certbot/ssl-letsencrypt/","title":"[SSL] 用 Let's Encrypt \u0026 Certbot 為網站加密","content":"在講求資訊安全的時代，大部分的網站幾乎都使用 https 做為網站的通訊協定，這篇文章將記錄怎麼使用免費的第三方憑證 Let\u0026amp;rsquo;s Encrypt \u0026amp;amp; Certbot，為網站添加安全保障。\nLet\u0026amp;rsquo;s Encryt\rLet’s Encrypt 是一個免費、自動化且開放的憑證機構 (Certificate Authority, CA)，取得憑證後，可為網站提供 SSL/TLS 加密。\nLet’s Encrypt 使用 ACME 協定，來驗證申請網域控制權。使用者透過 ACME 客戶端軟體取得並管理憑證。最常見的 ACME 客戶端軟體為 Certbot\nNote\nACME (Automatic Certificate Management Environment): 自動憑證管理環境。由 Let\u0026amp;rsquo;s Encrypt 所實作的協議，與它相容的軟體可以透過此協議與 Let’s Encrypt 溝通以獲得憑證。[1]\n實作\r在 Certbot\r官方網站有快速指引，只需要選擇網站使用的 HTTP Server 以及作業系統，下方就會列出指令，可以直接複製並安裝。\n本文以 Ubuntu 18.04 + Nginx HTTP Server 實作。\n1. 加入Certbot PPA\rNote\nPPA(Personal Pakage Archives)：個人軟體包文件，可加入個人開發者的 repository，使其他使用戶安裝和更新。\n請依序在 console 中下以下 command\n1// 檢查更新 2$ sudo apt-get update 3 4// 安裝套件管理的套件 5$ sudo apt-get install software-properties-common 6 7// 啟用universe倉庫 8$ sudo add-apt-repository universe 9 10// 加入 certbot ppa repository 11$ sudo add-apt-repository ppa:certbot/certbot 12 13// 再更新一次套件資訊 14$ sudo apt-get update 2. 安裝 Certbot\r1$ sudo apt-get install certbot …","date":"2020-02-13T21:32:00+00:00","updated":"2020-02-13T21:32:00+00:00"},{"objectID":"1580666700","permalink":"/post/os-introduction/os-introduction/","title":"[OS] 作業系統概論","content":"程式、程序、執行緒\r程式(Program)\r尚未載進記憶體的靜態程式碼集合。\n程序(Process)\r正在執行並載進記憶體中的動態程式，是作業系統分配資源的最小單位（OS 以程序為單位，分配系統資源，OS 下的工作管理員所標示的一個個 task 即為一個程序，擁有獨立的 PID 以及記憶體空間）。\n執行緒(Thread)\r又稱 Light-weight Process (LWP)，是 OS 中進行運算排程的最小單位，被包在一個 Process 中，而同一 Process 下的各個執行緒之間共享該 Process 資源。\n三者的關係是：一個程式可以會有多個程序，一個程序可能會有多個執行緒。 併發(concurrency) v.s. 並行(parallelism)\r併發\r同時執行多個獨立的程式邏輯，但並不代表同時進行處理。 若程式執行在單核單執行緒的 CPU 上，所有任務都須排隊等待 CPU 資源。\n並行\r讓程式真正的同時處理多個任務，並行並不是程式能夠帶來的特性，而是需要靠硬體，需仰賴多核 CPU 或是使用多台伺服器組成叢集。\n兩者的關係是：並行的狀況一定會併發，但發生併發時卻不一定是並行。 Multiprograming、Multiprocessing、Multitasking、Multithreading\rMultiprogramming\r電腦同時(concurrency)執行多個程式。（如同時使用 excel 以及 firefox）\nMultiprocessing\r電腦同時使用多個處理器，多個程序可在同一時間(parallelism)執行。\nMultitasking\r多個任務共享一個資源(如一顆 CPU 或記憶體)。\nMultithreading\r指一個程序中有多個執行緒在執行，彼此共用相同的程序資源。\n雖然字面上看起來意思接近，但每個名詞都有些微的差距，其中特別記錄在其他網站中看到針對 multitasking and multiprogramming 的闡述＿\n1Multitasking is a logical extension of multi programming. The major way in which multitasking differs from multi programming is that multi programming …","date":"2020-02-02T18:05:00+00:00","updated":"2020-02-02T18:05:00+00:00"},{"objectID":"1578168120","permalink":"/post/programming-golang-function/golang-function/","title":"[Golang] Function","content":"Introduction\r在 Go 裡，function 是一等公民，它可以作為另一 function 中的參數傳遞、可以將它指派給一變數、也可以在另一個 function 中當作 return 值。\n函式可以具名也可以匿名(anonynous function)；\n一般函式\nfunc (r receiverType) identifier(arguments argumentTypes) returnType{}\nfunction中的參數以及返回值可以有零個或多個：\n1\\\\傳入零個argument, 無回傳值 2func f1(){ 3\tfmt.Println(\u0026#34;hello!\u0026#34;) 4} 5 6\\\\傳入兩個arguments, 回傳兩個return values 7func f2(x, y int) (int, int) { 8 return x + y, x - y 9} 10 11\\\\傳入未知個數的多個arguments 12func f3(x ...int) { 13\tfor _, v := range x{ 14 fmt.Println(x) 15 } 16} 匿名函式為沒有名字的函式\nfunc(arguments) returnType{}\n以下將針對function作為一等公民的實踐做闡述。\nCallback 回呼函式\r定義 把function A當作參數傳進另一個function B內，故在執行B的時候會callback回去參考A。 以下範例先定義一個用於加總的函式sum，再定義另一個用於計算奇數總和的函式odd_sum，在odd_sum中丟入sum當作參數去計算總合，實現callback。\n1package main 2 3import ( 4\t\u0026#34;fmt\u0026#34; 5) 6 7func main() { 8\ti := []int{1, 2, 3, 4, 5, 6, 7} 9\toddsum := odd_sum(sum, i...) //把sum函數當參數傳入 10\tfmt.Println(\u0026#34;The sum of the odd number in the list I is\u0026#34;, oddsum) 11} 12 13func sum(x ...int) int { 14\ttotal := 0 15\tfor _, v := range x { 16\ttotal += v 17\t} 18\treturn total 19} 20 21 22//使用func(x ...int) int 當函式傳入, 函式變數為f 23func odd_sum(f func(x ...int) int, y ...int) int { 24\toddlist := []int{} 25\tfor _, v := range y { 26\tif v%2 == 1 { 27\toddlist = append(oddlist, v) 28\t} 29\t} 30\treturn f(oddlist...) 31} Nested function 內嵌函式\r定義 在 Go 中，不允許在function A中又宣告另一個function B，但可以把匿名的function B指定給某變數並放在function A中。 在func main宣告變數f為匿名函式：\n1package main 2import \u0026#34;fmt\u0026#34; 3 4func main(){ 5\tx := 0 6\tf := func() int{ 7\tx++ 8\treturn x 9\t} 10\tfmt.Println(f()) 11} closure 閉包\r定義 當匿名函數中使用了外部變數x，此時這個匿名函式形成一個閉包。所以閉包可以視為匿名函式的特殊案例之一。 乘上範例，當重複呼叫f()時，會發現x的值存留在函式中：\n1\tfmt.Println(f()) 2+\tfmt.Println(f()) 3+\tfmt.Println(f()) 4} 執行結果\n11 22 33 為了更實踐閉包數據隔離的優勢，我們將x與匿名函數移出main function外獨立成為一個函式\n其中我們使用匿名函式func() int{\u0026hellip;}作為function bar的回傳值\n1package main 2import \u0026#34;fmt\u0026#34; 3 4func main(){ 5\tf := foo() 6 fmt.Println(f()) 7\tfmt.Println(f()) 8 fmt.Println(f()) 9} 10 11func foo() func() int{ 12\tx := 0 13\treturn func() int { 14\tx++ 15\treturn x 16\t} 執行結果\n11 22 33 此時即使我們執行完foo()，變數x仍然存在在閉包中，外部無法訪問汙染。\n","date":"2020-01-04T20:02:00+00:00","updated":"2020-01-04T20:02:00+00:00"},{"objectID":"1577738460","permalink":"/post/golang-array-and-slice/golang-array-and-slice/","title":"[Golang] Array v.s. Slice","content":"Array\r陣列的長度固定，且為陣列類型的一部份。 （[4]int 以及 [5]int 為兩種獨立且不相容的不同type）\n宣告及初始化array\r1 2// var [len]Type，無初始化值，則初始值為zero value 3var arr1 [2]int 4 5// var [len]Type，並給初始值 6var arr2 = [2]int{1, 2} 7 8// var [...]Type，讓complier自行計算長度 9var arr3 = [...]int{1, 2} 10 11// := [len]Type，在func中使用簡短宣告符號，無給初始值 12arr4 := [2]int{} 13 14// := [len]Type，並給初始值 15arr5 := [2]int{} 16 17// := [...]Type，讓complier自行計算長度 18arr6 := [...]int{1, 2} 當賦值和傳遞array時，是複製整個陣列內容\n1package main 2 3import ( 4\t\u0026amp;#34;fmt\u0026amp;#34; 5) 6 7func main() { 8\tarr1 := [2]string{\u0026amp;#34;apple\u0026amp;#34;, \u0026amp;#34;banana\u0026amp;#34;} 9\tarr2 := [2]string{} 10 11\tarr2 = arr1 12 13\tfmt.Printf(\u0026amp;#34;arr1: %p, %v \\n\u0026amp;#34;, \u0026amp;amp;arr1, arr1) 14\tfmt.Printf(\u0026amp;#34;arr2: %p, %v \\n\u0026amp;#34;, \u0026amp;amp;arr2, arr2) 15 16\tarr3(arr1) 17} 18 19func arr3(x [2]string) { 20\tfmt.Printf(\u0026amp;#34;arr3: %p, %v \\n\u0026amp;#34;, \u0026amp;amp;x, x) 21} 執行結果\n1arr1: 0x40a0e0, [apple banana] 2arr2: 0x40a0f0, [apple banana] 3arr3: 0x40a120, [apple banana] Array 缺點\r上面三個array的位址皆不相同，驗證傳遞時都是複製整個陣列\n如此，可發現使用array的兩個弊處\n1. array是固定長度, 使用起 …","date":"2019-12-30T20:41:00+00:00","updated":"2019-12-30T20:41:00+00:00"},{"objectID":"1577595960","permalink":"/post/programming-golang-method/golang-method/","title":"[Golang] Method","content":"\rInfo\nGolang is all about TYPE!\n定義\rmethod是一個額外帶有receiver(接收器)的fuction，receiver為某type的變量，而type的型態可以是struct或是non-struct\n語法如下\n1func (r receiverType) identifier(parameter) returnTypes {} 範例如下\n1\ttype student struct { 2\tname string 3\tschool int 4\t} 5\t6\tfunc (s student) greet() string { 7\treturn fmt.Sprintf(\u0026#34;Hi, I\u0026#39;m %v from %v.\u0026#34;, s.name, s.school) 8\t} 呼叫 method\rtype的變量則透過 . 呼叫method：\n9 10func main() { 11\ts1 := student{\u0026#34;Alice\u0026#34;, 112} 12\ts2 := student{\u0026#34;Bob\u0026#34;, 118} //s1 \u0026amp; s2為student type 的變數 13\t14\tfmt.Println(s1.greet()) 15\tfmt.Println(s2.greet()) //透過variable.method呼叫 16\t} Pointer v.s. Value as a receiver\rmethod中的receiver可以是值，也可以是pointer，若為pointer，則可以直接修改receiver中的內容。\n📢 使用pointer傳遞的時機 📢\nWarning\n用於欲改變receiver變數的值時 struct 本身很大，複製代價高時 當struct的method有一個receiver為pointer時，則其餘的method的receiver也必須使用pointer 以下範例展現傳值(value)及傳址(pointer)的差異，請留意此範例打破了上面的第三點原則，僅是為了要顯示兩者的異同：\n1\tpackage main 2\t3\timport \u0026#34;fmt\u0026#34; 4\t5\ttype student struct { 6\tname string 7\tschool int 8\t} 9\t10\tfunc (s student) transByValue(x int) string { 11\ts.school = x 12\treturn fmt.Sprintf(\u0026#34;Hi, I\u0026#39;m %v from %v.\u0026#34;, s.name, s.school) 13\t} 14\t15\tfunc (s *student) transByInference(x int) string { 16\ts.school = x 17\treturn fmt.Sprintf(\u0026#34;Hi, I\u0026#39;m %v from %v.\u0026#34;, s.name, s.school) 18\t} 19\t20\tfunc main() { 21\ts1 := student{\u0026#34;Alice\u0026#34;, 112} 22\t23\tfmt.Println(s1.transByValue(118)) 24\tfmt.Println(s1.school) 25 26\tfmt.Println((\u0026amp;s1).transByValue(117)) 27\tfmt.Println(s1.school) 28\t29\tfmt.Println(s1.transByInference(113)) 30\tfmt.Println(s1.school) 31\t32\tfmt.Println((\u0026amp;s1).transByInference(114)) 33\tfmt.Println(s1.school) 34\t35\t} 執行結果\n1Hi, I\u0026#39;m Alice from 118. 2112 3Hi, I\u0026#39;m Alice from 117. 4112 5Hi, I\u0026#39;m Alice from 113. 6113 7Hi, I\u0026#39;m Alice from 114. 8114 由上述範例中，可以發現到：\nWarning\n🔎 當使用value作為receiver傳遞時，函數僅copy了值，並無改變到變數本身\n🔎 當使用pointer作為receiver傳遞時，是傳遞整個變數，故會改到變數本身\n","date":"2019-12-29T05:06:00+00:00","updated":"2019-12-29T05:06:00+00:00"},{"objectID":"1577116560","permalink":"/post/hexo-font/","title":"[Hexo] NexT 更改字體","content":"Blog最重要的組成為文章，所以第一篇就決定來記錄我怎麼改網站的文字字體。\n使用的方法是載入Google Font API -\nGoogle Font\r是一個包含近千種開源字體的字型庫，使用者除了可以免費下載外，還能在網站上自動產生程式碼，將字體內嵌到自己的網站上使用。\n目前網站上僅提供兩種繁體中文，思源黑體以及思源宋體：\n以下為步驟：\n選定字型\r進入官網選擇喜歡的字型，依照網站樣式需求可以選擇多個，如文章內文中文欲使用思源宋體、英文欲使用Lato\u0026hellip;\n請在字體右上角點選 + 加入選擇清單(底部提示框) 在已選擇清單中，切換分頁至 CUSTOMIZE ，勾選font-weight(regular400, bold700\u0026hellip;)以及下方的語言(Latin \u0026amp; Chinese Traditional)\n切回 EMBED 分頁，並複製網站自動編好的HTML code\n編輯本地端文件\r_config.yml\r綠底及紅底分別代表code的增減。需特別注意的是 global 的 family 參數建議輸入英文字體，因為中文字型庫通常會包含英文字，故如若設為中文字體，則網站全域的預設文字包含英文皆會使用指定的中文字體。\n另外下方還有 title 網站標題、 headings 標頭、 posts 文章、 codes 程式碼，可以特別指定字體。\n1# 文件位置：~/blog/themes/next/_config.yml 2 3font: 4- enable: false 5+ enable: true 6 7 # Uri of fonts host, e.g. //fonts.googleapis.com (Default). 8 host: 9 10 # Font options: 11 # `external: true` will load this font family from `host` above. 12 # `family: Times New Roman`. Without any quotes. 13 # `size: x.x`. Use `em` as unit. Default: 1 (16px) 14 15 # Global font settings used for all elements inside \u0026lt;body\u0026gt;. 16 global: 17 external: true 18 family: Lato 19 size: 20 21 # Font settings for site title (.site-title). 22 title: 23 external: true 24 family: Amatic SC 25 size: 26 27 # Font settings for headlines (\u0026lt;h1\u0026gt; to \u0026lt;h6\u0026gt;). 28 headings: 29 external: true 30 family: Lato 31 size: 32 33 # Font settings for posts (.post-body). 34 posts: 35 external: true 36 family: 37 38 # Font settings for \u0026lt;code\u0026gt; and code blocks. 39 codes: 40 external: true 41 family: 而中文字體則到下方文件設置：\nbase.styl\r1# 文件位置: ~/blog/themes/next/source/css/_variables/base.styl 2 3// Font families. 4 5- $font-family-chinese = \u0026#34;PingFang SC\u0026#34;, \u0026#34;Microsoft YaHei\u0026#34;; 6+ $font-family-chinese = \u0026#34;Noto Serif TC\u0026#34;; head.swig\r接著將步驟一Google Font所複製的 EMBED 代碼貼上到 head.swig\n1# 文件位置: ~/blog/themes/next/layout/_partials/head/head.swig 2... 3\u0026lt;link href=\u0026#34;https://fonts.googleapis.com/css?family=Noto+Serif+TC:400,500,700\u0026amp;display=swap\u0026amp;subset=chinese-traditional\u0026#34; rel=\u0026#34;stylesheet\u0026#34;\u0026gt; 最後重新佈署便完成了! 😆\n","date":"2019-12-23T15:56:00+00:00","updated":"2019-12-23T15:56:00+00:00"},{"objectID":"1551312000","permalink":"/about.html","title":"About","content":"是個動靜自如的宅女，養了隻貓，但也是個超級狗派 :3\n興趣是追劇、動漫跟 NBA。順便說一下，喜歡的球星是 Kawhi Leonard 🖐🏾\n雖然宅但很喜歡旅行，去過美東、加拿大、中國、日本和馬來西亞。\n最瘋狂的事就是單攻玉山跟單車環島~\n這個部落格是我用來記錄技術筆記，以及一些重要生活紀錄的地方！\n","date":"2019-02-28T00:00:00+00:00","updated":"2019-02-28T00:00:00+00:00"}]