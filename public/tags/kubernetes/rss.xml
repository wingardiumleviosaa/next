<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Kubernetes on UlaGraphy</title><link>https://ulagraphy.netlify.com/tags/kubernetes/</link><description>Recent content in Kubernetes on UlaGraphy</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Sun, 23 Jul 2023 17:45:00 +0800</lastBuildDate><atom:link href="https://ulagraphy.netlify.com/tags/kubernetes/rss.xml" rel="self" type="application/rss+xml"/><item><title>[Ingress] 指定了 TLS 憑證，卻吃到不正確的</title><link>https://ulagraphy.netlify.com/post/k8s-ssl-not-eat-right/</link><pubDate>Sun, 23 Jul 2023 17:45:00 +0800</pubDate><guid>https://ulagraphy.netlify.com/post/k8s-ssl-not-eat-right/</guid><description>&lt;h2 id="tl-dr">TL; DR
&lt;a class="header-anchor" href="#tl-dr">&lt;/a>
&lt;/h2>&lt;p>在公司部屬 Ingress 資源後，發現一直沒法法吃到指定的憑證，結果才發現是因為 wildcard 的問題。&lt;/p></description></item><item><title>CronJob to Run the Job (alpine base image) at the Last Sunday of the Month</title><link>https://ulagraphy.netlify.com/post/k8s-cronjob-run-job-alpine-at-last-sunday-of-the-month/cronjob-for-last-day-of-the-month-using-kubernetes/</link><pubDate>Sun, 23 Oct 2022 19:08:00 +0000</pubDate><guid>https://ulagraphy.netlify.com/post/k8s-cronjob-run-job-alpine-at-last-sunday-of-the-month/cronjob-for-last-day-of-the-month-using-kubernetes/</guid><description>&lt;h2 id="tldr">TL;DR
&lt;a class="header-anchor" href="#tldr">&lt;/a>
&lt;/h2>&lt;p>本篇文章記錄如何在 aplpine base 的 container 中於每月的最後一個禮拜日執行指定任務。&lt;/p></description></item><item><title>Minikube Pull Image from Private Repository in WSL2</title><link>https://ulagraphy.netlify.com/post/k8s-minikube-pull-img-from-harbor-in-wsl2/minikube-pull-harbor-image/</link><pubDate>Sun, 23 Oct 2022 19:07:00 +0000</pubDate><guid>https://ulagraphy.netlify.com/post/k8s-minikube-pull-img-from-harbor-in-wsl2/minikube-pull-harbor-image/</guid><description>&lt;h2 id="tldr">TL;DR
&lt;a class="header-anchor" href="#tldr">&lt;/a>
&lt;/h2>&lt;p>在 windows 環境的 wsl2 Ubuntu 上跑了 Minikube 要用來測試 kubernetes 的應用佈屬，但一直卡在拉取公司內部 harbor 鏡像的時候報 x509: certificate signed by unknown authority 錯誤。&lt;/p></description></item><item><title>Run Crond as Non Root in Alpine Container by Pod/Deployment</title><link>https://ulagraphy.netlify.com/post/k8s-run-crond-as-non-root-in-alpine/run-crond-as-non-root-in-alpine-container-by-pod-or-deployment/</link><pubDate>Sun, 23 Oct 2022 19:04:00 +0000</pubDate><guid>https://ulagraphy.netlify.com/post/k8s-run-crond-as-non-root-in-alpine/run-crond-as-non-root-in-alpine-container-by-pod-or-deployment/</guid><description>&lt;h2 id="tldr">TL;DR
&lt;a class="header-anchor" href="#tldr">&lt;/a>
&lt;/h2>&lt;p>在
&lt;a href="https://ulahsieh.netlify.app/p/docker-run-cronjob-as-non-root-user-in-alpine-container/" title="上一篇文章" rel="noopener external nofollow noreferrer" target="_blank" class=" exturl">
上一篇文章
&lt;i class="fa fa-external-link-alt">&lt;/i>
&lt;/a>中試了在 alpine docker container 中使用 non root user 跑 crond，但將 build 好的 docker image 搬到 kubernetes 給 deployment 的 pod 使用時，卻會出現 initgroup operation not permitted 的錯誤。&lt;/p></description></item><item><title>Kubernetes Introduction</title><link>https://ulagraphy.netlify.com/post/kubernets-basic/</link><pubDate>Wed, 07 Sep 2022 21:13:00 +0000</pubDate><guid>https://ulagraphy.netlify.com/post/kubernets-basic/</guid><description>&lt;h2 id="簡介">簡介
&lt;a class="header-anchor" href="#%e7%b0%a1%e4%bb%8b">&lt;/a>
&lt;/h2>&lt;p>官方定義:&lt;/p>
&lt;blockquote class="blockquote-center">
Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.
&lt;/blockquote></description></item><item><title>Kustomize Can’t Render the ConfigMap Hashing Name to CronJob Resource</title><link>https://ulagraphy.netlify.com/post/k8s-kustomize-render-configmap-hashing-name-cronjob-resource/kustomization-render-failed-in-cronjob/</link><pubDate>Tue, 06 Sep 2022 21:02:00 +0000</pubDate><guid>https://ulagraphy.netlify.com/post/k8s-kustomize-render-configmap-hashing-name-cronjob-resource/kustomization-render-failed-in-cronjob/</guid><description>&lt;h2 id="問題">問題
&lt;a class="header-anchor" href="#%e5%95%8f%e9%a1%8c">&lt;/a>
&lt;/h2>&lt;p>在使用 kustomize 配置 Kubernetes 資源時，Kustomization 定義的 ConfigMap 無法正確的渲染到 CronJob 資源中。
原 yaml 檔如下:&lt;/p></description></item><item><title>Helm Migrate stable/nginx-ingress to ingress-nginx</title><link>https://ulagraphy.netlify.com/post/k8s-helm-migrate-stable-nginx-ingress-to-ingress-nginx/helm-migrate-stable-nginx-ingress-to-ingress-nginx/</link><pubDate>Sun, 22 May 2022 21:17:00 +0000</pubDate><guid>https://ulagraphy.netlify.com/post/k8s-helm-migrate-stable-nginx-ingress-to-ingress-nginx/helm-migrate-stable-nginx-ingress-to-ingress-nginx/</guid><description>&lt;h2 id="前言">前言
&lt;a class="header-anchor" href="#%e5%89%8d%e8%a8%80">&lt;/a>
&lt;/h2>&lt;p>原先集群使用的 helm chart 為 stable/nginx-ingress，而此 helm chart 已經被棄用，若 nginx 維持在舊版的話，之後新的漏洞修補都無法被含括。&lt;/p>
&lt;p>此篇記錄如何將集群上面跑的 nginx-ingress-controller 換成新的版本的 chart ingress-nginx/ingress-nginx。&lt;/p></description></item><item><title>[Kubernetes] 停止調度 / 刪除節點</title><link>https://ulagraphy.netlify.com/post/k8s-delete-worker-node/kubernetes-delete-worker-node/</link><pubDate>Thu, 19 May 2022 11:36:00 +0000</pubDate><guid>https://ulagraphy.netlify.com/post/k8s-delete-worker-node/kubernetes-delete-worker-node/</guid><description>&lt;p>cordon、drain 和 delete 三個命令都會使 kubernetes node 停止被調度，本篇記錄如何優雅的刪除節點。&lt;/p></description></item><item><title>Calico Running but Unready (Ready 0/1)</title><link>https://ulagraphy.netlify.com/post/k8s-calico-running-but-unready/calico-running-but-unready/</link><pubDate>Thu, 19 May 2022 08:53:00 +0000</pubDate><guid>https://ulagraphy.netlify.com/post/k8s-calico-running-but-unready/calico-running-but-unready/</guid><description>&lt;h2 id="環境說明">環境說明
&lt;a class="header-anchor" href="#%e7%92%b0%e5%a2%83%e8%aa%aa%e6%98%8e">&lt;/a>
&lt;/h2>&lt;ul>
&lt;li>Kubernetes 1.20.10&lt;/li>
&lt;li>Calico 3.23&lt;/li>
&lt;/ul></description></item><item><title>在 Kubernetes 上佈署 MongoDB</title><link>https://ulagraphy.netlify.com/post/database-mongodb-intro/install-mongodb-on-kubernetes/</link><pubDate>Wed, 20 Apr 2022 19:59:00 +0000</pubDate><guid>https://ulagraphy.netlify.com/post/database-mongodb-intro/install-mongodb-on-kubernetes/</guid><description>&lt;p>紀錄一下 mongodb 的 kubernetes 佈署檔，分為 standalone 以及 replica set，會宣告 persistent volume 以儲存永久性資料 (如 database 資料、index 以及設定檔)。&lt;/p></description></item><item><title>已登入 harbor 但 kubelet 仍會 ImagePullBackOff</title><link>https://ulagraphy.netlify.com/post/k8s-kubelet-pull-image-backoff/image-pull-backoff-after-harbor-login/</link><pubDate>Thu, 14 Apr 2022 19:33:00 +0000</pubDate><guid>https://ulagraphy.netlify.com/post/k8s-kubelet-pull-image-backoff/image-pull-backoff-after-harbor-login/</guid><description>&lt;p>在 kubernetes 環境上拉取私有鏡像倉庫 harbor 的 image 時，一直卡在 ImagePullBackOff 的狀態，decribe pod 發現是權限問題導致拉取失敗。&lt;/p></description></item><item><title>設定 liveness probe 監聽應用以重啟 pod</title><link>https://ulagraphy.netlify.com/post/k8s-setup-liveness-probe-to-restart-pod-when-showup-err-log/setup-liveness-probe-to-restart-when-pod-has-error-log/</link><pubDate>Thu, 31 Mar 2022 22:52:15 +0000</pubDate><guid>https://ulagraphy.netlify.com/post/k8s-setup-liveness-probe-to-restart-pod-when-showup-err-log/setup-liveness-probe-to-restart-when-pod-has-error-log/</guid><description>&lt;p>目前手上有一個監聽 Oracle CDC 的程式跑在以 Debian 為基底的 kubernetes pod 中，會定期因為 Oracle 的錯誤訊息 ORA-12518: TNS 監聽程式無法分發客戶機連線的問題而斷線。此時雖然程式有 error log，但 Pod 的狀態仍然為 Running，只要重啟 Pod 即可重新正常運作。&lt;/p></description></item><item><title>在容器裡 curl Kubernetes API server</title><link>https://ulagraphy.netlify.com/post/k8s-curl-k8s-api-server-within-pod/curl-kubernetes-api-server-within-pod/</link><pubDate>Thu, 31 Mar 2022 21:57:12 +0000</pubDate><guid>https://ulagraphy.netlify.com/post/k8s-curl-k8s-api-server-within-pod/curl-kubernetes-api-server-within-pod/</guid><description>&lt;p>連接 k8s 的 api-server 有三種方式：&lt;/p>
&lt;ol>
&lt;li>Kubernetes Node 通過 kubectl proxy 中轉連接&lt;/li>
&lt;li>通過授權驗證直接連接，例如 kubectl 和各種 client 就是這種情況&lt;/li>
&lt;li>容器內部通過 ServiceAccount 連接&lt;/li>
&lt;/ol>
&lt;p>本文以第三種情況作範例。&lt;/p></description></item><item><title>Kubernetes namespace 一直 delete 不成功的原因 (卡在 terminating status)</title><link>https://ulagraphy.netlify.com/post/k8s-namespace-delete-terminating-status/kubernetes-namespace-delete-terminating/</link><pubDate>Tue, 22 Mar 2022 14:49:00 +0000</pubDate><guid>https://ulagraphy.netlify.com/post/k8s-namespace-delete-terminating-status/kubernetes-namespace-delete-terminating/</guid><description>&lt;p>最近在刪除 namespace 的時候總是會卡在 Terminating 的狀態，一直不疑有他的直接使用網路上常看的解決方法將 spec.finalizers 清空。但因為每次刪、每次卡，就連完全無任何資源的命名空間也是卡！仔細看後才發現原來是有其他元件錯誤，進而造成影響。&lt;/p></description></item><item><title>使用 helm 安裝 Metrics Server</title><link>https://ulagraphy.netlify.com/post/k8s-helm-install-metrics-server/kubernets-install-metrics-server-by-helm/</link><pubDate>Mon, 21 Mar 2022 21:28:00 +0000</pubDate><guid>https://ulagraphy.netlify.com/post/k8s-helm-install-metrics-server/kubernets-install-metrics-server-by-helm/</guid><description>&lt;p>Metrics Server 通過 kubelet（cAdvisor）獲取監控數據，主要作用是為 kube-scheduler、HPA(Horizontal Pod Autoscaler)等 k8s 核心組件，以及 kubectl top 命令和 Dashboard 等 UI 組件提供數據來源，可以用來看 node 或 pod 的資源 (CPU &amp;amp; Memory) 消耗。須注意的是，Metric Server 是 in memory 的 monitor，只可以查詢當前的度量數據，並不保存歷史數據。&lt;/p></description></item><item><title>在 Rocky Linux 8 安裝 Kubernetes 1.23 (containerd as cri)</title><link>https://ulagraphy.netlify.com/post/k8s-1.23-installation/install-kubernetes-123-on-rocky-linux/</link><pubDate>Sun, 13 Mar 2022 14:56:00 +0000</pubDate><guid>https://ulagraphy.netlify.com/post/k8s-1.23-installation/install-kubernetes-123-on-rocky-linux/</guid><description>&lt;p>kubernetes 1.22 版之後，就不再支持 Docker 作為 container runtime 以及管理容器及鏡像的工具了。可以使用 &lt;code>containerd&lt;/code> 取代 docker 的 container runtime；以及 &lt;code>crictl&lt;/code> 作為 CRI(Container Runtime Interface)，另外 podman 也可以用來管理容器和鏡像。本篇記錄基於 containerd &amp;amp; crictl 使用 kubeadm 部屬 Kubernetes 集群的過程。&lt;/p></description></item><item><title>安裝 Kubernetes Dashboard - 單集群可視化管理</title><link>https://ulagraphy.netlify.com/post/k8s-install-kubernetes-dashboard/kubernets-dashboard-installation/</link><pubDate>Sun, 13 Mar 2022 14:46:00 +0000</pubDate><guid>https://ulagraphy.netlify.com/post/k8s-install-kubernetes-dashboard/kubernets-dashboard-installation/</guid><description>&lt;p>Kubernetes Dashboard 是由官方維護的 Kubernetes 集群 WEB UI 管理工具，能查看 Kubernetes Cluster 上資源分佈與使用狀況，也可以創建或者修改 Kubernetes 資源，讓使用者透過 Web UI 介面取代指令的管理 Kubernetes。&lt;/p></description></item><item><title>Ingress does not contain a valid IngressClass</title><link>https://ulagraphy.netlify.com/post/kubernets-ingress-invalid-ingressclass/</link><pubDate>Fri, 11 Feb 2022 06:20:00 +0000</pubDate><guid>https://ulagraphy.netlify.com/post/kubernets-ingress-invalid-ingressclass/</guid><description>&lt;h2 id="問題描述">問題描述
&lt;a class="header-anchor" href="#%e5%95%8f%e9%a1%8c%e6%8f%8f%e8%bf%b0">&lt;/a>
&lt;/h2>&lt;p>nginx ingress 從原本 deprecated 的 stable/nginx-ingress helm chart 改為 ingress-nginx/ingress-nginx chart 後，發現 ingress resource 的 nginx 網頁 404 not found，查看 ingress nginx controller log 發現有 &lt;code>ingress does not contain a valid IngressClass&lt;/code> 的錯誤。&lt;/p></description></item><item><title>Unable to connect to the server: x509: certificate has expired or is not yet valid</title><link>https://ulagraphy.netlify.com/post/kubernets-ca-expired/</link><pubDate>Sun, 21 Nov 2021 17:36:00 +0000</pubDate><guid>https://ulagraphy.netlify.com/post/kubernets-ca-expired/</guid><description>&lt;p>在下 &lt;code>kubectl&lt;/code> 時出現 &lt;code>Unable to connect to the server: x509: certificate has expired or is not yet valid&lt;/code> 的錯誤，原因是 kubernetes apiserver 證書已過期，kubernetes 的 apiServer 與 kubelet 的訪問授權證書是一年，官方表示通過這種方式，讓用戶不斷的升級版本。&lt;/p></description></item><item><title>使用 istio operator 安裝 Istio v1.11</title><link>https://ulagraphy.netlify.com/post/kubernets-install-istio/</link><pubDate>Tue, 26 Oct 2021 20:27:00 +0000</pubDate><guid>https://ulagraphy.netlify.com/post/kubernets-install-istio/</guid><description>&lt;h2 id="下載-istio">下載 Istio
&lt;a class="header-anchor" href="#%e4%b8%8b%e8%bc%89-istio">&lt;/a>
&lt;/h2></description></item><item><title>[Node-RED] Deploy on Kubernetes</title><link>https://ulagraphy.netlify.com/post/k8s-install-nodered/install-nodered-on-kubernetes/</link><pubDate>Tue, 26 Oct 2021 09:44:00 +0000</pubDate><guid>https://ulagraphy.netlify.com/post/k8s-install-nodered/install-nodered-on-kubernetes/</guid><description>&lt;p>原先使用 k8s-at-home 的
&lt;a href="https://github.com/k8s-at-home/charts/tree/master/charts/stable/node-red" title="helm chart" rel="noopener external nofollow noreferrer" target="_blank" class=" exturl">
helm chart
&lt;i class="fa fa-external-link-alt">&lt;/i>
&lt;/a> 部屬，但完成後發現 node 安裝後會 deploy 異常，懷疑是 persistence 設定問題，但又不想花時間深究，所以就直接自己寫 yaml 部屬比較快。&lt;/p></description></item><item><title>Change MetalLB IP Range</title><link>https://ulagraphy.netlify.com/post/kubernets-metallb/</link><pubDate>Thu, 16 Sep 2021 10:30:00 +0000</pubDate><guid>https://ulagraphy.netlify.com/post/kubernets-metallb/</guid><description>&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1&lt;/span>&lt;span>&lt;span style="color:#75715e"># note the old IPs allocated to the services&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2&lt;/span>&lt;span>kubectl get svc
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4&lt;/span>&lt;span>&lt;span style="color:#75715e"># edit config&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5&lt;/span>&lt;span>kubectl edit cm -n metallb-system config
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7&lt;/span>&lt;span>&lt;span style="color:#75715e"># delete the metallb pods&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8&lt;/span>&lt;span>kubectl -n metallb-system delete pod --all
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10&lt;/span>&lt;span>&lt;span style="color:#75715e"># watch the pods come back up&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11&lt;/span>&lt;span>kubectl -n metallb-system get pods -w
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13&lt;/span>&lt;span>&lt;span style="color:#75715e"># inspect new IPs of services&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14&lt;/span>&lt;span>kubectl get svc
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Minikube</title><link>https://ulagraphy.netlify.com/post/kubernets-minikube/</link><pubDate>Sun, 05 Sep 2021 22:17:00 +0000</pubDate><guid>https://ulagraphy.netlify.com/post/kubernets-minikube/</guid><description>&lt;p>minikube 是一個由 Google 發布的部署單節點的 Kubernetes Cluster 的工具，可以安裝在本機上，支援 Windows 與 Mac Minikube 只有一個 Node (節點)。對於本地實驗可以避免節點不足的困擾；讓開發者可以在本機上輕易架設一個 Kubernetes Cluster，快速上手 Kubernetes 的指令與環境。&lt;/p></description></item><item><title>解決 scheduler and controller-manager unhealthy state</title><link>https://ulagraphy.netlify.com/post/kubernets-scheduler-controller-manager-unhealthy/</link><pubDate>Tue, 31 Aug 2021 21:15:00 +0000</pubDate><guid>https://ulagraphy.netlify.com/post/kubernets-scheduler-controller-manager-unhealthy/</guid><description>&lt;h2 id="problem">Problem
&lt;a class="header-anchor" href="#problem">&lt;/a>
&lt;/h2>&lt;p>在嘗試更新 Kubernetes 時，下了下面的 command 取得目前集群的組件狀態：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2;">&lt;code class="language-fallback" data-lang="fallback">&lt;span style="display:flex;">&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>$ kubectl get cs
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>k8s v1.20 nfs-client-provisioner 創建 pvc 時停在 Pending</title><link>https://ulagraphy.netlify.com/post/kubernets-nfs-client-provisioner-pending-in-creating-pvc/</link><pubDate>Sun, 29 Aug 2021 21:47:00 +0000</pubDate><guid>https://ulagraphy.netlify.com/post/kubernets-nfs-client-provisioner-pending-in-creating-pvc/</guid><description>&lt;p>上次將 K8s 集群從 1.7 升級到 1.20 之後，在創建 pvc 時，發現狀態會一直停留在 Pending，詳細資訊如下：&lt;/p></description></item><item><title>Terraform Connection to Kuberentes Refused Error</title><link>https://ulagraphy.netlify.com/post/devops-terraform-connection-refused-error/terraform-connection-refused-error/</link><pubDate>Sun, 29 Aug 2021 21:40:00 +0000</pubDate><guid>https://ulagraphy.netlify.com/post/devops-terraform-connection-refused-error/terraform-connection-refused-error/</guid><description>&lt;h2 id="問題">問題
&lt;a class="header-anchor" href="#%e5%95%8f%e9%a1%8c">&lt;/a>
&lt;/h2>&lt;p>在 terrform apply 的時候一直卡在 &lt;code>Error: Post &amp;quot;http://localhost/api/v1/namespaces&amp;quot;: dial tcp [::1]:80: connect: connection refused&lt;/code> 的錯誤&lt;/p></description></item><item><title>Kubernetes 升級紀錄</title><link>https://ulagraphy.netlify.com/post/k8s-upgrade/k8s-upgrade/</link><pubDate>Sun, 22 Aug 2021 21:29:00 +0000</pubDate><guid>https://ulagraphy.netlify.com/post/k8s-upgrade/k8s-upgrade/</guid><description>&lt;p>紀錄在現有 kubernetes 1.17 集群升級到 1.20 的過程。&lt;/p></description></item><item><title>ConfigMap 建立及掛載</title><link>https://ulagraphy.netlify.com/post/kubernets-configmap/</link><pubDate>Fri, 08 Jan 2021 14:26:00 +0000</pubDate><guid>https://ulagraphy.netlify.com/post/kubernets-configmap/</guid><description>&lt;h2 id="configmap">ConfigMap
&lt;a class="header-anchor" href="#configmap">&lt;/a>
&lt;/h2>&lt;p>ConfigMap 以 key-vaule 的方式用來描述系統相關設定，所有與應用程式相關的&lt;strong>非敏感性&lt;/strong>未加密的資訊可放在 ConfigMap 內。而如有敏感性資料，則需透過 &lt;code>Secret&lt;/code>。&lt;/p></description></item><item><title>Kubernetes Object 基本對象介紹</title><link>https://ulagraphy.netlify.com/post/kubernets-object/</link><pubDate>Thu, 07 Jan 2021 17:38:00 +0000</pubDate><guid>https://ulagraphy.netlify.com/post/kubernets-object/</guid><description>&lt;h2 id="k8s-obejcts">K8s Obejcts
&lt;a class="header-anchor" href="#k8s-obejcts">&lt;/a>
&lt;/h2>&lt;h3 id="常用的基本-objects">常用的基本 objects
&lt;a class="header-anchor" href="#%e5%b8%b8%e7%94%a8%e7%9a%84%e5%9f%ba%e6%9c%ac-objects">&lt;/a>
&lt;/h3>&lt;ul>
&lt;li>Pod&lt;br>
Pod 有兩種類型：普通 Pod 和靜態 Pod (static pod)。靜態 Pod 即不通過 K8S 調度和創建，直接在某個具體的 Node 機器上通過具體的文件來啟動。普通 Pod 則是由 K8S 創建、調度，同時數據存放在 etcd 中。&lt;/li>
&lt;/ul></description></item></channel></rss>